{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Fast Grammar Fuzzers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uname -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sw_vers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!system_profiler SPHardwareDataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "def ipkg(pkg, repo):\n",
    "    try:\n",
    "        distinfo = pkg_resources.get_distribution(pkg)\n",
    "    except pkg_resources.DistributionNotFound:\n",
    "        !pip install {repo}\n",
    "    else:\n",
    "        print(pkg, 'found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipkg('fuzzingbook', 'pip install fuzzingbook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzingbook.Timer import Timer\n",
    "from fuzzingbook.ExpectError import ExpectTimeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /tmp/stop.ffg\n",
    "!rm -rf testers tests\n",
    "!mkdir -p testers tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**important** We rely on a relatively high recursion limit : 20900 which is only available on MacOSX (Not in Linux)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why focus on faster grammar fuzzing?\n",
    "* Guided fuzzing of uninstrumented code (aka. memory fuzzing).\n",
    "* Grammar Mining advances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a simple fuzzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def producer(chars, l, n=1):\n",
    "    return [''.join([random.choice(chars) for i in range(l)]) for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer(string.printable, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A better tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess\n",
    "from datetime import datetime\n",
    "from resource import getrusage as resource_usage, RUSAGE_CHILDREN\n",
    "from time import time as timestamp\n",
    "START_TIME = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class timeit():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __enter__(self):\n",
    "        self.start_time, self.start_resources = timestamp(), resource_usage(RUSAGE_CHILDREN)\n",
    "        return self\n",
    "    def __exit__(self, *args, **kwargs):\n",
    "        end_time, end_resources = timestamp(), resource_usage(RUSAGE_CHILDREN)\n",
    "        self._runtime = end_time - self.start_time\n",
    "        self.sys_runtime = end_resources.ru_stime - self.start_resources.ru_stime \n",
    "        self.usr_runtime = end_resources.ru_utime - self.start_resources.ru_utime\n",
    "        self.runtime = self.sys_runtime + self.usr_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TX = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# II\n",
    "class Tester:\n",
    "    def __init__(self, name=None, max_num=10000, start_depth=3, limit_depth=9, timeout=3600, iterations=2):\n",
    "        global TX\n",
    "        if name is not None:\n",
    "            self.tname = name\n",
    "        else:\n",
    "            self.tname = self.__class__.__name__\n",
    "        self.tx = TX\n",
    "        self.max_num, self.start_depth, self.limit_depth, self.timeout, self.iterations = \\\n",
    "            max_num, start_depth, limit_depth, timeout, iterations\n",
    "        self.tst = {}\n",
    "        self.tx[self.tname] = self.tst\n",
    "        self.WARMUP_TIMES = 10\n",
    "        self.timedout = None\n",
    "        \n",
    "    def write_t(self, cmd):\n",
    "        self.t = \"testers/%s-t.sh\" % self.tname\n",
    "        with open(self.t, 'w') as f:\n",
    "            print('''\\\n",
    "#!/usr/bin/env bash\n",
    "TIMEFORMAT=\"%%U %%S\";\n",
    "time %(cmd)s''' % {'cmd':cmd}, file=f)\n",
    "        !chmod +x {self.t}\n",
    "        \n",
    "        \n",
    "    def init_run(self):\n",
    "        !rm -rf testers/{self.tname}\n",
    "        !mkdir -p testers/{self.tname}\n",
    "\n",
    "    def pre_time(self):\n",
    "        !rm -rf tests\n",
    "        !mkdir -p tests\n",
    "        \n",
    "    def pre_exec(self, t):\n",
    "        pass\n",
    "        \n",
    "    def exec_program(self, seed, max_depth, t):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def post_exec(self, t):\n",
    "        pass\n",
    "    \n",
    "    def post_time(self):\n",
    "        if not self._runtime: return\n",
    "        if self.file is not None and os.path.exists(self.file):\n",
    "            lines_cmd = (\"cat %s| wc -l\" % self.file)\n",
    "            self.lines = subprocess.getoutput(lines_cmd).strip()\n",
    "            #unique_cmd = (\"cat %s| sort -u| wc -l\" % self.file)\n",
    "            #self.unique_lines = subprocess.getoutput(unique_cmd).strip()\n",
    "            self.size = os.stat(self.file).st_size\n",
    "        else:\n",
    "            self.unique_lines = ''\n",
    "            self.lines = ''\n",
    "            self.size = 0\n",
    "        self.throughput = (self.size/1024/self._runtime, self._runtime)\n",
    "    \n",
    "    def timed_exec(self, seed, max_depth, verbose):\n",
    "        self.pre_time()\n",
    "        self._runtime = None\n",
    "        self.timedout = True\n",
    "        with ExpectTimeout(self.timeout): #, print_traceback=False, mute=True):\n",
    "            #with timeit() as t:\n",
    "            t = None\n",
    "            self.pre_exec(t)\n",
    "            cmdline = self.exec_program(seed, max_depth, t)\n",
    "            self.write_t(cmdline)\n",
    "            !{self.t} 2>./testers/time.out\n",
    "            self.post_exec(t)\n",
    "            with open('testers/time.out') as f:\n",
    "                usr, sys = f.read().strip().split(' ')\n",
    "            self._runtime = float(usr)+ float(sys)\n",
    "            self._sys_runtime = float(sys)\n",
    "            self._usr_runtime = float(usr)\n",
    "            self.timedout = False\n",
    "        self.post_time()\n",
    "\n",
    "    def ofile(self, max_depth, seed):\n",
    "        fn = 'testers/%s/%d_%d.x' % (self.tname, max_depth, seed)\n",
    "        return fn\n",
    "    \n",
    "    def check_continue(self):\n",
    "        if os.path.exists('/tmp/stop.ffg'):\n",
    "            raise Exception('/tmp/stop.ffg -- abort tests')\n",
    "\n",
    "    def run_test(self, verbose=False):\n",
    "        def warmup(seed):\n",
    "            # for warming up, we simply run it a few times before in the\n",
    "            # same seed as the first, and discount it in computation.\n",
    "            return [seed]*self.WARMUP_TIMES\n",
    "        current_time = datetime.now()\n",
    "        self.init_run()\n",
    "        # depth is for later when we deal with grammars.\n",
    "        \n",
    "        # warmup loop\n",
    "        for md in [self.start_depth]:\n",
    "            max_depth = 2**md\n",
    "            for seed in warmup(0):\n",
    "                self.file = self.ofile(max_depth, seed)\n",
    "                self.timed_exec(seed, max_depth, verbose)\n",
    "                if os.path.exists(self.file): os.remove(self.file)\n",
    "                \n",
    "        for md in range(self.start_depth, self.limit_depth):\n",
    "            max_depth = 2**md\n",
    "            v = {}\n",
    "            res = {'detail': v}\n",
    "            self.tst[max_depth] = res\n",
    "            seeds = list(range(self.iterations))\n",
    "            for seed in seeds:\n",
    "                if self.timedout: break\n",
    "                self.file = self.ofile(max_depth, seed)\n",
    "                if verbose: print('depth:', max_depth, 'seed:', seed, 'file:', self.file)\n",
    "                self.timed_exec(seed, max_depth, verbose)\n",
    "                if self._runtime:\n",
    "                    v[seed] = {\n",
    "                        'runtime':self._runtime,\n",
    "                        'sys_runtime':self._sys_runtime,\n",
    "                        'usr_runtime':self._usr_runtime,\n",
    "                        'size': self.size,\n",
    "                        #'uniq': self.unique_lines,\n",
    "                        'lines': self.lines,\n",
    "                        # in kbytes\n",
    "                        'throughput': self.size/self._runtime/(1024)}\n",
    "                if verbose:\n",
    "                    print(v[seed])\n",
    "                if os.path.exists(self.file): os.remove(self.file)\n",
    "                self.check_continue()\n",
    "            if self.timedout:\n",
    "                print('Timeout')\n",
    "                break # we do not expect larger depths to work.\n",
    "            size = [t['size'] for t in v.values()]\n",
    "            res['avgsize'] = statistics.mean(size)\n",
    "                \n",
    "            sec = [t['runtime'] for t in v.values()]\n",
    "            res['avgruntime'] = statistics.mean(sec)\n",
    "            res['stdevruntime'] = statistics.stdev(sec)\n",
    "                \n",
    "            tp = [t['throughput'] for t in v.values()]\n",
    "            res['avgthroughput'] = statistics.mean(tp)\n",
    "            res['stdevthroughput'] = statistics.stdev(tp)\n",
    "            print('depth=', max_depth, \"size=\", res['avgsize'], 'time=', round(res['avgruntime'],3), \"stdev(%s)\" % str(round(res['stdevruntime'],3)), 'throughput=',res['avgthroughput'], \"stdev(%s)\" % str(round(res['stdevthroughput'])))\n",
    "        self.total_test_time = datetime.now() - current_time\n",
    "        self.dump()\n",
    "        return self\n",
    "    \n",
    "    def dump(self):\n",
    "        curtime = datetime.now().isoformat()\n",
    "        name = 'results/%s-tx.json' % (self.tname)\n",
    "        !mkdir -p results\n",
    "        with open(name, 'w+') as f:\n",
    "            print(json.dumps(TX), file=f)\n",
    "    \n",
    "    def show(self):\n",
    "        max_throughput = 0\n",
    "        best_depth = None\n",
    "        for depth in self.tst.keys():\n",
    "            res = self.tst[depth]\n",
    "            if res.get('avgthroughput',0) > max_throughput:\n",
    "                max_throughput = res['avgthroughput'] \n",
    "                best_depth = depth\n",
    "        print('Throughput of ', max_throughput, ' kilobytes per second at depth = ', best_depth)\n",
    "        print(\"Total time:\",str(self.total_test_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomTester(Tester):\n",
    "    def pre_time(self):\n",
    "        with open('testers/RandomTester/r.py', 'w+') as f:\n",
    "            print('''\n",
    "import string,random,sys\n",
    "random.seed(int(sys.argv[1]))\n",
    "def producer(chars, l, n=1):\n",
    "    return [''.join([random.choice(chars) for i in range(l)]) for i in range(n)]\n",
    "print(producer(string.printable, int(sys.argv[2])))''', file=f)\n",
    "            \n",
    "    def exec_program(self, seed, max_depth, t):\n",
    "        fn = self.ofile(max_depth, seed)\n",
    "        return f\"python testers/RandomTester/r.py {seed} {max_depth} > {fn}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomTester().run_test().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grammars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fuzzingbook.Parser import make_grammar\n",
    "# We need a bit of modification make the grammars more varied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prod_line_grammar(nonterminals, terminals):\n",
    "    g = {\n",
    "        '<start>': ['<symbols>'],\n",
    "        '<symbols>': ['<symbol><symbols>', '<symbol>'],\n",
    "        '<symbol>': ['<nonterminals>', '<terminals>'],\n",
    "        '<nonterminals>': ['<lt><alpha><gt>'],\n",
    "        '<lt>': ['<'],\n",
    "        '<gt>': ['>'],\n",
    "        '<alpha>': nonterminals,\n",
    "        '<terminals>': terminals\n",
    "    }\n",
    "\n",
    "    if not nonterminals:\n",
    "        g['<nonterminals>'] = ['']\n",
    "        del g['<lt>']\n",
    "        del g['<alpha>']\n",
    "        del g['<gt>']\n",
    "\n",
    "    return g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzingbook.GrammarFuzzer import GrammarFuzzer\n",
    "from fuzzingbook.Parser import canonical\n",
    "from fuzzingbook.Grammars import unreachable_nonterminals, RE_NONTERMINAL\n",
    "import random, string, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_rule(nonterminals, terminals, num_alts):\n",
    "    prod_grammar = prod_line_grammar(nonterminals, terminals)\n",
    "\n",
    "    gf = GrammarFuzzer(prod_grammar, min_nonterminals=3, max_nonterminals=5)\n",
    "    name = \"<%s>\" % ''.join(random.choices(string.ascii_uppercase, k=3))\n",
    "\n",
    "    return (name, [gf.fuzz() for _ in range(num_alts)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_rule([\"A\", \"B\", \"C\"], [\"1\", \"2\", \"3\"], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_grammar(num_symbols=3, num_alts=3):\n",
    "    terminals = list(string.ascii_lowercase)\n",
    "    grammar = {}\n",
    "    name = None\n",
    "    for _ in range(num_symbols):\n",
    "        nonterminals = [k[1:-1] for k in grammar.keys()]\n",
    "        name, expansions = \\\n",
    "            make_rule(nonterminals, terminals, num_alts)\n",
    "        grammar[name] = expansions\n",
    "\n",
    "    grammar['<start>'] = [name]\n",
    "\n",
    "    # Remove unused parts\n",
    "    for nonterminal in unreachable_nonterminals(grammar):\n",
    "        del grammar[nonterminal]\n",
    "        \n",
    "    return grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canonical(make_grammar())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A CSS grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "css_grammar = {\n",
    "    '<start>': [['<stylesheet>']],\n",
    "    '<stylesheet>': [[\n",
    "        '<[CHARSET_SYM_STRING_SEMI]-1>', ' ', '<[S_OR_CDO_OR_CDC]-1>', ' ',\n",
    "        '<[import_CDO_S_OR_CDC_S]-1>', ' ', '<[stylesheet_closing_GROUPING]-1>'\n",
    "    ]],\n",
    "    '<[CHARSET_SYM_STRING_SEMI]>': [['<CHARSET_SYM>', ' ', '<STRING>', ' ;']],\n",
    "    '<[S_OR_CDO_OR_CDC]>': [['<Sp>'], ['<CDO>'], ['<CDC>']],\n",
    "    '<[import_CDO_S_OR_CDC_S]>': [['<import>', ' ', '<[CDO_S_OR_CDC_S]-1>']],\n",
    "    '<[CDO_S_OR_CDC_S]>': [['<CDO>', ' ', '<Ss>'], ['<CDC>', ' ', '<Ss>']],\n",
    "    '<[ruleset_OR_media_OR_page]>': [['<ruleset>'], ['<media>'], ['<page>']],\n",
    "    '<[stylesheet_closing_GROUPING]>':\n",
    "    [['<[ruleset_OR_media_OR_page]>', ' ', '<[CDO_S_OR_CDC_S]-2>']],\n",
    "    '<import>': [[\n",
    "        '<IMPORT_SYM>', ' ', '<Ss>', ' ', '<[STRING_OR_URI]>', ' ', '<Ss>',\n",
    "        ' ', '<media_list-1>', ' ; ', '<Ss>'\n",
    "    ]],\n",
    "    '<[STRING_OR_URI]>': [['<STRING>'], ['<URI>']],\n",
    "    '<media>': [[\n",
    "        '<MEDIA_SYM>', ' ', '<Ss>', ' ', '<media_list>', ' { ', '<Ss>', ' ',\n",
    "        '<ruleset-1>', ' } ', '<Ss>'\n",
    "    ]],\n",
    "    '<media_list>': [['<medium>', ' ', '<[COMMA_S_medium]-1>']],\n",
    "    '<[COMMA_S_medium]>': [[', ', '<Ss>', ' ', '<medium>']],\n",
    "    '<medium>': [['<IDENT>', ' ', '<Ss>']],\n",
    "    '<page>': [[\n",
    "        '<PAGE_SYM>', ' ', '<Ss>', ' ', '<pseudo_page-1>', ' { ', '<Ss>', ' ',\n",
    "        '<declaration-1>', ' ', '<[SEMI_S_declaration]-1>', ' } ', '<Ss>'\n",
    "    ]],\n",
    "    '<[SEMI_S_declaration]>': [['; ', '<Ss>', ' ', '<declaration-2>']],\n",
    "    '<pseudo_page>': [[': ', '<IDENT>', ' ', '<Ss>']],\n",
    "    '<operator>': [['/ ', '<Ss>'], [', ', '<Ss>']],\n",
    "    '<combinator>': [['+ ', '<Ss>'], ['> ', '<Ss>']],\n",
    "    '<unary_operator>': [['-'], ['+']],\n",
    "    '<property>': [['<IDENT>', ' ', '<Ss>']],\n",
    "    '<ruleset>': [[\n",
    "        '<selector>', ' ', '<COMMA_S_selector-1>', ' { ', '<Ss>', ' ',\n",
    "        '<declaration-3>', ' ', '<[SEMI_S_declaration]-2>', ' } ', '<Ss>'\n",
    "    ]],\n",
    "    '<COMMA_S_selector>': [[', ', '<Ss>', ' selector']],\n",
    "    '<selector>':\n",
    "    [['<simple_selector>', ' ', '<[combinator_selector_OR_S]-1>']],\n",
    "    '<[combinator_selector]>': [['<combinator-1>', ' ', '<selector>']],\n",
    "    '<[combinator_selector_OR_S]>':\n",
    "    [['<combinator>', ' ', '<selector>'],\n",
    "     ['<Sp>', ' ', '<[combinator_selector]-1>']],\n",
    "    '<simple_selector>':\n",
    "    [['<element_name>', ' ', '<[HASH_OR_class_OR_attrib_OR_pseudo]-1>'],\n",
    "     ['<[HASH_OR_class_OR_attrib_OR_pseudo]-2>']],\n",
    "    '<[HASH_OR_class_OR_attrib_OR_pseudo]>': [['<HASH>'], ['<class>'],\n",
    "                                              ['<attrib>'], ['<pseudo>']],\n",
    "    '<class>': [['.', '<IDENT>']],\n",
    "    '<element_name>': [['<IDENT>'], ['*']],\n",
    "    '<attrib>': [[\n",
    "        '[ ', '<Ss>', ' ', '<IDENT>', ' ', '<Ss>', ' ',\n",
    "        '<[attrib_GROUPING]-1>', ' ]'\n",
    "    ]],\n",
    "    '<[EQUAL_OR_INCLUDES_OR_DASHMATCH]>': [['='], ['<INCLUDES>'],\n",
    "                                           ['<DASHMATCH>']],\n",
    "    '<[IDENT_OR_STRING]>': [['<IDENT>'], ['<STRING>']],\n",
    "    '<[attrib_GROUPING]>': [[\n",
    "        '<[EQUAL_OR_INCLUDES_OR_DASHMATCH]>', ' ', '<Ss>', ' ',\n",
    "        '<[IDENT_OR_STRING]>', ' ', '<Ss>'\n",
    "    ]],\n",
    "    '<pseudo>': [[': ', '<[IDENT_OR_FUNCTION]>']],\n",
    "    '<[IDENT_OR_FUNCTION]>':\n",
    "    [['<IDENT>'], ['<FUNCTION>', ' ', '<Ss>', ' ', '<[IDENT_S]-1>', ' )']],\n",
    "    '<[IDENT_S]>': [['<IDENT>', ' ', '<Ss>']],\n",
    "    '<declaration>':\n",
    "    [['<property>', ' : ', '<Ss>', ' ', '<expr>', ' ', '<prio-1>']],\n",
    "    '<prio>': [['<IMPORT_SYM>', ' ', '<Ss>']],\n",
    "    '<expr>': [['<term>', ' ', '<[operator_term]-1>']],\n",
    "    '<[operator_term]>': [['<operator-1>', ' ', '<term>']],\n",
    "    '<term>': [['<unary_operator-1>', ' ', '<[term_GROUPING]>'],\n",
    "               ['<STRING>', ' ', '<Ss>'], ['<IDENT>', ' ', '<Ss>'],\n",
    "               ['<URI>', ' ', '<Ss>'], ['<hexcolor>'], ['<function>']],\n",
    "    '<[term_GROUPING]>': [['<NUMBER>', ' ', '<Ss>'],\n",
    "                          ['<PERCENTAGE>', ' ', '<Ss>'],\n",
    "                          ['<LENGTH>', ' ', '<Ss>'], ['<EMS>', ' ', '<Ss>'],\n",
    "                          ['<EXS>', ' ', '<Ss>'], ['<ANGLE>', ' ', '<Ss>'],\n",
    "                          ['<TIME>', ' ', '<Ss>'], ['<FREQ>', ' ', '<Ss>']],\n",
    "    '<function>': [['<FUNCTION>', ' ', '<Ss>', ' ', '<expr>', ' ) ', '<Ss>']],\n",
    "    '<hexcolor>': [['#', '<[three_char_HEX]>', ' ', '<Ss>'],\n",
    "                   ['#', '<[six_char_HEX]>', ' ', '<Ss>']],\n",
    "    '<[three_char_HEX]>': [['<HEX_CHAR>', '<HEX_CHAR>', '<HEX_CHAR>']],\n",
    "    '<[six_char_HEX]>': [[\n",
    "        '<HEX_CHAR>', '<HEX_CHAR>', '<HEX_CHAR>', '<HEX_CHAR>', '<HEX_CHAR>',\n",
    "        '<HEX_CHAR>'\n",
    "    ]],\n",
    "    '<HEX_CHAR>': [['0'], ['1'], ['2'], ['3'], ['4'], ['5'], ['6'], ['7'],\n",
    "                   ['8'], ['9'], ['a'], ['b'], ['c'], ['d'], ['e'], ['f']],\n",
    "    '<Sp>': [['<WHITESPACE-1>']],\n",
    "    '<Ss>': [['<WHITESPACE-2>']],\n",
    "    '<So>': [[], ['<WHITESPACE>']],\n",
    "    '<CDO>': [['<!--']],\n",
    "    '<CDC>': [['-->']],\n",
    "    '<INCLUDES>': [['~=']],\n",
    "    '<DASHMATCH>': [['|=']],\n",
    "    '<STRING>': [['<string1>'], ['<string2>']],\n",
    "    '<IDENT>': [['<{ident}>']],\n",
    "    '<HASH>': [['#', '<{name}>']],\n",
    "    '<IMPORT_SYM>': [['@import']],\n",
    "    '<PAGE_SYM>': [['@page']],\n",
    "    '<MEDIA_SYM>': [['@media']],\n",
    "    '<CHARSET_SYM>': [['@charset']],\n",
    "    '<EMS>': [['<{num}>', 'em']],\n",
    "    '<EXS>': [['<{num}>', 'ex']],\n",
    "    '<LENGTH>': [['<{num}>', 'px'], ['<{num}>', 'cm'], ['<{num}>', 'mm'],\n",
    "                 ['<{num}>', 'in'], ['<{num}>', 'pt'], ['<{num}>', 'pc']],\n",
    "    '<ANGLE>': [['<{num}>', 'deg'], ['<{num}>', 'rad'], ['<{num}>', 'grad']],\n",
    "    '<TIME>': [['<{num}>', 'ms'], ['<{num}>', 's']],\n",
    "    '<FREQ>': [['<{num}>', 'hz'], ['<{num}>', 'khz']],\n",
    "    '<PERCENTAGE>': [['<{num}>', '%']],\n",
    "    '<NUMBER>': [['<{num}>']],\n",
    "    '<URI>': [['url(\"', '<So>', '<url>', '<So>', '\")'],\n",
    "              ['url(\"', '<So>', '<STRING>', '<So>', '\")']],\n",
    "    '<url>': [['<url_-1>']],\n",
    "    '<url_>': [['!'], ['#'], ['$'], ['%'], ['&'], ['*'], ['-'], ['~'],\n",
    "               ['<escape>']],\n",
    "    '<FUNCTION>': [['<{ident}>', '(']],\n",
    "    '<string1>': [['\"', '<qmychars1-1>', '\"']],\n",
    "    '<string2>': [[\"'\", '<qmychars2-1>', \"'\"]],\n",
    "    '<qnonl1>': [['7'], ['Q'], ['J'], ['@'], ['2'], ['g'], ['\\t'], ['X'],\n",
    "                 ['`'], ['G'], ['e'], ['['], ['?'], ['v'], ['$'], ['j'], ['K'],\n",
    "                 ['d'], ['A'], ['n'], ['h'], ['l'], ['4'], ['D'], ['a'], ['#'],\n",
    "                 ['f'], ['y'], ['B'], ['U'], ['P'], ['3'], ['O'], ['S'], [')'],\n",
    "                 [' '], ['W'], ['o'], ['b'], ['|'], ['q'], ['L'], [']'], ['V'],\n",
    "                 ['*'], ['z'], ['}'], ['6'], ['u'], ['^'], [','], ['N'], ['>'],\n",
    "                 ['+'], ['Y'], ['t'], ['k'], ['!'], ['p'], ['Z'], ['E'], ['('],\n",
    "                 ['\\\\'], ['<'], ['F'], ['%'], ['9'], ['0'], ['s'], [';'],\n",
    "                 ['&'], ['C'], ['T'], ['r'], ['5'], ['R'], [\"'\"], ['_'], ['.'],\n",
    "                 ['8'], ['H'], ['i'], ['/'], ['M'], ['~'], ['{'], [':'], ['c'],\n",
    "                 ['I'], ['-'], ['\\x0b'], ['1'], ['w'], ['x'], ['m'], ['=']],\n",
    "    '<qnonl2>': [['7'], ['Q'], ['J'], ['@'], ['2'], ['g'], ['\\t'], ['X'],\n",
    "                 ['`'], ['G'], ['e'], ['['], ['?'], ['v'], ['$'], ['j'], ['K'],\n",
    "                 ['d'], ['A'], ['n'], ['h'], ['l'], ['4'], ['D'], ['a'], ['#'],\n",
    "                 ['f'], ['y'], ['B'], ['U'], ['P'], ['3'], ['O'], ['S'], [')'],\n",
    "                 [' '], ['W'], ['o'], ['b'], ['|'], ['q'], ['L'], [']'], ['V'],\n",
    "                 ['*'], ['z'], ['}'], ['6'], ['u'], ['^'], [','], ['N'], ['>'],\n",
    "                 ['+'], ['Y'], ['t'], ['k'], ['!'], ['p'], ['Z'], ['E'], ['('],\n",
    "                 ['\\\\'], ['<'], ['F'], ['%'], ['9'], ['0'], ['s'], [';'],\n",
    "                 ['&'], ['C'], ['T'], ['r'], ['5'], ['\"'], ['R'], ['_'], ['.'],\n",
    "                 ['8'], ['H'], ['i'], ['/'], ['M'], ['~'], ['{'], [':'], ['c'],\n",
    "                 ['I'], ['-'], ['\\x0b'], ['1'], ['w'], ['x'], ['m'], ['=']],\n",
    "    '<qmychars1>': [['<qnonl1>'], ['\\\\', '<nl>'], ['<escape>']],\n",
    "    '<qmychars2>': [['<qnonl2>'], ['\\\\', '<nl>'], ['<escape>']],\n",
    "    '<nl>': [['\\r'], ['\\n'], ['\\x0c'], ['\\r\\n']],\n",
    "    '<escape>': [['\\\\', '<echar>']],\n",
    "    '<echar>': [['Q'], ['J'], ['@'], ['g'], ['\\t'], ['X'], ['`'], ['G'], ['['],\n",
    "                ['?'], ['v'], ['$'], ['j'], ['K'], ['A'], ['n'], ['h'], ['l'],\n",
    "                ['D'], ['#'], ['y'], ['B'], ['U'], ['P'], ['O'], ['S'], [')'],\n",
    "                [' '], ['W'], ['o'], ['|'], ['q'], ['L'], [']'], ['V'], ['*'],\n",
    "                ['z'], ['}'], ['u'], ['^'], [','], ['N'], ['>'], ['+'], ['Y'],\n",
    "                ['t'], ['k'], ['!'], ['p'], ['Z'], ['E'], ['('], ['\\\\'], ['<'],\n",
    "                ['F'], ['%'], ['s'], [';'], ['&'], ['C'], ['T'], ['r'], ['\"'],\n",
    "                ['R'], [\"'\"], ['_'], ['.'], ['H'], ['i'], ['/'], ['M'], ['~'],\n",
    "                ['{'], [':'], ['I'], ['-'], ['\\x0b'], ['w'], ['x'], ['m'],\n",
    "                ['=']],\n",
    "    '<{ident}>': [['<minus-1>', '<nmstart>', '<nmchar-1>']],\n",
    "    '<nmstart>': [['a'], ['b'], ['c'], ['d'], ['e'], ['f'], ['g'], ['h'],\n",
    "                  ['i'], ['j'], ['k'], ['l'], ['m'], ['n'], ['o'], ['p'],\n",
    "                  ['q'], ['r'], ['s'], ['t'], ['u'], ['v'], ['w'], ['x'],\n",
    "                  ['y'], ['z'], ['<escape>'], ['_']],\n",
    "    '<nmchar>': [['a'], ['b'], ['c'], ['d'], ['e'], ['f'], ['g'], ['h'], ['i'],\n",
    "                 ['j'], ['k'], ['l'], ['m'], ['n'], ['o'], ['p'], ['q'], ['r'],\n",
    "                 ['s'], ['t'], ['u'], ['v'], ['w'], ['x'], ['y'], ['z'], ['0'],\n",
    "                 ['1'], ['2'], ['3'], ['4'], ['5'], ['6'], ['7'], ['8'], ['9'],\n",
    "                 ['<escape>'], ['_'], ['-']],\n",
    "    '<minus>': [['-']],\n",
    "    '<{name}>': [['<nmchar-2>']],\n",
    "    '<{num}>': [['<INTEGER>']],\n",
    "    '<WHITESPACE>': [[' '], ['\\t']],\n",
    "    '<INTEGER>': [['<DIGIT>', '<INTEGER>'], ['<DIGIT>']],\n",
    "    '<DIGIT>': [['0'], ['1'], ['2'], ['3'], ['4'], ['5'], ['6'], ['7'], ['8'],\n",
    "                ['9']],\n",
    "    '<[CHARSET_SYM_STRING_SEMI]-1>': [[], ['<[CHARSET_SYM_STRING_SEMI]>']],\n",
    "    '<[S_OR_CDO_OR_CDC]-1>': [[],\n",
    "                              ['<[S_OR_CDO_OR_CDC]>',\n",
    "                               '<[S_OR_CDO_OR_CDC]-1>']],\n",
    "    '<[import_CDO_S_OR_CDC_S]-1>':\n",
    "    [[], ['<[import_CDO_S_OR_CDC_S]>', '<[import_CDO_S_OR_CDC_S]-1>']],\n",
    "    '<[stylesheet_closing_GROUPING]-1>':\n",
    "    [[],\n",
    "     ['<[stylesheet_closing_GROUPING]>', '<[stylesheet_closing_GROUPING]-1>']],\n",
    "    '<[CDO_S_OR_CDC_S]-1>': [[],\n",
    "                             ['<[CDO_S_OR_CDC_S]>', '<[CDO_S_OR_CDC_S]-1>']],\n",
    "    '<[CDO_S_OR_CDC_S]-2>': [[],\n",
    "                             ['<[CDO_S_OR_CDC_S]>', '<[CDO_S_OR_CDC_S]-2>']],\n",
    "    '<media_list-1>': [[], ['<media_list>']],\n",
    "    '<ruleset-1>': [[], ['<ruleset>', '<ruleset-1>']],\n",
    "    '<[COMMA_S_medium]-1>': [[],\n",
    "                             ['<[COMMA_S_medium]>', '<[COMMA_S_medium]-1>']],\n",
    "    '<pseudo_page-1>': [[], ['<pseudo_page>']],\n",
    "    '<declaration-1>': [[], ['<declaration>']],\n",
    "    '<[SEMI_S_declaration]-1>':\n",
    "    [[], ['<[SEMI_S_declaration]>', '<[SEMI_S_declaration]-1>']],\n",
    "    '<declaration-2>': [[], ['<declaration>']],\n",
    "    '<COMMA_S_selector-1>': [[],\n",
    "                             ['<COMMA_S_selector>', '<COMMA_S_selector-1>']],\n",
    "    '<declaration-3>': [[], ['<declaration>']],\n",
    "    '<[SEMI_S_declaration]-2>':\n",
    "    [[], ['<[SEMI_S_declaration]>', '<[SEMI_S_declaration]-2>']],\n",
    "    '<[combinator_selector_OR_S]-1>': [[], ['<[combinator_selector_OR_S]>']],\n",
    "    '<combinator-1>': [[], ['<combinator>']],\n",
    "    '<[combinator_selector]-1>': [[], ['<[combinator_selector]>']],\n",
    "    '<[HASH_OR_class_OR_attrib_OR_pseudo]-1>':\n",
    "    [[],\n",
    "     [\n",
    "         '<[HASH_OR_class_OR_attrib_OR_pseudo]>',\n",
    "         '<[HASH_OR_class_OR_attrib_OR_pseudo]-1>'\n",
    "     ]],\n",
    "    '<[HASH_OR_class_OR_attrib_OR_pseudo]-2>':\n",
    "    [['<[HASH_OR_class_OR_attrib_OR_pseudo]>'],\n",
    "     [\n",
    "         '<[HASH_OR_class_OR_attrib_OR_pseudo]>',\n",
    "         '<[HASH_OR_class_OR_attrib_OR_pseudo]-2>'\n",
    "     ]],\n",
    "    '<[attrib_GROUPING]-1>': [[], ['<[attrib_GROUPING]>']],\n",
    "    '<[IDENT_S]-1>': [[], ['<[IDENT_S]>']],\n",
    "    '<prio-1>': [[], ['<prio>']],\n",
    "    '<[operator_term]-1>': [[], ['<[operator_term]>', '<[operator_term]-1>']],\n",
    "    '<operator-1>': [[], ['<operator>']],\n",
    "    '<unary_operator-1>': [[], ['<unary_operator>']],\n",
    "    '<WHITESPACE-1>': [['<WHITESPACE>'], ['<WHITESPACE>', '<WHITESPACE-1>']],\n",
    "    '<WHITESPACE-2>': [[], ['<WHITESPACE>', '<WHITESPACE-2>']],\n",
    "    '<url_-1>': [[], ['<url_>', '<url_-1>']],\n",
    "    '<qmychars1-1>': [[], ['<qmychars1>', '<qmychars1-1>']],\n",
    "    '<qmychars2-1>': [[], ['<qmychars2>', '<qmychars2-1>']],\n",
    "    '<minus-1>': [[], ['<minus>']],\n",
    "    '<nmchar-1>': [[], ['<nmchar>', '<nmchar-1>']],\n",
    "    '<nmchar-2>': [['<nmchar>'], ['<nmchar>', '<nmchar-2>']]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A JSON grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "json_grammar = {\n",
    "    '<start>': [['<json>']],\n",
    "    '<json>': [['<element>']],\n",
    "    '<element>': [['<ws>', '<value>', '<ws>']],\n",
    "    '<value>': [['<object>'], ['<array>'], ['<string>'], ['<number>'],\n",
    "                ['true'], ['false'],\n",
    "                ['null']],\n",
    "    '<object>': [['{', '<ws>', '}'], ['{', '<members>', '}']],\n",
    "    '<members>': [['<member>', '<symbol-2>']],\n",
    "    '<member>': [['<ws>', '<string>', '<ws>', ':', '<element>']],\n",
    "    '<array>': [['[', '<ws>', ']'], ['[', '<elements>', ']']],\n",
    "    '<elements>': [['<element>', '<symbol-1-1>']],\n",
    "    '<string>': [['\"', '<characters>', '\"']],\n",
    "    '<characters>': [['<character-1>']],\n",
    "    '<character>': [['0'], ['1'], ['2'], ['3'], ['4'], ['5'], ['6'], ['7'],\n",
    "                    ['8'], ['9'], ['a'], ['b'], ['c'], ['d'], ['e'], ['f'],\n",
    "                    ['g'], ['h'], ['i'], ['j'], ['k'], ['l'], ['m'], ['n'],\n",
    "                    ['o'], ['p'], ['q'], ['r'], ['s'], ['t'], ['u'], ['v'],\n",
    "                    ['w'], ['x'], ['y'], ['z'], ['A'], ['B'], ['C'], ['D'],\n",
    "                    ['E'], ['F'], ['G'], ['H'], ['I'], ['J'], ['K'], ['L'],\n",
    "                    ['M'], ['N'], ['O'], ['P'], ['Q'], ['R'], ['S'], ['T'],\n",
    "                    ['U'], ['V'], ['W'], ['X'], ['Y'], ['Z'], ['!'], ['#'],\n",
    "                    ['$'], ['%'], ['&'], ['\\''], ['('], [')'], ['*'], ['+'],\n",
    "                    [','], ['-'], ['.'], ['/'], [':'], [';'], ['<'], ['='],\n",
    "                    ['>'], ['?'], ['@'], ['['], [']'], ['^'], ['_'], ['`'],\n",
    "                    ['{'], ['|'], ['}'], ['~'], [' '], ['<esc>']],\n",
    "    '<esc>': [['\\\\','<escc>']],\n",
    "    '<escc>': [['\\\\'],['b'],['f'], ['n'], ['r'],['t'],['\"']],\n",
    "    '<number>': [['<int>', '<frac>', '<exp>']],\n",
    "    '<int>': [['<digit>'], ['<onenine>', '<digits>'], ['-', '<digits>'],\n",
    "              ['-', '<onenine>', '<digits>']],\n",
    "    '<digits>': [['<digit-1>']],\n",
    "    '<digit>': [['0'], ['<onenine>']],\n",
    "    '<onenine>': [['1'], ['2'], ['3'], ['4'], ['5'], ['6'], ['7'], ['8'],\n",
    "                  ['9']],\n",
    "    '<frac>': [[], ['.', '<digits>']],\n",
    "    '<exp>': [[], ['E', '<sign>', '<digits>'], ['e', '<sign>', '<digits>']],\n",
    "    '<sign>': [[], ['+'], ['-']],\n",
    "    '<ws>': [['<sp1>', '<ws>'], []],\n",
    "    '<sp1>': [[' '],['\\n'],['\\t'],['\\r']],\n",
    "    '<symbol>': [[',', '<members>']],\n",
    "    '<symbol-1>': [[',', '<elements>']],\n",
    "    '<symbol-2>': [[], ['<symbol>', '<symbol-2>']],\n",
    "    '<symbol-1-1>': [[], ['<symbol-1>', '<symbol-1-1>']],\n",
    "    '<character-1>': [[], ['<character>', '<character-1>']],\n",
    "    '<digit-1>': [['<digit>'], ['<digit>', '<digit-1>']]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An EXPR grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "expr_grammar = {\n",
    "    \"<start>\": [[\"<expr>\"]],\n",
    "    \"<expr>\": [[\"<term>\", \"+\", \"<expr>\"], [\"<term>\", \"-\", \"<expr>\"],\n",
    "               [\"<term>\"]],\n",
    "    \"<term>\": [[\"<factor>\", \"*\", \"<term>\"], [\"<factor>\", \"/\", \"<term>\"],\n",
    "               [\"<factor>\"]],\n",
    "    \"<factor>\": [[\"+\", \"<factor>\"], [\"-\", \"<factor>\"], [\"(\", \"<expr>\", \")\"],\n",
    "                 [\"<integer>\", \".\", \"<integer>\"], [\"<integer>\"]],\n",
    "    \"<integer>\": [[\"<digit>\", \"<integer>\"], [\"<digit>\"]],\n",
    "    \"<digit>\": [[\"0\"], [\"1\"], [\"2\"], [\"3\"], [\"4\"], [\"5\"], [\"6\"], [\"7\"], [\"8\"],\n",
    "                [\"9\"]]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An HTML grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# NOTE: HTML grammar requires precomputing key and rule recursion\n",
    "html_grammar = {\n",
    "    '<start>': [['<_l_>', '!DOCTYPE html', '<_r_>', '<html_document>']],\n",
    "    '<_l_>': [['<']],\n",
    "    '<_r_>': [['>']],\n",
    "    '<_cl_>': [['</']],\n",
    "    '<a_tag>':\n",
    "    [['<_l_>', 'a', '<d>', '<_r_>', '<a_content-1>', '<_cl_>', 'a', '<_r_>']],\n",
    "    '<a_content>': [['<heading>'], ['<text>']],\n",
    "    '<abbr_tag>':\n",
    "    [['<_l_>', 'abbr', '<d>', '<_r_>', '<text>', '<_cl_>', 'abbr', '<_r_>']],\n",
    "    '<acronym_tag>': [[\n",
    "        '<_l_>', 'acronym', '<d>', '<_r_>', '<text>', '<_cl_>', 'acronym',\n",
    "        '<_r_>'\n",
    "    ]],\n",
    "    '<address_tag>': [[\n",
    "        '<_l_>', 'address', '<d>', '<_r_>', '<address_content-1>', '<_cl_>',\n",
    "        'address', '<_r_>'\n",
    "    ]],\n",
    "    '<address_content>': [['<p_tag>'], ['<text>']],\n",
    "    '<applet_content>': [['<param-1>', '<body_content>']],\n",
    "    '<area>': [['<_l_>', 'area', '<d>', '<_r_>']],\n",
    "    '<applet_tag>': [[\n",
    "        '<_l_>', 'applet', '<d>', '<_r_>', '<applet_content>', '<_cl_>',\n",
    "        'applet', '<_r_>'\n",
    "    ]],\n",
    "    '<b_tag>':\n",
    "    [['<_l_>', 'b', '<d>', '<_r_>', '<text>', '<_cl_>', 'b', '<_r_>']],\n",
    "    '<basefont_tag>': [[\n",
    "        '<_l_>', 'basefront', '<d>', '<_r_>', '<body_content>', '<_cl_>',\n",
    "        'basefront', '<_r_>'\n",
    "    ]],\n",
    "    '<bdo_tag>':\n",
    "    [['<_l_>', 'bdo', '<d>', '<_r_>', '<text>', '<_cl_>', 'bdo', '<_r_>']],\n",
    "    '<big_tag>':\n",
    "    [['<_l_>', 'big', '<d>', '<_r_>', '<text>', '<_cl_>', 'big', '<_r_>']],\n",
    "    '<blink_tag>':\n",
    "    [['<_l_>', 'blink', '<d>', '<_r_>', '<text>', '<_cl_>', 'blink', '<_r_>']],\n",
    "    '<block>': [['<block_content-1>']],\n",
    "    '<block_content>': [['<basefont_tag>'], ['<blockquote_tag>'],\n",
    "                        ['<center_tag>'], ['<dir_tag>'], ['<div_tag>'],\n",
    "                        ['<dl_tag>'], ['<form_tag>'], ['<listing_tag>'],\n",
    "                        ['<menu_tag>'], ['<multicol_tag>'], ['<nobr_tag>'],\n",
    "                        ['<ol_tag>'], ['<p_tag>'], ['<pre_tag>'],\n",
    "                        ['<table_tag>'], ['<ul_tag>'], ['<xmp_tag>']],\n",
    "    '<blockquote_tag>': [[\n",
    "        '<_l_>', 'blockquote', '<d>', '<_r_>', '<body_content>', '<_cl_>',\n",
    "        'blockquote', '<_r_>'\n",
    "    ]],\n",
    "    '<body_content>': [['<_l_>', 'bgsound', '<d>', '<_r_>'],\n",
    "                       ['<_l_>', 'hr', '<_r_>'],\n",
    "                       ['<address_tag>'], ['<block>'], ['<del_tag>'],\n",
    "                       ['<heading>'], ['<ins_tag>'], ['<layer_tag>'],\n",
    "                       ['<map_tag>'], ['<marquee_tag>'], ['<text>']],\n",
    "    '<body_tag>': [[\n",
    "        '<_l_>', 'body', '<d>', '<_r_>', '<body_content-1>', '<_cl_>', 'body',\n",
    "        '<_r_>'\n",
    "    ]],\n",
    "    '<caption_tag>': [[\n",
    "        '<_l_>', 'caption', '<d>', '<_r_>', '<body_content-2>', '<_cl_>',\n",
    "        'caption', '<_r_>'\n",
    "    ]],\n",
    "    '<center_tag>': [[\n",
    "        '<_l_>', 'center', '<d>', '<_r_>', '<body_content-3>', '<_cl_>',\n",
    "        'center', '<_r_>'\n",
    "    ]],\n",
    "    '<cite_tag>':\n",
    "    [['<_l_>', 'cite', '<d>', '<_r_>', '<text>', '<_cl_>', 'cite', '<_r_>']],\n",
    "    '<code_tag>':\n",
    "    [['<_l_>', 'code', '<d>', '<_r_>', '<text>', '<_cl_>', 'code', '<_r_>']],\n",
    "    '<colgroup_content>': [['<_l_>', 'col', '<d>', '<_r_-1>']],\n",
    "    '<colgroup_tag>':\n",
    "    [['<_l_>', 'colgroup', '<d>', '<_r_>', '<colgroup_content>']],\n",
    "    '<content_style>': [['<abbr_tag>'], ['<acronym_tag>'], ['<cite_tag>'],\n",
    "                        ['<code_tag>'], ['<dfn_tag>'], ['<em_tag>'],\n",
    "                        ['<kbd_tag>'], ['<q_tag>'], ['<strong_tag>'],\n",
    "                        ['<var_tag>']],\n",
    "    '<dd_tag>':\n",
    "    [['<_l_>', 'dd', '<d>', '<_r_>', '<flow>', '<_cl_>', 'dd', '<_r_>']],\n",
    "    '<del_tag>':\n",
    "    [['<_l_>', 'del', '<d>', '<_r_>', '<flow>', '<_cl_>', 'del', '<_r_>']],\n",
    "    '<dfn_tag>':\n",
    "    [['<_l_>', 'dfn', '<d>', '<_r_>', '<text>', '<_cl_>', 'dfn', '<_r_>']],\n",
    "    '<dir_tag>': [[\n",
    "        '<_l_>', 'dir', '<d>', '<_r_>', '<li_tag-1>', '<_cl_>', 'dir', '<_r_>'\n",
    "    ]],\n",
    "    '<div_tag>': [[\n",
    "        '<_l_>', 'div', '<d>', '<_r_>', '<body_content>', '<_cl_>', 'div',\n",
    "        '<_r_>'\n",
    "    ]],\n",
    "    '<dl_content>': [['<dt_tag>', '<dd_tag>']],\n",
    "    '<dl_tag>': [[\n",
    "        '<_l_>', 'dl', '<d>', '<_r_>', '<dl_content-1>', '<_cl_>', 'dl',\n",
    "        '<_r_>'\n",
    "    ]],\n",
    "    '<dt_tag>': [[\n",
    "        '<_l_>', 'dt', '<d>', '<_r_>', '<text>', '<_cl_>', 'dt', '<_r_>'\n",
    "    ]],\n",
    "    '<em_tag>': [[\n",
    "        '<_l_>', 'em', '<d>', '<_r_>', '<text>', '<_cl_>', 'em', '<_r_>'\n",
    "    ]],\n",
    "    '<fieldset_tag>': [[\n",
    "        '<_l_>', 'fieldset', '<d>', '<_r_>', '<legend_tag-1>',\n",
    "        '<form_content-1>', '<_cl_>', 'fieldset', '<_r_>'\n",
    "    ]],\n",
    "    '<flow>': [['<flow_content-1>']],\n",
    "    '<flow_content>': [['<block>'], ['<text>']],\n",
    "    '<font_tag>': [[\n",
    "        '<_l_>', 'font', '<d>', '<_r_>', '<style_text>', '<_cl_>', 'font',\n",
    "        '<_r_>'\n",
    "    ]],\n",
    "    '<form_content>': [['<_l_>', 'input', '<d>', '<_r_>'],\n",
    "                       ['<_l_>', 'keygen', '<d>', '<_r_>'], ['<body_content>'],\n",
    "                       ['<fieldset_tag>'], ['<label_tag>'], ['<select_tag>'],\n",
    "                       ['<textarea_tag>']],\n",
    "    '<form_tag>': [[\n",
    "        '<_l_>', 'form', '<d>', '<_r_>', '<form_content-2>', '<_cl_>', 'form',\n",
    "        '<_r_>'\n",
    "    ]],\n",
    "    '<frameset_content>': [['<_l_>', 'frame', '<d>', '<_r_>'],\n",
    "                           ['<noframes_tag>']],\n",
    "    '<frameset_tag>': [[\n",
    "        '<_l_>', 'frameset', '<d>', '<_r_>', '<frameset_content-1>', '<_cl_>',\n",
    "        'frameset', '<_r_>'\n",
    "    ]],\n",
    "    '<h1_tag>': [[\n",
    "        '<_l_>', 'h1', '<d>', '<_r_>', '<text>', '<_cl_>', 'h1', '<_r_>'\n",
    "    ]],\n",
    "    '<h2_tag>': [[\n",
    "        '<_l_>', 'h2', '<d>', '<_r_>', '<text>', '<_cl_>', 'h2', '<_r_>'\n",
    "    ]],\n",
    "    '<h3_tag>': [[\n",
    "        '<_l_>', 'h3', '<d>', '<_r_>', '<text>', '<_cl_>', 'h3', '<_r_>'\n",
    "    ]],\n",
    "    '<h4_tag>': [[\n",
    "        '<_l_>', 'h4', '<d>', '<_r_>', '<text>', '<_cl_>', 'h4', '<_r_>'\n",
    "    ]],\n",
    "    '<h5_tag>': [[\n",
    "        '<_l_>', 'h5', '<d>', '<_r_>', '<text>', '<_cl_>', 'h5', '<_r_>'\n",
    "    ]],\n",
    "    '<h6_tag>': [[\n",
    "        '<_l_>', 'h6', '<d>', '<_r_>', '<text>', '<_cl_>', 'h6', '<_r_>'\n",
    "    ]],\n",
    "    '<head_content>': [['<_l_>', 'base', '<d>', '<_r_>'],\n",
    "                       ['<_l_>', 'link', '<d>', '<_r_>'],\n",
    "                       ['<_l_>', 'meta', '<d>', '<_r_>'], ['<style_tag>'],\n",
    "                       ['<title_tag>'], ['<script_tag>']],\n",
    "    '<head_tag>': [[\n",
    "        '<_l_>', 'head', '<d>', '<_r_>', '<head_content-1>', '<_cl_>', 'head',\n",
    "        '<_r_>'\n",
    "    ]],\n",
    "    '<heading>': [['<h1_tag>'], ['<h2_tag>'], ['<h3_tag>'], ['<h4_tag>'],\n",
    "                  ['<h5_tag>'], ['<h6_tag>']],\n",
    "    '<html_content>': [['<head_tag>', '<body_tag>'],\n",
    "                       ['<head_tag>', '<frameset_tag>']],\n",
    "    '<html_document>': [['<html_tag>']],\n",
    "    '<html_tag>': [[\n",
    "        '<_l_>', 'html', '<_r_>', '<html_content>', '<_cl_>', 'html', '<_r_>'\n",
    "    ]],\n",
    "    '<i_tag>': [[\n",
    "        '<_l_>', 'i', '<d>', '<_r_>', '<text>', '<_cl_>', 'i', '<_r_>'\n",
    "    ]],\n",
    "    '<ilayer_tag>': [[\n",
    "        '<_l_>', 'ilayer', '<d>', '<_r_>', '<body_content>', '<_cl_>',\n",
    "        'ilayer', '<_r_>'\n",
    "    ]],\n",
    "    '<ins_tag>': [[\n",
    "        '<_l_>', 'ins', '<d>', '<_r_>', '<flow>', '<_cl_>', 'ins', '<_r_>'\n",
    "    ]],\n",
    "    '<kbd_tag>': [[\n",
    "        '<_l_>', 'kbd', '<d>', '<_r_>', '<text>', '<_cl_>', 'kbd', '<_r_>'\n",
    "    ]],\n",
    "    '<label_content>': [['<_l_>', 'input', '<d>', '<_r_>'], ['<body_content>'],\n",
    "                        ['<select_tag>'], ['<textarea_tag>']],\n",
    "    '<label_tag>': [[\n",
    "        '<_l_>', 'label', '<d>', '<_r_>', '<label_content-1>', '<_cl_>',\n",
    "        'label', '<_r_>'\n",
    "    ]],\n",
    "    '<layer_tag>': [[\n",
    "        '<_l_>', 'layer', '<d>', '<_r_>', '<body_content>', '<_cl_>', 'layer',\n",
    "        '<_r_>'\n",
    "    ]],\n",
    "    '<legend_tag>': [[\n",
    "        '<_l_>', 'legend', '<d>', '<_r_>', '<text>', '<_cl_>', 'legend',\n",
    "        '<_r_>'\n",
    "    ]],\n",
    "    '<li_tag>': [[\n",
    "        '<_l_>', 'li', '<d>', '<_r_>', '<flow>', '<_cl_>', 'li', '<_r_>'\n",
    "    ]],\n",
    "    '<literal_text>': [['<plain_text>']],\n",
    "    '<listing_tag>': [[\n",
    "        '<_l_>', 'listing', '<d>', '<_r_>', '<literal_text>', '<_cl_>',\n",
    "        'listing', '<_r_>'\n",
    "    ]],\n",
    "    '<map_content>': [['<area-1>']],\n",
    "    '<map_tag>': [[\n",
    "        '<_l_>', 'map', '<d>', '<_r_>', '<map_content>', '<_cl_>', 'map',\n",
    "        '<_r_>'\n",
    "    ]],\n",
    "    '<marquee_tag>': [[\n",
    "        '<_l_>', 'marquee', '<d>', '<_r_>', '<style_text>', '<_cl_>',\n",
    "        'marquee', '<_r_>'\n",
    "    ]],\n",
    "    '<menu_tag>': [[\n",
    "        '<_l_>', 'menu', '<d>', '<_r_>', '<li_tag-2>', '<_cl_>', 'menu',\n",
    "        '<_r_>'\n",
    "    ]],\n",
    "    '<multicol_tag>': [[\n",
    "        '<_l_>', 'multicol', '<d>', '<_r_>', '<body_content>', '<_cl_>',\n",
    "        'multicol', '<_r_>'\n",
    "    ]],\n",
    "    '<nobr_tag>': [[\n",
    "        '<_l_>', 'nobr', '<d>', '<_r_>', '<text>', '<_cl_>', 'nobr', '<_r_>'\n",
    "    ]],\n",
    "    '<noembed_tag>': [[\n",
    "        '<_l_>', 'noembed', '<d>', '<_r_>', '<text>', '<_cl_>', 'noembed',\n",
    "        '<_r_>'\n",
    "    ]],\n",
    "    '<noframes_tag>': [[\n",
    "        '<_l_>', 'noframes', '<d>', '<_r_>', '<body_content-4>', '<_cl_>',\n",
    "        'noframes', '<_r_>'\n",
    "    ]],\n",
    "    '<noscript_tag>': [[\n",
    "        '<_l_>', 'noscript', '<d>', '<_r_>', '<text>', '<_cl_>', 'noscript',\n",
    "        '<_r_>'\n",
    "    ]],\n",
    "    '<object_content>': [['<param-2>', '<body_content>']],\n",
    "    '<object_tag>': [[\n",
    "        '<_l_>', 'object', '<d>', '<_r_>', '<object_content>', '<_cl_>',\n",
    "        'object', '<_r_>'\n",
    "    ]],\n",
    "    '<ol_tag>': [[\n",
    "        '<_l_>', 'ol', '<d>', '<_r_>', '<li_tag-3>', '<_cl_>', 'ol', '<_r_>'\n",
    "    ]],\n",
    "    '<optgroup_tag>': [[\n",
    "        '<_l_>', 'optgroup', '<d>', '<_r_>', '<option_tag-1>', '<_cl_>',\n",
    "        'optgroup', '<_r_>'\n",
    "    ]],\n",
    "    '<option_tag>': [[\n",
    "        '<_l_>', 'option', '<d>', '<_r_>', '<plain_text-1>', '<_cl_>',\n",
    "        'option', '<_r_>'\n",
    "    ]],\n",
    "    '<p_tag>': [['<_l_>', 'p', '<_r_>', '<text>', '<_cl_>', 'p', '<_r_>']],\n",
    "    '<param>': [['<_l_>', 'param', '<_r_>']],\n",
    "    '<plain_text>': [['<entity-1>']],\n",
    "    '<entity>': [['<char>'], ['<ampersand>']],\n",
    "    '<char>': [['7'], ['*'], [':'], [']'], ['n'], ['m'], ['N'], ['/'], ['.'],\n",
    "               ['K'], ['T'], ['I'], ['f'], ['o'], [','], ['l'], ['W'], ['-'],\n",
    "               ['?'], ['\\\\'], ['%'], ['1'], ['c'], ['H'], ['!'], ['A'], ['$'],\n",
    "               ['9'], ['q'], ['['], [')'], [' '], [';'], ['b'], ['i'], ['L'],\n",
    "               [\"'\"], ['Y'], ['\\t'], ['3'], ['g'], ['F'], ['E'], ['D'], ['C'],\n",
    "               ['@'], ['t'], ['R'], ['\"'], ['2'], ['}'], ['~'], ['5'], ['4'],\n",
    "               ['z'], ['X'], ['S'], ['O'], ['v'], ['J'], ['`'], ['B'], ['\\n'],\n",
    "               ['y'], ['p'], ['6'], ['0'], ['k'], ['w'], ['\\r'], ['V'], ['_'],\n",
    "               ['s'], ['x'], ['{'], ['d'], ['a'], ['#'], ['Q'], ['<'], ['u'],\n",
    "               ['r'], ['U'], ['h'], ['>'], ['('], ['P'], ['G'], ['\\x0c'],\n",
    "               ['Z'], ['j'], ['|'], ['e'], ['^'], ['='], ['8'], ['+'], ['M']],\n",
    "    '<ampersand>': [['&nbsp;']],\n",
    "    '<physical_style>': [['<b_tag>'], ['<bdo_tag>'], ['<big_tag>'],\n",
    "                         ['<blink_tag>'], ['<font_tag>'], ['<i_tag>'],\n",
    "                         ['<s_tag>'], ['<small_tag>'], ['<span_tag>'],\n",
    "                         ['<strike_tag>'], ['<sub_tag>'], ['<sup_tag>'],\n",
    "                         ['<tt_tag>'], ['<u_tag>']],\n",
    "    '<pre_content>': [['<_l_>', 'br', '<_r_>'], ['<_l_>', 'hr', '<_r_>'],\n",
    "                      ['<a_tag>'], ['<style_text>']],\n",
    "    '<pre_tag>': [[\n",
    "        '<_l_>', 'pre', '<_r_>', '<pre_content-1>', '<_cl_>', 'pre', '<_r_>'\n",
    "    ]],\n",
    "    '<q_tag>': [['<_l_>', 'q', '<_r_>', '<text>', '<_cl_>', 'q', '<_r_>']],\n",
    "    '<s_tag>': [['<_l_>', 's', '<_r_>', '<text>', '<_cl_>', 's', '<_r_>']],\n",
    "    '<script_tag>': [[\n",
    "        '<_l_>', 'script', '<d>', '<_r_>', '<plain_text>', '<_cl_>', 'script',\n",
    "        '<_r_>'\n",
    "    ]],\n",
    "    '<select_content>': [['<optgroup_tag>'], ['<option_tag>']],\n",
    "    '<select_tag>': [[\n",
    "        '<_l_>', 'select', '<d>', '<_r_>', '<select_content-1>', '<_cl_>',\n",
    "        'select', '<_r_>'\n",
    "    ]],\n",
    "    '<small_tag>': [[\n",
    "        '<_l_>', 'small', '<d>', '<_r_>', '<text>', '<_cl_>', 'small', '<_r_>'\n",
    "    ]],\n",
    "    '<span_tag>': [[\n",
    "        '<_l_>', 'span', '<d>', '<_r_>', '<text>', '<_cl_>', 'span', '<_r_>'\n",
    "    ]],\n",
    "    '<strike_tag>': [[\n",
    "        '<_l_>', 'strike', '<d>', '<_r_>', '<text>', '<_cl_>', 'strike',\n",
    "        '<_r_>'\n",
    "    ]],\n",
    "    '<strong_tag>': [[\n",
    "        '<_l_>', 'strong', '<d>', '<_r_>', '<text>', '<_cl_>', 'strong',\n",
    "        '<_r_>'\n",
    "    ]],\n",
    "    '<style_tag>': [[\n",
    "        '<_l_>', 'style', '<d>', '<_r_>', '<plain_text>', '<_cl_>', 'style',\n",
    "        '<_r_>'\n",
    "    ]],\n",
    "    '<style_text>': [['<plain_text>']],\n",
    "    '<sub_tag>': [[\n",
    "        '<_l_>', 'sub', '<d>', '<_r_>', '<text>', '<_cl_>', 'sub', '<_r_>'\n",
    "    ]],\n",
    "    '<sup_tag>': [[\n",
    "        '<_l_>', 'sup', '<d>', '<_r_>', '<text>', '<_cl_>', 'sup', '<_r_>'\n",
    "    ]],\n",
    "    '<table_cell>': [['<td_tag>'], ['<th_tag>']],\n",
    "    '<table_content>': [['<_l_>', 'tbody', '<d>', '<_r_>'],\n",
    "                        ['<_l_>', 'tfoot', '<d>', '<_r_>'],\n",
    "                        ['<_l_>', 'thead', '<d>', '<_r_>'], ['<tr_tag>']],\n",
    "    '<table_tag>': [[\n",
    "        '<_l_>', 'table', '<d>', '<_r_>', '<caption_tag-1>',\n",
    "        '<colgroup_tag-1>', '<table_content-1>', '<_cl_>', 'table', '<_r_>'\n",
    "    ]],\n",
    "    '<td_tag>': [[\n",
    "        '<_l_>', 'td', '<d>', '<_r_>', '<body_content>', '<_cl_>', 'td',\n",
    "        '<_r_>'\n",
    "    ]],\n",
    "    '<text>': [['<text_content-1>']],\n",
    "    '<text_content>': [['<_l_>', 'br', '<d>', '<_r_>'],\n",
    "                       ['<_l_>', 'embed', '<d>', '<_r_>'],\n",
    "                       ['<_l_>', 'iframe', '<d>', '<_r_>'],\n",
    "                       ['<_l_>', 'img', '<d>', '<_r_>'],\n",
    "                       ['<_l_>', 'spacer', '<d>', '<_r_>'],\n",
    "                       ['<_l_>', 'wbr', '<d>', '<_r_>'], ['<a_tag>'],\n",
    "                       ['<applet_tag>'], ['<content_style>'], ['<ilayer_tag>'],\n",
    "                       ['<noembed_tag>'], ['<noscript_tag>'], ['<object_tag>'],\n",
    "                       ['<plain_text>'], ['<physical_style>']],\n",
    "    '<textarea_tag>': [[\n",
    "        '<_l_>', 'textarea', '<d>', '<_r_>', '<plain_text>', '<_cl_>',\n",
    "        'textarea', '<_r_>'\n",
    "    ]],\n",
    "    '<th_tag>': [[\n",
    "        '<_l_>', 'th', '<d>', '<_r_>', '<body_content>', '<_cl_>', 'th',\n",
    "        '<_r_>'\n",
    "    ]],\n",
    "    '<title_tag>': [[\n",
    "        '<_l_>', 'title', '<d>', '<_r_>', '<plain_text>', '<_cl_>', 'title',\n",
    "        '<_r_>'\n",
    "    ]],\n",
    "    '<tr_tag>': [[\n",
    "        '<_l_>', 'tr', '<d>', '<_r_>', '<table_cell-1>', '<_cl_>', 'tr',\n",
    "        '<_r_>'\n",
    "    ]],\n",
    "    '<tt_tag>': [[\n",
    "        '<_l_>', 'tt', '<d>', '<_r_>', '<text>', '<_cl_>', 'tt', '<_r_>'\n",
    "    ]],\n",
    "    '<u_tag>': [[\n",
    "        '<_l_>', 'u', '<d>', '<_r_>', '<text>', '<_cl_>', 'u', '<_r_>'\n",
    "    ]],\n",
    "    '<ul_tag>': [[\n",
    "        '<_l_>', 'ul', '<d>', '<_r_>', '<li_tag-4>', '<_cl_>', 'ul', '<_r_>'\n",
    "    ]],\n",
    "    '<var_tag>': [[\n",
    "        '<_l_>', 'var', '<d>', '<_r_>', '<text>', '<_cl_>', 'var', '<_r_>'\n",
    "    ]],\n",
    "    '<xmp_tag>': [[\n",
    "        '<_l_>', 'xmp', '<d>', '<_r_>', '<literal_text>', '<_cl_>', 'xmp',\n",
    "        '<_r_>'\n",
    "    ]],\n",
    "    '<d>': [['<space-1>', '<attributes-1>', '<space-2>'], []],\n",
    "    '<attribute>': [['<key>'], ['<key>', '=\"', '<value>', '\"'],\n",
    "                    ['<key>', \"='\", '<value>', \"'\"],\n",
    "                    ['<key>', '=', '<uqvalue>']],\n",
    "    '<key>': [['<allchars>']],\n",
    "    '<allchars>': [\n",
    "        ['7'], ['*'], [':'], ['&'], [']'], ['n'], ['m'], ['N'], ['.'], ['K'],\n",
    "        ['T'], ['I'], ['f'], ['o'], [','], ['l'], ['W'], ['-'], ['?'], ['\\\\'],\n",
    "        ['%'], ['1'], ['c'], ['H'], ['!'], ['A'], ['$'], ['9'], ['q'], ['['],\n",
    "        [')'], [';'], ['b'], ['i'], ['L'], ['Y'], ['3'], ['g'], ['F'], ['E'],\n",
    "        ['D'], ['C'], ['@'], ['t'], ['R'], ['2'], ['}'], ['~'], ['5'], ['4'],\n",
    "        ['z'], ['X'], ['S'], ['O'], ['v'], ['J'], ['`'], ['B'], ['y'], ['p'],\n",
    "        ['6'], ['0'], ['k'], ['w'], ['\\r'], ['V'], ['_'], ['s'], ['x'], ['{'],\n",
    "        ['d'], ['a'], ['#'], ['Q'], ['u'], ['r'], ['U'], ['h'], ['('], ['P'],\n",
    "        ['G'], ['\\x0c'], ['Z'], ['j'], ['|'], ['e'], ['^'], ['8'], ['+'],\n",
    "        ['M']\n",
    "    ],\n",
    "    '<value>': [['<anychars>']],\n",
    "    '<anychar>': [['0'], ['1'], ['2'], ['3'], ['4'], ['5'], ['6'], ['7'],\n",
    "                  ['8'], ['9'], ['a'], ['b'], ['c'], ['d'], ['e'], ['f'\n",
    "                                                                    ], ['g'],\n",
    "                  ['h'], ['i'], ['j'], ['k'], ['l'], ['m'], ['n'], ['o'\n",
    "                                                                    ], ['p'],\n",
    "                  ['q'], ['r'], ['s'], ['t'], ['u'], ['v'], ['w'], ['x'\n",
    "                                                                    ], ['y'],\n",
    "                  ['z'], ['A'], ['B'], ['C'], ['D'], ['E'], ['F'], ['G'\n",
    "                                                                    ], ['H'],\n",
    "                  ['I'], ['J'], ['K'], ['L'], ['M'], ['N'], ['O'], ['P'\n",
    "                                                                    ], ['Q'],\n",
    "                  ['R'], ['S'], ['T'], ['U'], ['V'], ['W'], ['X'], ['Y'\n",
    "                                                                    ], ['Z'],\n",
    "                  ['!'], ['\"'], ['#'], ['$'], ['%'], ['&'], [\"'\"], ['('\n",
    "                                                                    ], [')'],\n",
    "                  ['*'], ['+'], [','], ['-'], ['.'], ['/'], [':'], [';'\n",
    "                                                                    ], ['<'],\n",
    "                  ['='], ['>'], ['?'], ['@'], ['['], ['\\\\'], [']'], ['^'],\n",
    "                  ['_'], ['`'], ['{'], ['|'], ['}'], ['~'], [' '], ['\\t'],\n",
    "                  ['\\n'], ['\\r'], ['\\x0b'], ['\\x0c']],\n",
    "    '<anychars>': [['<anychar-1>']],\n",
    "    '<uqvalue>': [['<uqchars>']],\n",
    "    '<uqchar>': [['7'], ['*'], [':'], ['&'], [']'], ['n'], ['m'], ['N'], ['.'],\n",
    "                 ['K'], ['T'], ['I'], ['f'], ['o'], [','], ['l'], ['W'], ['-'],\n",
    "                 ['?'], ['\\\\'], ['%'], ['1'], ['c'], ['H'], ['!'], ['A'],\n",
    "                 ['$'], ['9'], ['q'], ['['], [')'], [';'], ['b'], ['i'], ['L'],\n",
    "                 ['Y'], ['3'], ['g'], ['F'], ['E'], ['D'], ['C'], ['@'], ['t'],\n",
    "                 ['R'], ['2'], ['}'], ['~'], ['5'], ['4'], ['z'], ['X'], ['S'],\n",
    "                 ['O'], ['v'], ['J'], ['B'], ['y'], ['p'], ['6'], ['0'], ['k'],\n",
    "                 ['w'], ['\\r'], ['V'], ['_'], ['s'], ['x'], ['{'], ['d'],\n",
    "                 ['a'], ['#'], ['Q'], ['u'], ['r'], ['U'], ['h'], ['('], ['P'],\n",
    "                 ['G'], ['\\x0c'], ['Z'], ['j'], ['|'], ['e'], ['^'], ['8'],\n",
    "                 ['+'], ['M']],\n",
    "    '<uqchars>': [['<uqchar-1>']],\n",
    "    '<attributes>': [['<attribute>'],\n",
    "                     ['<attribute>', '<space-3>', '<attributes>']],\n",
    "    '<space>': [[' '], ['\\t'], ['\\n']],\n",
    "    '<a_content-1>': [[], ['<a_content>', '<a_content-1>']],\n",
    "    '<address_content-1>': [[], ['<address_content>', '<address_content-1>']],\n",
    "    '<param-1>': [[], ['<param>', '<param-1>']],\n",
    "    '<block_content-1>': [[], ['<block_content>', '<block_content-1>']],\n",
    "    '<body_content-1>': [[], ['<body_content>', '<body_content-1>']],\n",
    "    '<body_content-2>': [[], ['<body_content>', '<body_content-2>']],\n",
    "    '<body_content-3>': [[], ['<body_content>', '<body_content-3>']],\n",
    "    '<_r_-1>': [[], ['<_r_>', '<_r_-1>']],\n",
    "    '<li_tag-1>': [['<li_tag>'], ['<li_tag>', '<li_tag-1>']],\n",
    "    '<dl_content-1>': [['<dl_content>'], ['<dl_content>', '<dl_content-1>']],\n",
    "    '<legend_tag-1>': [[], ['<legend_tag>', '<legend_tag-1>']],\n",
    "    '<form_content-1>': [[], ['<form_content>', '<form_content-1>']],\n",
    "    '<flow_content-1>': [[], ['<flow_content>', '<flow_content-1>']],\n",
    "    '<form_content-2>': [[], ['<form_content>', '<form_content-2>']],\n",
    "    '<frameset_content-1>': [[],\n",
    "                             ['<frameset_content>', '<frameset_content-1>']],\n",
    "    '<head_content-1>': [[], ['<head_content>', '<head_content-1>']],\n",
    "    '<label_content-1>': [[], ['<label_content>', '<label_content-1>']],\n",
    "    '<area-1>': [[], ['<area>', '<area-1>']],\n",
    "    '<li_tag-2>': [[], ['<li_tag>', '<li_tag-2>']],\n",
    "    '<body_content-4>': [[], ['<body_content>', '<body_content-4>']],\n",
    "    '<param-2>': [[], ['<param>', '<param-2>']],\n",
    "    '<li_tag-3>': [['<li_tag>'], ['<li_tag>', '<li_tag-3>']],\n",
    "    '<option_tag-1>': [[], ['<option_tag>', '<option_tag-1>']],\n",
    "    '<plain_text-1>': [['<plain_text>'], ['<plain_text>', '<plain_text-1>']],\n",
    "    '<entity-1>': [[], ['<entity>', '<entity-1>']],\n",
    "    '<pre_content-1>': [[], ['<pre_content>', '<pre_content-1>']],\n",
    "    '<select_content-1>': [[], ['<select_content>', '<select_content-1>']],\n",
    "    '<caption_tag-1>': [[], ['<caption_tag>', '<caption_tag-1>']],\n",
    "    '<colgroup_tag-1>': [[], ['<colgroup_tag>', '<colgroup_tag-1>']],\n",
    "    '<table_content-1>': [[], ['<table_content>', '<table_content-1>']],\n",
    "    '<text_content-1>': [[], ['<text_content>', '<text_content-1>']],\n",
    "    '<table_cell-1>': [[], ['<table_cell>', '<table_cell-1>']],\n",
    "    '<li_tag-4>': [[], ['<li_tag>', '<li_tag-4>']],\n",
    "    '<space-1>': [['<space>'], ['<space>', '<space-1>']],\n",
    "    '<attributes-1>': [[], ['<attributes>', '<attributes-1>']],\n",
    "    '<space-2>': [[], ['<space>', '<space-2>']],\n",
    "    '<anychar-1>': [[], ['<anychar>', '<anychar-1>']],\n",
    "    '<uqchar-1>': [['<uqchar>'], ['<uqchar>', '<uqchar-1>']],\n",
    "    \n",
    "    '<space-3>': [['<space>'], ['<space>', '<space-3>']]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Warning -- set to False if not HTML\n",
    "IS_HTML = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     173
    ]
   },
   "outputs": [],
   "source": [
    "HTML_KEY_RECURSION = {'<start>': False,\n",
    " '<_l_>': False,\n",
    " '<_r_>': False,\n",
    " '<_cl_>': False,\n",
    " '<a_tag>': False,\n",
    " '<a_content>': False,\n",
    " '<abbr_tag>': False,\n",
    " '<acronym_tag>': False,\n",
    " '<address_tag>': False,\n",
    " '<address_content>': False,\n",
    " '<applet_content>': False,\n",
    " '<area>': False,\n",
    " '<applet_tag>': False,\n",
    " '<b_tag>': False,\n",
    " '<basefont_tag>': False,\n",
    " '<bdo_tag>': False,\n",
    " '<big_tag>': False,\n",
    " '<blink_tag>': False,\n",
    " '<block>': False,\n",
    " '<block_content>': False,\n",
    " '<blockquote_tag>': False,\n",
    " '<body_content>': False,\n",
    " '<body_tag>': False,\n",
    " '<caption_tag>': False,\n",
    " '<center_tag>': False,\n",
    " '<cite_tag>': False,\n",
    " '<code_tag>': False,\n",
    " '<colgroup_content>': False,\n",
    " '<colgroup_tag>': False,\n",
    " '<content_style>': False,\n",
    " '<dd_tag>': False,\n",
    " '<del_tag>': False,\n",
    " '<dfn_tag>': False,\n",
    " '<dir_tag>': False,\n",
    " '<div_tag>': False,\n",
    " '<dl_content>': False,\n",
    " '<dl_tag>': False,\n",
    " '<dt_tag>': False,\n",
    " '<em_tag>': False,\n",
    " '<fieldset_tag>': False,\n",
    " '<flow>': False,\n",
    " '<flow_content>': False,\n",
    " '<font_tag>': False,\n",
    " '<form_content>': False,\n",
    " '<form_tag>': False,\n",
    " '<frameset_content>': False,\n",
    " '<frameset_tag>': False,\n",
    " '<h1_tag>': False,\n",
    " '<h2_tag>': False,\n",
    " '<h3_tag>': False,\n",
    " '<h4_tag>': False,\n",
    " '<h5_tag>': False,\n",
    " '<h6_tag>': False,\n",
    " '<head_content>': False,\n",
    " '<head_tag>': False,\n",
    " '<heading>': False,\n",
    " '<html_content>': False,\n",
    " '<html_document>': False,\n",
    " '<html_tag>': False,\n",
    " '<i_tag>': False,\n",
    " '<ilayer_tag>': False,\n",
    " '<ins_tag>': False,\n",
    " '<kbd_tag>': False,\n",
    " '<label_content>': False,\n",
    " '<label_tag>': False,\n",
    " '<layer_tag>': False,\n",
    " '<legend_tag>': False,\n",
    " '<li_tag>': False,\n",
    " '<literal_text>': False,\n",
    " '<listing_tag>': False,\n",
    " '<map_content>': False,\n",
    " '<map_tag>': False,\n",
    " '<marquee_tag>': False,\n",
    " '<menu_tag>': False,\n",
    " '<multicol_tag>': False,\n",
    " '<nobr_tag>': False,\n",
    " '<noembed_tag>': False,\n",
    " '<noframes_tag>': False,\n",
    " '<noscript_tag>': False,\n",
    " '<object_content>': False,\n",
    " '<object_tag>': False,\n",
    " '<ol_tag>': False,\n",
    " '<optgroup_tag>': False,\n",
    " '<option_tag>': False,\n",
    " '<p_tag>': False,\n",
    " '<param>': False,\n",
    " '<plain_text>': False,\n",
    " '<entity>': False,\n",
    " '<char>': False,\n",
    " '<ampersand>': False,\n",
    " '<physical_style>': False,\n",
    " '<pre_content>': False,\n",
    " '<pre_tag>': False,\n",
    " '<q_tag>': False,\n",
    " '<s_tag>': False,\n",
    " '<script_tag>': False,\n",
    " '<select_content>': False,\n",
    " '<select_tag>': False,\n",
    " '<small_tag>': False,\n",
    " '<span_tag>': False,\n",
    " '<strike_tag>': False,\n",
    " '<strong_tag>': False,\n",
    " '<style_tag>': False,\n",
    " '<style_text>': False,\n",
    " '<sub_tag>': False,\n",
    " '<sup_tag>': False,\n",
    " '<table_cell>': False,\n",
    " '<table_content>': False,\n",
    " '<table_tag>': False,\n",
    " '<td_tag>': False,\n",
    " '<text>': False,\n",
    " '<text_content>': False,\n",
    " '<textarea_tag>': False,\n",
    " '<th_tag>': False,\n",
    " '<title_tag>': False,\n",
    " '<tr_tag>': False,\n",
    " '<tt_tag>': False,\n",
    " '<u_tag>': False,\n",
    " '<ul_tag>': False,\n",
    " '<var_tag>': False,\n",
    " '<xmp_tag>': False,\n",
    " '<d>': False,\n",
    " '<attribute>': False,\n",
    " '<key>': False,\n",
    " '<allchars>': False,\n",
    " '<value>': False,\n",
    " '<anychar>': False,\n",
    " '<anychars>': False,\n",
    " '<uqvalue>': False,\n",
    " '<uqchar>': False,\n",
    " '<uqchars>': False,\n",
    " '<attributes>': True,\n",
    " '<space>': False,\n",
    " '<a_content_1>': True,\n",
    " '<address_content_1>': True,\n",
    " '<param_1>': True,\n",
    " '<block_content_1>': True,\n",
    " '<body_content_1>': True,\n",
    " '<body_content_2>': True,\n",
    " '<body_content_3>': True,\n",
    " '<_r__1>': True,\n",
    " '<li_tag_1>': True,\n",
    " '<dl_content_1>': True,\n",
    " '<legend_tag_1>': True,\n",
    " '<form_content_1>': True,\n",
    " '<flow_content_1>': True,\n",
    " '<form_content_2>': True,\n",
    " '<frameset_content_1>': True,\n",
    " '<head_content_1>': True,\n",
    " '<label_content_1>': True,\n",
    " '<area_1>': True,\n",
    " '<li_tag_2>': True,\n",
    " '<body_content_4>': True,\n",
    " '<param_2>': True,\n",
    " '<li_tag_3>': True,\n",
    " '<option_tag_1>': True,\n",
    " '<plain_text_1>': True,\n",
    " '<entity_1>': True,\n",
    " '<pre_content_1>': True,\n",
    " '<select_content_1>': True,\n",
    " '<caption_tag_1>': True,\n",
    " '<colgroup_tag_1>': True,\n",
    " '<table_content_1>': True,\n",
    " '<text_content_1>': True,\n",
    " '<table_cell_1>': True,\n",
    " '<li_tag_4>': True,\n",
    " '<space_1>': True,\n",
    " '<attributes_1>': True,\n",
    " '<space_2>': True,\n",
    " '<anychar_1>': True,\n",
    " '<uqchar_1>': True,\n",
    " '<space_3>': True}\n",
    "\n",
    "HTML_RULE_RECURSION = {'gen_start_0': False,\n",
    " 'gen__l__0': False,\n",
    " 'gen__r__0': False,\n",
    " 'gen__cl__0': False,\n",
    " 'gen_a_tag_0': True,\n",
    " 'gen_a_content_0': True,\n",
    " 'gen_a_content_1': True,\n",
    " 'gen_abbr_tag_0': True,\n",
    " 'gen_acronym_tag_0': True,\n",
    " 'gen_address_tag_0': True,\n",
    " 'gen_address_content_0': True,\n",
    " 'gen_address_content_1': True,\n",
    " 'gen_applet_content_0': True,\n",
    " 'gen_area_0': False,\n",
    " 'gen_applet_tag_0': True,\n",
    " 'gen_b_tag_0': True,\n",
    " 'gen_basefont_tag_0': True,\n",
    " 'gen_bdo_tag_0': True,\n",
    " 'gen_big_tag_0': True,\n",
    " 'gen_blink_tag_0': True,\n",
    " 'gen_block_0': True,\n",
    " 'gen_block_content_0': True,\n",
    " 'gen_block_content_1': True,\n",
    " 'gen_block_content_2': True,\n",
    " 'gen_block_content_3': True,\n",
    " 'gen_block_content_4': True,\n",
    " 'gen_block_content_5': True,\n",
    " 'gen_block_content_6': True,\n",
    " 'gen_block_content_7': True,\n",
    " 'gen_block_content_8': False,\n",
    " 'gen_block_content_9': False,\n",
    " 'gen_block_content_10': True,\n",
    " 'gen_block_content_11': True,\n",
    " 'gen_block_content_12': True,\n",
    " 'gen_block_content_13': True,\n",
    " 'gen_block_content_14': True,\n",
    " 'gen_block_content_15': True,\n",
    " 'gen_block_content_16': True,\n",
    " 'gen_blockquote_tag_0': True,\n",
    " 'gen_body_content_0': False,\n",
    " 'gen_body_content_1': False,\n",
    " 'gen_body_content_2': True,\n",
    " 'gen_body_content_3': True,\n",
    " 'gen_body_content_4': True,\n",
    " 'gen_body_content_5': True,\n",
    " 'gen_body_content_6': True,\n",
    " 'gen_body_content_7': False,\n",
    " 'gen_body_content_8': False,\n",
    " 'gen_body_content_9': True,\n",
    " 'gen_body_content_10': True,\n",
    " 'gen_body_tag_0': False,\n",
    " 'gen_caption_tag_0': True,\n",
    " 'gen_center_tag_0': True,\n",
    " 'gen_cite_tag_0': True,\n",
    " 'gen_code_tag_0': True,\n",
    " 'gen_colgroup_content_0': False,\n",
    " 'gen_colgroup_tag_0': False,\n",
    " 'gen_content_style_0': True,\n",
    " 'gen_content_style_1': True,\n",
    " 'gen_content_style_2': True,\n",
    " 'gen_content_style_3': True,\n",
    " 'gen_content_style_4': True,\n",
    " 'gen_content_style_5': True,\n",
    " 'gen_content_style_6': True,\n",
    " 'gen_content_style_7': True,\n",
    " 'gen_content_style_8': True,\n",
    " 'gen_content_style_9': True,\n",
    " 'gen_dd_tag_0': True,\n",
    " 'gen_del_tag_0': True,\n",
    " 'gen_dfn_tag_0': True,\n",
    " 'gen_dir_tag_0': True,\n",
    " 'gen_div_tag_0': True,\n",
    " 'gen_dl_content_0': True,\n",
    " 'gen_dl_tag_0': True,\n",
    " 'gen_dt_tag_0': True,\n",
    " 'gen_em_tag_0': True,\n",
    " 'gen_fieldset_tag_0': True,\n",
    " 'gen_flow_0': True,\n",
    " 'gen_flow_content_0': True,\n",
    " 'gen_flow_content_1': True,\n",
    " 'gen_font_tag_0': False,\n",
    " 'gen_form_content_0': False,\n",
    " 'gen_form_content_1': False,\n",
    " 'gen_form_content_2': True,\n",
    " 'gen_form_content_3': True,\n",
    " 'gen_form_content_4': True,\n",
    " 'gen_form_content_5': False,\n",
    " 'gen_form_content_6': False,\n",
    " 'gen_form_tag_0': True,\n",
    " 'gen_frameset_content_0': False,\n",
    " 'gen_frameset_content_1': False,\n",
    " 'gen_frameset_tag_0': False,\n",
    " 'gen_h1_tag_0': True,\n",
    " 'gen_h2_tag_0': True,\n",
    " 'gen_h3_tag_0': True,\n",
    " 'gen_h4_tag_0': True,\n",
    " 'gen_h5_tag_0': True,\n",
    " 'gen_h6_tag_0': True,\n",
    " 'gen_head_content_0': False,\n",
    " 'gen_head_content_1': False,\n",
    " 'gen_head_content_2': False,\n",
    " 'gen_head_content_3': False,\n",
    " 'gen_head_content_4': False,\n",
    " 'gen_head_content_5': False,\n",
    " 'gen_head_tag_0': False,\n",
    " 'gen_heading_0': True,\n",
    " 'gen_heading_1': True,\n",
    " 'gen_heading_2': True,\n",
    " 'gen_heading_3': True,\n",
    " 'gen_heading_4': True,\n",
    " 'gen_heading_5': True,\n",
    " 'gen_html_content_0': False,\n",
    " 'gen_html_content_1': False,\n",
    " 'gen_html_document_0': False,\n",
    " 'gen_html_tag_0': False,\n",
    " 'gen_i_tag_0': True,\n",
    " 'gen_ilayer_tag_0': True,\n",
    " 'gen_ins_tag_0': True,\n",
    " 'gen_kbd_tag_0': True,\n",
    " 'gen_label_content_0': False,\n",
    " 'gen_label_content_1': True,\n",
    " 'gen_label_content_2': False,\n",
    " 'gen_label_content_3': False,\n",
    " 'gen_label_tag_0': True,\n",
    " 'gen_layer_tag_0': True,\n",
    " 'gen_legend_tag_0': True,\n",
    " 'gen_li_tag_0': True,\n",
    " 'gen_literal_text_0': False,\n",
    " 'gen_listing_tag_0': False,\n",
    " 'gen_map_content_0': False,\n",
    " 'gen_map_tag_0': False,\n",
    " 'gen_marquee_tag_0': False,\n",
    " 'gen_menu_tag_0': True,\n",
    " 'gen_multicol_tag_0': True,\n",
    " 'gen_nobr_tag_0': True,\n",
    " 'gen_noembed_tag_0': True,\n",
    " 'gen_noframes_tag_0': False,\n",
    " 'gen_noscript_tag_0': True,\n",
    " 'gen_object_content_0': True,\n",
    " 'gen_object_tag_0': True,\n",
    " 'gen_ol_tag_0': True,\n",
    " 'gen_optgroup_tag_0': False,\n",
    " 'gen_option_tag_0': False,\n",
    " 'gen_p_tag_0': True,\n",
    " 'gen_param_0': False,\n",
    " 'gen_plain_text_0': False,\n",
    " 'gen_entity_0': False,\n",
    " 'gen_entity_1': False,\n",
    " 'gen_char_0': False,\n",
    " 'gen_char_1': False,\n",
    " 'gen_char_2': False,\n",
    " 'gen_char_3': False,\n",
    " 'gen_char_4': False,\n",
    " 'gen_char_5': False,\n",
    " 'gen_char_6': False,\n",
    " 'gen_char_7': False,\n",
    " 'gen_char_8': False,\n",
    " 'gen_char_9': False,\n",
    " 'gen_char_10': False,\n",
    " 'gen_char_11': False,\n",
    " 'gen_char_12': False,\n",
    " 'gen_char_13': False,\n",
    " 'gen_char_14': False,\n",
    " 'gen_char_15': False,\n",
    " 'gen_char_16': False,\n",
    " 'gen_char_17': False,\n",
    " 'gen_char_18': False,\n",
    " 'gen_char_19': False,\n",
    " 'gen_char_20': False,\n",
    " 'gen_char_21': False,\n",
    " 'gen_char_22': False,\n",
    " 'gen_char_23': False,\n",
    " 'gen_char_24': False,\n",
    " 'gen_char_25': False,\n",
    " 'gen_char_26': False,\n",
    " 'gen_char_27': False,\n",
    " 'gen_char_28': False,\n",
    " 'gen_char_29': False,\n",
    " 'gen_char_30': False,\n",
    " 'gen_char_31': False,\n",
    " 'gen_char_32': False,\n",
    " 'gen_char_33': False,\n",
    " 'gen_char_34': False,\n",
    " 'gen_char_35': False,\n",
    " 'gen_char_36': False,\n",
    " 'gen_char_37': False,\n",
    " 'gen_char_38': False,\n",
    " 'gen_char_39': False,\n",
    " 'gen_char_40': False,\n",
    " 'gen_char_41': False,\n",
    " 'gen_char_42': False,\n",
    " 'gen_char_43': False,\n",
    " 'gen_char_44': False,\n",
    " 'gen_char_45': False,\n",
    " 'gen_char_46': False,\n",
    " 'gen_char_47': False,\n",
    " 'gen_char_48': False,\n",
    " 'gen_char_49': False,\n",
    " 'gen_char_50': False,\n",
    " 'gen_char_51': False,\n",
    " 'gen_char_52': False,\n",
    " 'gen_char_53': False,\n",
    " 'gen_char_54': False,\n",
    " 'gen_char_55': False,\n",
    " 'gen_char_56': False,\n",
    " 'gen_char_57': False,\n",
    " 'gen_char_58': False,\n",
    " 'gen_char_59': False,\n",
    " 'gen_char_60': False,\n",
    " 'gen_char_61': False,\n",
    " 'gen_char_62': False,\n",
    " 'gen_char_63': False,\n",
    " 'gen_char_64': False,\n",
    " 'gen_char_65': False,\n",
    " 'gen_char_66': False,\n",
    " 'gen_char_67': False,\n",
    " 'gen_char_68': False,\n",
    " 'gen_char_69': False,\n",
    " 'gen_char_70': False,\n",
    " 'gen_char_71': False,\n",
    " 'gen_char_72': False,\n",
    " 'gen_char_73': False,\n",
    " 'gen_char_74': False,\n",
    " 'gen_char_75': False,\n",
    " 'gen_char_76': False,\n",
    " 'gen_char_77': False,\n",
    " 'gen_char_78': False,\n",
    " 'gen_char_79': False,\n",
    " 'gen_char_80': False,\n",
    " 'gen_char_81': False,\n",
    " 'gen_char_82': False,\n",
    " 'gen_char_83': False,\n",
    " 'gen_char_84': False,\n",
    " 'gen_char_85': False,\n",
    " 'gen_char_86': False,\n",
    " 'gen_char_87': False,\n",
    " 'gen_char_88': False,\n",
    " 'gen_char_89': False,\n",
    " 'gen_char_90': False,\n",
    " 'gen_char_91': False,\n",
    " 'gen_char_92': False,\n",
    " 'gen_char_93': False,\n",
    " 'gen_char_94': False,\n",
    " 'gen_char_95': False,\n",
    " 'gen_char_96': False,\n",
    " 'gen_char_97': False,\n",
    " 'gen_ampersand_0': False,\n",
    " 'gen_physical_style_0': False,\n",
    " 'gen_physical_style_1': True,\n",
    " 'gen_physical_style_2': True,\n",
    " 'gen_physical_style_3': True,\n",
    " 'gen_physical_style_4': True,\n",
    " 'gen_physical_style_5': True,\n",
    " 'gen_physical_style_6': True,\n",
    " 'gen_physical_style_7': True,\n",
    " 'gen_physical_style_8': True,\n",
    " 'gen_physical_style_9': True,\n",
    " 'gen_physical_style_10': True,\n",
    " 'gen_physical_style_11': True,\n",
    " 'gen_physical_style_12': True,\n",
    " 'gen_physical_style_13': True,\n",
    " 'gen_pre_content_0': False,\n",
    " 'gen_pre_content_1': False,\n",
    " 'gen_pre_content_2': True,\n",
    " 'gen_pre_content_3': False,\n",
    " 'gen_pre_tag_0': True,\n",
    " 'gen_q_tag_0': True,\n",
    " 'gen_s_tag_0': True,\n",
    " 'gen_script_tag_0': False,\n",
    " 'gen_select_content_0': False,\n",
    " 'gen_select_content_1': False,\n",
    " 'gen_select_tag_0': False,\n",
    " 'gen_small_tag_0': True,\n",
    " 'gen_span_tag_0': True,\n",
    " 'gen_strike_tag_0': True,\n",
    " 'gen_strong_tag_0': True,\n",
    " 'gen_style_tag_0': False,\n",
    " 'gen_style_text_0': False,\n",
    " 'gen_sub_tag_0': True,\n",
    " 'gen_sup_tag_0': True,\n",
    " 'gen_table_cell_0': True,\n",
    " 'gen_table_cell_1': True,\n",
    " 'gen_table_content_0': False,\n",
    " 'gen_table_content_1': False,\n",
    " 'gen_table_content_2': False,\n",
    " 'gen_table_content_3': True,\n",
    " 'gen_table_tag_0': True,\n",
    " 'gen_td_tag_0': True,\n",
    " 'gen_text_0': True,\n",
    " 'gen_text_content_0': False,\n",
    " 'gen_text_content_1': False,\n",
    " 'gen_text_content_2': False,\n",
    " 'gen_text_content_3': False,\n",
    " 'gen_text_content_4': False,\n",
    " 'gen_text_content_5': False,\n",
    " 'gen_text_content_6': True,\n",
    " 'gen_text_content_7': False,\n",
    " 'gen_text_content_8': True,\n",
    " 'gen_text_content_9': True,\n",
    " 'gen_text_content_10': True,\n",
    " 'gen_text_content_11': True,\n",
    " 'gen_text_content_12': True,\n",
    " 'gen_text_content_13': True,\n",
    " 'gen_text_content_14': True,\n",
    " 'gen_textarea_tag_0': False,\n",
    " 'gen_th_tag_0': True,\n",
    " 'gen_title_tag_0': False,\n",
    " 'gen_tr_tag_0': True,\n",
    " 'gen_tt_tag_0': True,\n",
    " 'gen_u_tag_0': True,\n",
    " 'gen_ul_tag_0': True,\n",
    " 'gen_var_tag_0': True,\n",
    " 'gen_xmp_tag_0': False,\n",
    " 'gen_d_0': False,\n",
    " 'gen_d_1': False,\n",
    " 'gen_attribute_0': False,\n",
    " 'gen_attribute_1': False,\n",
    " 'gen_attribute_2': False,\n",
    " 'gen_attribute_3': False,\n",
    " 'gen_key_0': False,\n",
    " 'gen_allchars_0': False,\n",
    " 'gen_allchars_1': False,\n",
    " 'gen_allchars_2': False,\n",
    " 'gen_allchars_3': False,\n",
    " 'gen_allchars_4': False,\n",
    " 'gen_allchars_5': False,\n",
    " 'gen_allchars_6': False,\n",
    " 'gen_allchars_7': False,\n",
    " 'gen_allchars_8': False,\n",
    " 'gen_allchars_9': False,\n",
    " 'gen_allchars_10': False,\n",
    " 'gen_allchars_11': False,\n",
    " 'gen_allchars_12': False,\n",
    " 'gen_allchars_13': False,\n",
    " 'gen_allchars_14': False,\n",
    " 'gen_allchars_15': False,\n",
    " 'gen_allchars_16': False,\n",
    " 'gen_allchars_17': False,\n",
    " 'gen_allchars_18': False,\n",
    " 'gen_allchars_19': False,\n",
    " 'gen_allchars_20': False,\n",
    " 'gen_allchars_21': False,\n",
    " 'gen_allchars_22': False,\n",
    " 'gen_allchars_23': False,\n",
    " 'gen_allchars_24': False,\n",
    " 'gen_allchars_25': False,\n",
    " 'gen_allchars_26': False,\n",
    " 'gen_allchars_27': False,\n",
    " 'gen_allchars_28': False,\n",
    " 'gen_allchars_29': False,\n",
    " 'gen_allchars_30': False,\n",
    " 'gen_allchars_31': False,\n",
    " 'gen_allchars_32': False,\n",
    " 'gen_allchars_33': False,\n",
    " 'gen_allchars_34': False,\n",
    " 'gen_allchars_35': False,\n",
    " 'gen_allchars_36': False,\n",
    " 'gen_allchars_37': False,\n",
    " 'gen_allchars_38': False,\n",
    " 'gen_allchars_39': False,\n",
    " 'gen_allchars_40': False,\n",
    " 'gen_allchars_41': False,\n",
    " 'gen_allchars_42': False,\n",
    " 'gen_allchars_43': False,\n",
    " 'gen_allchars_44': False,\n",
    " 'gen_allchars_45': False,\n",
    " 'gen_allchars_46': False,\n",
    " 'gen_allchars_47': False,\n",
    " 'gen_allchars_48': False,\n",
    " 'gen_allchars_49': False,\n",
    " 'gen_allchars_50': False,\n",
    " 'gen_allchars_51': False,\n",
    " 'gen_allchars_52': False,\n",
    " 'gen_allchars_53': False,\n",
    " 'gen_allchars_54': False,\n",
    " 'gen_allchars_55': False,\n",
    " 'gen_allchars_56': False,\n",
    " 'gen_allchars_57': False,\n",
    " 'gen_allchars_58': False,\n",
    " 'gen_allchars_59': False,\n",
    " 'gen_allchars_60': False,\n",
    " 'gen_allchars_61': False,\n",
    " 'gen_allchars_62': False,\n",
    " 'gen_allchars_63': False,\n",
    " 'gen_allchars_64': False,\n",
    " 'gen_allchars_65': False,\n",
    " 'gen_allchars_66': False,\n",
    " 'gen_allchars_67': False,\n",
    " 'gen_allchars_68': False,\n",
    " 'gen_allchars_69': False,\n",
    " 'gen_allchars_70': False,\n",
    " 'gen_allchars_71': False,\n",
    " 'gen_allchars_72': False,\n",
    " 'gen_allchars_73': False,\n",
    " 'gen_allchars_74': False,\n",
    " 'gen_allchars_75': False,\n",
    " 'gen_allchars_76': False,\n",
    " 'gen_allchars_77': False,\n",
    " 'gen_allchars_78': False,\n",
    " 'gen_allchars_79': False,\n",
    " 'gen_allchars_80': False,\n",
    " 'gen_allchars_81': False,\n",
    " 'gen_allchars_82': False,\n",
    " 'gen_allchars_83': False,\n",
    " 'gen_allchars_84': False,\n",
    " 'gen_allchars_85': False,\n",
    " 'gen_allchars_86': False,\n",
    " 'gen_allchars_87': False,\n",
    " 'gen_allchars_88': False,\n",
    " 'gen_allchars_89': False,\n",
    " 'gen_value_0': False,\n",
    " 'gen_anychar_0': False,\n",
    " 'gen_anychar_1': False,\n",
    " 'gen_anychar_2': False,\n",
    " 'gen_anychar_3': False,\n",
    " 'gen_anychar_4': False,\n",
    " 'gen_anychar_5': False,\n",
    " 'gen_anychar_6': False,\n",
    " 'gen_anychar_7': False,\n",
    " 'gen_anychar_8': False,\n",
    " 'gen_anychar_9': False,\n",
    " 'gen_anychar_10': False,\n",
    " 'gen_anychar_11': False,\n",
    " 'gen_anychar_12': False,\n",
    " 'gen_anychar_13': False,\n",
    " 'gen_anychar_14': False,\n",
    " 'gen_anychar_15': False,\n",
    " 'gen_anychar_16': False,\n",
    " 'gen_anychar_17': False,\n",
    " 'gen_anychar_18': False,\n",
    " 'gen_anychar_19': False,\n",
    " 'gen_anychar_20': False,\n",
    " 'gen_anychar_21': False,\n",
    " 'gen_anychar_22': False,\n",
    " 'gen_anychar_23': False,\n",
    " 'gen_anychar_24': False,\n",
    " 'gen_anychar_25': False,\n",
    " 'gen_anychar_26': False,\n",
    " 'gen_anychar_27': False,\n",
    " 'gen_anychar_28': False,\n",
    " 'gen_anychar_29': False,\n",
    " 'gen_anychar_30': False,\n",
    " 'gen_anychar_31': False,\n",
    " 'gen_anychar_32': False,\n",
    " 'gen_anychar_33': False,\n",
    " 'gen_anychar_34': False,\n",
    " 'gen_anychar_35': False,\n",
    " 'gen_anychar_36': False,\n",
    " 'gen_anychar_37': False,\n",
    " 'gen_anychar_38': False,\n",
    " 'gen_anychar_39': False,\n",
    " 'gen_anychar_40': False,\n",
    " 'gen_anychar_41': False,\n",
    " 'gen_anychar_42': False,\n",
    " 'gen_anychar_43': False,\n",
    " 'gen_anychar_44': False,\n",
    " 'gen_anychar_45': False,\n",
    " 'gen_anychar_46': False,\n",
    " 'gen_anychar_47': False,\n",
    " 'gen_anychar_48': False,\n",
    " 'gen_anychar_49': False,\n",
    " 'gen_anychar_50': False,\n",
    " 'gen_anychar_51': False,\n",
    " 'gen_anychar_52': False,\n",
    " 'gen_anychar_53': False,\n",
    " 'gen_anychar_54': False,\n",
    " 'gen_anychar_55': False,\n",
    " 'gen_anychar_56': False,\n",
    " 'gen_anychar_57': False,\n",
    " 'gen_anychar_58': False,\n",
    " 'gen_anychar_59': False,\n",
    " 'gen_anychar_60': False,\n",
    " 'gen_anychar_61': False,\n",
    " 'gen_anychar_62': False,\n",
    " 'gen_anychar_63': False,\n",
    " 'gen_anychar_64': False,\n",
    " 'gen_anychar_65': False,\n",
    " 'gen_anychar_66': False,\n",
    " 'gen_anychar_67': False,\n",
    " 'gen_anychar_68': False,\n",
    " 'gen_anychar_69': False,\n",
    " 'gen_anychar_70': False,\n",
    " 'gen_anychar_71': False,\n",
    " 'gen_anychar_72': False,\n",
    " 'gen_anychar_73': False,\n",
    " 'gen_anychar_74': False,\n",
    " 'gen_anychar_75': False,\n",
    " 'gen_anychar_76': False,\n",
    " 'gen_anychar_77': False,\n",
    " 'gen_anychar_78': False,\n",
    " 'gen_anychar_79': False,\n",
    " 'gen_anychar_80': False,\n",
    " 'gen_anychar_81': False,\n",
    " 'gen_anychar_82': False,\n",
    " 'gen_anychar_83': False,\n",
    " 'gen_anychar_84': False,\n",
    " 'gen_anychar_85': False,\n",
    " 'gen_anychar_86': False,\n",
    " 'gen_anychar_87': False,\n",
    " 'gen_anychar_88': False,\n",
    " 'gen_anychar_89': False,\n",
    " 'gen_anychar_90': False,\n",
    " 'gen_anychar_91': False,\n",
    " 'gen_anychar_92': False,\n",
    " 'gen_anychar_93': False,\n",
    " 'gen_anychar_94': False,\n",
    " 'gen_anychar_95': False,\n",
    " 'gen_anychar_96': False,\n",
    " 'gen_anychar_97': False,\n",
    " 'gen_anychar_98': False,\n",
    " 'gen_anychar_99': False,\n",
    " 'gen_anychars_0': False,\n",
    " 'gen_uqvalue_0': False,\n",
    " 'gen_uqchar_0': False,\n",
    " 'gen_uqchar_1': False,\n",
    " 'gen_uqchar_2': False,\n",
    " 'gen_uqchar_3': False,\n",
    " 'gen_uqchar_4': False,\n",
    " 'gen_uqchar_5': False,\n",
    " 'gen_uqchar_6': False,\n",
    " 'gen_uqchar_7': False,\n",
    " 'gen_uqchar_8': False,\n",
    " 'gen_uqchar_9': False,\n",
    " 'gen_uqchar_10': False,\n",
    " 'gen_uqchar_11': False,\n",
    " 'gen_uqchar_12': False,\n",
    " 'gen_uqchar_13': False,\n",
    " 'gen_uqchar_14': False,\n",
    " 'gen_uqchar_15': False,\n",
    " 'gen_uqchar_16': False,\n",
    " 'gen_uqchar_17': False,\n",
    " 'gen_uqchar_18': False,\n",
    " 'gen_uqchar_19': False,\n",
    " 'gen_uqchar_20': False,\n",
    " 'gen_uqchar_21': False,\n",
    " 'gen_uqchar_22': False,\n",
    " 'gen_uqchar_23': False,\n",
    " 'gen_uqchar_24': False,\n",
    " 'gen_uqchar_25': False,\n",
    " 'gen_uqchar_26': False,\n",
    " 'gen_uqchar_27': False,\n",
    " 'gen_uqchar_28': False,\n",
    " 'gen_uqchar_29': False,\n",
    " 'gen_uqchar_30': False,\n",
    " 'gen_uqchar_31': False,\n",
    " 'gen_uqchar_32': False,\n",
    " 'gen_uqchar_33': False,\n",
    " 'gen_uqchar_34': False,\n",
    " 'gen_uqchar_35': False,\n",
    " 'gen_uqchar_36': False,\n",
    " 'gen_uqchar_37': False,\n",
    " 'gen_uqchar_38': False,\n",
    " 'gen_uqchar_39': False,\n",
    " 'gen_uqchar_40': False,\n",
    " 'gen_uqchar_41': False,\n",
    " 'gen_uqchar_42': False,\n",
    " 'gen_uqchar_43': False,\n",
    " 'gen_uqchar_44': False,\n",
    " 'gen_uqchar_45': False,\n",
    " 'gen_uqchar_46': False,\n",
    " 'gen_uqchar_47': False,\n",
    " 'gen_uqchar_48': False,\n",
    " 'gen_uqchar_49': False,\n",
    " 'gen_uqchar_50': False,\n",
    " 'gen_uqchar_51': False,\n",
    " 'gen_uqchar_52': False,\n",
    " 'gen_uqchar_53': False,\n",
    " 'gen_uqchar_54': False,\n",
    " 'gen_uqchar_55': False,\n",
    " 'gen_uqchar_56': False,\n",
    " 'gen_uqchar_57': False,\n",
    " 'gen_uqchar_58': False,\n",
    " 'gen_uqchar_59': False,\n",
    " 'gen_uqchar_60': False,\n",
    " 'gen_uqchar_61': False,\n",
    " 'gen_uqchar_62': False,\n",
    " 'gen_uqchar_63': False,\n",
    " 'gen_uqchar_64': False,\n",
    " 'gen_uqchar_65': False,\n",
    " 'gen_uqchar_66': False,\n",
    " 'gen_uqchar_67': False,\n",
    " 'gen_uqchar_68': False,\n",
    " 'gen_uqchar_69': False,\n",
    " 'gen_uqchar_70': False,\n",
    " 'gen_uqchar_71': False,\n",
    " 'gen_uqchar_72': False,\n",
    " 'gen_uqchar_73': False,\n",
    " 'gen_uqchar_74': False,\n",
    " 'gen_uqchar_75': False,\n",
    " 'gen_uqchar_76': False,\n",
    " 'gen_uqchar_77': False,\n",
    " 'gen_uqchar_78': False,\n",
    " 'gen_uqchar_79': False,\n",
    " 'gen_uqchar_80': False,\n",
    " 'gen_uqchar_81': False,\n",
    " 'gen_uqchar_82': False,\n",
    " 'gen_uqchar_83': False,\n",
    " 'gen_uqchar_84': False,\n",
    " 'gen_uqchar_85': False,\n",
    " 'gen_uqchar_86': False,\n",
    " 'gen_uqchar_87': False,\n",
    " 'gen_uqchar_88': False,\n",
    " 'gen_uqchars_0': False,\n",
    " 'gen_attributes_0': False,\n",
    " 'gen_attributes_1': True,\n",
    " 'gen_space_0': False,\n",
    " 'gen_space_1': False,\n",
    " 'gen_space_2': False,\n",
    " 'gen_a_content_1_0': False,\n",
    " 'gen_a_content_1_1': True,\n",
    " 'gen_address_content_1_0': False,\n",
    " 'gen_address_content_1_1': True,\n",
    " 'gen_param_1_0': False,\n",
    " 'gen_param_1_1': True,\n",
    " 'gen_block_content_1_0': False,\n",
    " 'gen_block_content_1_1': True,\n",
    " 'gen_body_content_1_0': False,\n",
    " 'gen_body_content_1_1': True,\n",
    " 'gen_body_content_2_0': False,\n",
    " 'gen_body_content_2_1': True,\n",
    " 'gen_body_content_3_0': False,\n",
    " 'gen_body_content_3_1': True,\n",
    " 'gen__r__1_0': False,\n",
    " 'gen__r__1_1': True,\n",
    " 'gen_li_tag_1_0': True,\n",
    " 'gen_li_tag_1_1': True,\n",
    " 'gen_dl_content_1_0': True,\n",
    " 'gen_dl_content_1_1': True,\n",
    " 'gen_legend_tag_1_0': False,\n",
    " 'gen_legend_tag_1_1': True,\n",
    " 'gen_form_content_1_0': False,\n",
    " 'gen_form_content_1_1': True,\n",
    " 'gen_flow_content_1_0': False,\n",
    " 'gen_flow_content_1_1': True,\n",
    " 'gen_form_content_2_0': False,\n",
    " 'gen_form_content_2_1': True,\n",
    " 'gen_frameset_content_1_0': False,\n",
    " 'gen_frameset_content_1_1': True,\n",
    " 'gen_head_content_1_0': False,\n",
    " 'gen_head_content_1_1': True,\n",
    " 'gen_label_content_1_0': False,\n",
    " 'gen_label_content_1_1': True,\n",
    " 'gen_area_1_0': False,\n",
    " 'gen_area_1_1': True,\n",
    " 'gen_li_tag_2_0': False,\n",
    " 'gen_li_tag_2_1': True,\n",
    " 'gen_body_content_4_0': False,\n",
    " 'gen_body_content_4_1': True,\n",
    " 'gen_param_2_0': False,\n",
    " 'gen_param_2_1': True,\n",
    " 'gen_li_tag_3_0': True,\n",
    " 'gen_li_tag_3_1': True,\n",
    " 'gen_option_tag_1_0': False,\n",
    " 'gen_option_tag_1_1': True,\n",
    " 'gen_plain_text_1_0': False,\n",
    " 'gen_plain_text_1_1': True,\n",
    " 'gen_entity_1_0': False,\n",
    " 'gen_entity_1_1': True,\n",
    " 'gen_pre_content_1_0': False,\n",
    " 'gen_pre_content_1_1': True,\n",
    " 'gen_select_content_1_0': False,\n",
    " 'gen_select_content_1_1': True,\n",
    " 'gen_caption_tag_1_0': False,\n",
    " 'gen_caption_tag_1_1': True,\n",
    " 'gen_colgroup_tag_1_0': False,\n",
    " 'gen_colgroup_tag_1_1': True,\n",
    " 'gen_table_content_1_0': False,\n",
    " 'gen_table_content_1_1': True,\n",
    " 'gen_text_content_1_0': False,\n",
    " 'gen_text_content_1_1': True,\n",
    " 'gen_table_cell_1_0': False,\n",
    " 'gen_table_cell_1_1': True,\n",
    " 'gen_li_tag_4_0': False,\n",
    " 'gen_li_tag_4_1': True,\n",
    " 'gen_space_1_0': False,\n",
    " 'gen_space_1_1': True,\n",
    " 'gen_attributes_1_0': False,\n",
    " 'gen_attributes_1_1': True,\n",
    " 'gen_space_2_0': False,\n",
    " 'gen_space_2_1': True,\n",
    " 'gen_anychar_1_0': False,\n",
    " 'gen_anychar_1_1': True,\n",
    " 'gen_uqchar_1_0': False,\n",
    " 'gen_uqchar_1_1': True,\n",
    " 'gen_space_3_0': False,\n",
    " 'gen_space_3_1': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_grammar = json_grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum depth is 7; beyond that the size goes > 50G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Existing tools\n",
    "* Grammarinator (Python based)\n",
    "* Gramfuzz (Python based)\n",
    "* Dharma (Python based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sanitize:\n",
    "    def __init__(self, g):\n",
    "        self.g = g\n",
    "  \n",
    "    def to_key(self, k):\n",
    "        s = k.replace('-', '_')\n",
    "        s = s.replace('[', 'Osq').replace(']','Csq')\n",
    "        s = s.replace('{','Obr').replace('}','Cbr')\n",
    "        s = s.replace('import','XimportX')\n",
    "        s = s.replace('class', 'XclassX')\n",
    "        s = s.replace('def', 'XdefX')\n",
    "        return s\n",
    "\n",
    "    def to_token(self, t):\n",
    "        return t\n",
    "    \n",
    "    def split_tokens(self, t, grammar):\n",
    "        if t in grammar: return [t]\n",
    "        my_tokens = []\n",
    "        # these should not matter for performance comparisons,\n",
    "        # and makes my life simpler\n",
    "        esc = {'\\r': '\\r', '\\n': '\\n',\n",
    "             '\\\\': '\\\\',\n",
    "             '\"':'\"',\n",
    "             \"'\":\"'\"}\n",
    "        for i in t:\n",
    "            if i in esc:\n",
    "                my_tokens.append(esc[i])\n",
    "            else:\n",
    "                my_tokens.append(i)\n",
    "        return my_tokens\n",
    "            \n",
    "        return list(t)\n",
    "\n",
    "    def to_rule(self, rule, grammar):\n",
    "        tokens = [k for t in rule for k in self.split_tokens(t, grammar)]\n",
    "        return [self.to_token(t) if t not in grammar else self.to_key(t)\n",
    "                for t in tokens]\n",
    "\n",
    "    def translate(self):\n",
    "        new_grammar = {}\n",
    "        for k in self.g:\n",
    "            rules = self.g[k]\n",
    "            new_grammar[self.to_key(k)] = [self.to_rule(rule, self.g) for rule in rules]\n",
    "        return new_grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammarinator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be sure to install the latest from git. The --random-seed argument is only in git HEAD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipkg('grammarinator', 'git+https://github.com/renatahodovan/grammarinator.git')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AntlrG(Sanitize):\n",
    "    def to_key(self, k):\n",
    "        return super().to_key(k)[1:-1]\n",
    "\n",
    "    def esc_token(self, t):\n",
    "        # these are multi-char tokens\n",
    "        t = t.replace('\\\\','\\\\\\\\')\n",
    "        t = t.replace(\"'\",\"\\\\\\'\")\n",
    "        t = t.replace('\\n','\\\\n')\n",
    "        t = t.replace('\\r','\\\\r')\n",
    "        t = t.replace('\\t','\\\\t')\n",
    "        return t\n",
    "\n",
    "    def rule_to_s(self, rule, grammar):\n",
    "        return ' '.join([\"'%s'\" % self.esc_token(t)\n",
    "                         if t not in grammar else self.to_key(t)\n",
    "                         for t in rule])\n",
    "\n",
    "    def translate(self):\n",
    "        lines = ['grammar Grammar;']\n",
    "        for k in self.g:\n",
    "            rules = self.g[k]\n",
    "            v = '\\n    |'.join([self.rule_to_s(rule, self.g)\n",
    "                                for rule in rules])\n",
    "            lines.append('''\\\n",
    "%s\n",
    "    : %s\n",
    "    ;''' % (self.to_key(k), v))\n",
    "        return '\\n'.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g4 = AntlrG(my_grammar).translate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('testers/grammar.g4', 'w+') as f:\n",
    "    print(g4, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grammarinator-process testers/grammar.g4 -o testers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glexer = 'testers/GrammarUnlexer.py'\n",
    "gparser = 'testers/GrammarUnparser.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grammarinator-generate -l testers/GrammarUnlexer.py -p testers/GrammarUnparser.py -r start -n 10 -o tests/ -j 1 --sys-recursion-limit 20900 -d 10 --random-seed 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrammarinatorTester(Tester):\n",
    "    def show_files(self, path):\n",
    "        tests = pathlib.Path(path)\n",
    "        for tf in tests.glob('*'):\n",
    "            with open(tf) as f:\n",
    "                print(repr(f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GrammarinatorTester().show_files('tests')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrammarinatorTester(GrammarinatorTester):\n",
    "    def folder_size(self, path='.'):\n",
    "        def cksum(fn):\n",
    "            v = !cksum {fn}\n",
    "            return (v.fields(0)[0])\n",
    "            #with open(fn) as f: return f.read()\n",
    "        total = 0\n",
    "        num = 0\n",
    "        ufiles = set()\n",
    "        for entry in os.scandir(path):\n",
    "            if entry.is_file():\n",
    "                total += entry.stat().st_size\n",
    "                num += 1\n",
    "                #ufiles.add(cksum(entry.path))\n",
    "            elif entry.is_dir():\n",
    "                raise Exception('Only flat directories expected now.') \n",
    "                # total += self.folder_size(entry.path)\n",
    "        return total, num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GrammarinatorTester().folder_size('tests')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrammarinatorTester(GrammarinatorTester):\n",
    "    def init_run(self):\n",
    "        super().init_run()\n",
    "        !grammarinator-process testers/grammar.g4 -o testers/\n",
    "        \n",
    "    def pre_time(self):\n",
    "        super().pre_time()\n",
    "    \n",
    "    def exec_program(self, seed, max_depth, t):\n",
    "        glexer = 'testers/GrammarUnlexer.py'\n",
    "        gparser = 'testers/GrammarUnparser.py'\n",
    "        # seed, maxnum, max_depth\n",
    "        return f\"grammarinator-generate -l {glexer} -p {gparser}  --random-seed {seed} -n {self.max_num} -d {max_depth}  -r start  -o tests/ -j 1 --sys-recursion-limit 20900\"\n",
    "\n",
    "    def post_time(self):\n",
    "        super().post_time()\n",
    "        self.size, self.lines = self.folder_size(path='tests/')\n",
    "        !rm -rf tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GrammarinatorTester().run_test().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GramFuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fork at `vrthra-forks` contains a few fixes for python 2to3 which have not made to the main repo yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipkg('gramfuzz', 'git+https://github.com/vrthra-forks/gramfuzz.git')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RDefG(AntlrG):\n",
    "    def to_key(self, k):\n",
    "        return 'RRef(\"%s\")' % super().to_key(k)\n",
    " \n",
    "    def esc_token(self, t):\n",
    "        # these are multi-char tokens\n",
    "        t = t.replace('\\\\','\\\\\\\\')\n",
    "        t = t.replace('\"','\\\\\\\"') # main difference from Antlr -- Quote char.\n",
    "        t = t.replace('\\n','\\\\n')\n",
    "        t = t.replace('\\r','\\\\r')\n",
    "        t = t.replace('\\t','\\\\t')\n",
    "        return t\n",
    "\n",
    "    def rule_to_s(self, rule, grammar):\n",
    "        if len(rule) == 0: return '\"\"'\n",
    "        s =['\"%s\"' % self.esc_token(t)\n",
    "                         if t not in grammar else 'RRef(\"%s\")' % t\n",
    "                         for t in rule]\n",
    "        if len(s) == 1: return s[0]\n",
    "        return 'And(%s)' % ', '.join(s)\n",
    "\n",
    "    def translate(self):\n",
    "        lines = ['''\\\n",
    "from gramfuzz.fields import*\n",
    "TOP_CAT = \"grammar\"\n",
    "class RDef(Def): cat=\"grammar-def\"\n",
    "class RRef(Ref): cat=\"grammar-def\"\n",
    "\n",
    "# top-level rule\n",
    "Def(\"grammar\", RRef(\"<start>\", cat=\"grammar-def\") )\n",
    "''']\n",
    "        for key in self.g:\n",
    "            rules = self.g[key]\n",
    "            if len(rules) == 1:\n",
    "                srules = self.rule_to_s(rules[0], self.g)\n",
    "            else:\n",
    "                srules = \"Or(%s)\" % ' ,'.join(sorted([self.rule_to_s(rule, self.g) for rule in rules], reverse=True))\n",
    "            lines.append('''\\\n",
    "RDef(\"%(key)s\",\n",
    "%(rules)s\n",
    ")\n",
    "''' % {'key':key, 'rules':srules})\n",
    "        return '\\n'.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roman_grammar = {\n",
    "    '<start>' : [['<roman>']],\n",
    "    '<roman>' : [['<hundreds>','<tens>','<units>']],\n",
    "    '<hundreds>' : [['<lowhundreds>'],['CD'],['D','<lowhundreds>'],['CM']],\n",
    "    '<lowhundreds>' : [[],['<lowhundreds>','C']],\n",
    "    '<tens>' : [['<lowtens>'],['XL'], ['<lowtens>'], ['XC']],\n",
    "    '<lowtens>' : [[],['<lowtens>','X']],\n",
    "    '<units>' : [['<lowunits>'],['IV'], ['V', '<lowunits>'],['IX']],\n",
    "    '<lowunits>' : [[],['<lowunits>', 'I']]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('testers/roman_rdef.py', 'w+') as f:\n",
    "    print(RDefG(roman_grammar).translate(), file=f)\n",
    "!cat testers/roman_rdef.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gramfuzz\n",
    "fuzzer = gramfuzz.GramFuzzer()\n",
    "fuzzer.load_grammar(\"testers/roman_rdef.py\")\n",
    "names = fuzzer.gen(cat=\"default\", num=10)\n",
    "print(\"\\n\".join(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('testers/grammar_rdef.py', 'w+') as f:\n",
    "    print(RDefG(my_grammar).translate(), file=f)\n",
    "!cat testers/grammar_rdef.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('testers/gram_fuzz.py', 'w+') as f:\n",
    "    print('''\\\n",
    "import gramfuzz, sys\n",
    "gramfuzz.rand.seed(int(sys.argv[2]))\n",
    "fuzzer = gramfuzz.GramFuzzer()\n",
    "fuzzer.load_grammar(sys.argv[1])\n",
    "names = fuzzer.gen(cat=\"default\",\n",
    "                   num=int(sys.argv[3]),\n",
    "                   max_recursion=int(sys.argv[4]))\n",
    "for n in names:\n",
    "   print(repr(n))''', file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python testers/gram_fuzz.py testers/grammar_rdef.py 0 10 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GramFuzzTester(Tester):\n",
    "    def exec_program(self, seed, max_depth, t):\n",
    "        # seed, maxnum, max_depth\n",
    "        fn = self.ofile(max_depth, seed)\n",
    "        return f\"python testers/gram_fuzz.py testers/grammar_rdef.py {seed} {self.max_num} {max_depth} > {fn}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GramFuzzTester().run_test().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dharma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipkg('dharma', 'dharma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DharmaG(AntlrG):\n",
    "    def esc_token(self, t):\n",
    "        if t.strip() == '':\n",
    "            return '%range%( - )'\n",
    "        # these are multi-char tokens\n",
    "        #t = t.replace('\\\\','\\\\\\\\')\n",
    "        #t = t.replace('\"','\\\\\\\"')\n",
    "        t = t.replace('\\n','') # dont know what to do\n",
    "        t = t.replace('\\r','') # dont know what to do\n",
    "        t = t.replace('\\t','') # dont know what to do\n",
    "        return t\n",
    "    \n",
    "    def rule_to_s(self, rule, grammar):\n",
    "        if len(rule) == 0: return '\"\"'\n",
    "        s =['\"%s\"' % self.esc_token(t)\n",
    "                         if t not in grammar else 'RRef(\"%s\")' % t\n",
    "                         for t in rule]\n",
    "        if len(s) == 1: return s[0]\n",
    "        return 'And(%s)' % ', '.join(s)\n",
    "\n",
    "    def rule_to_s(self, rule, grammar):\n",
    "        if len(rule) == 0:\n",
    "            # *note* Dharma does not let us define epsilon rules. So we have to make do with\n",
    "            # generating whitespace.\n",
    "            return '%range%( - )'\n",
    "        return ''.join([self.esc_token(t)\n",
    "                        if t not in grammar else '+%s+' % self.to_key(t)\n",
    "                         for t in rule])\n",
    "\n",
    "    def translate(self):\n",
    "        lines = ['''%%% Dharma Grammar\n",
    "        ''']\n",
    "        for k in self.g:\n",
    "            rules = self.g[k]\n",
    "            v = '\\n    '.join([s for s in [self.rule_to_s(rule, self.g)\n",
    "                                for rule in rules] if s.strip() != ''])\n",
    "            lines.append('''\\\n",
    "%(key)s :=\n",
    "    %(rules)s\n",
    "'''% {'key':self.to_key(k), 'rules':v})\n",
    "        lines.append(''' \n",
    "%section% := variance\n",
    "main :=\n",
    "    +start+\n",
    "''')\n",
    "        return '\\n'.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('testers/grammar.dg', 'w+') as f:\n",
    "    print(DharmaG(my_grammar).translate(), file=f)\n",
    "!cat testers/grammar.dg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf tests\n",
    "!python -m dharma -grammars testers/grammar.dg  -count 10 -seed 200 -storage tests/ -format txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat tests/*.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DharmaTester(GrammarinatorTester):\n",
    "    def exec_program(self, seed, max_depth, t):\n",
    "        # seed, maxnum, max_depth\n",
    "        fn = self.ofile(max_depth, seed)\n",
    "        return f\"python -m dharma -grammars testers/grammar.dg -seed {seed} -count {self.max_num} -logging 30 -storage tests/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DharmaTester().run_test().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Fuzzing book approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pp_grammar(grammar): return json.dumps(grammar, indent=2, sort_keys=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trans(Sanitize):\n",
    "    def split_tokens(self, t, grammar):\n",
    "        if t in grammar: return [t]\n",
    "        my_tokens = []\n",
    "        esc = {'\\r': '\\\\\\r', '\\n': '\\\\\\n',\n",
    "             '\\\\': '\\\\\\\\',\n",
    "             '\"':'\\\\\\\"',\n",
    "             \"'\":\"'\"}\n",
    "        for i in t:\n",
    "            if i in esc:\n",
    "                my_tokens.append(esc[i])\n",
    "            else:\n",
    "                my_tokens.append(i)\n",
    "        return my_tokens\n",
    "            \n",
    "        return list(t)\n",
    "\n",
    "    def to_rule(self, rule, grammar):\n",
    "        tokens = [k for t in rule for k in self.split_tokens(t, grammar)]\n",
    "        return [self.to_token(t) if t not in grammar else self.to_key(t)\n",
    "                for t in tokens]\n",
    "\n",
    "    def translate(self):\n",
    "        new_grammar = {}\n",
    "        for k in self.g:\n",
    "            rules = self.g[k]\n",
    "            new_grammar[self.to_key(k)] = [self.to_rule(rule, self.g) for rule in rules]\n",
    "        return new_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_grammar = Sanitize(my_grammar).translate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('testers/fuzzingbook_gfuzzer.py', 'w+') as f:\n",
    "    print('''grammar = ''', pp_grammar(s_grammar), file=f)\n",
    "    print(\"\"\"\n",
    "result = ''\n",
    "from fuzzingbook.GrammarFuzzer import GrammarFuzzer\n",
    "def canonical(grammar):\n",
    "    new_g = {}\n",
    "    for k in grammar:\n",
    "        new_g[k] = []\n",
    "        for rule in grammar[k]:\n",
    "            new_g[k].append(''.join(rule))\n",
    "    return new_g\n",
    "import random\n",
    "def main(args):\n",
    "    random.seed(int(sys.argv[1]))\n",
    "    global result\n",
    "    max_num = int(args[2])\n",
    "    max_depth = int(args[3])\n",
    "    fuzzer = GrammarFuzzer(canonical(grammar), max_nonterminals=max_depth)\n",
    "    global result\n",
    "    for i in range(max_num):\n",
    "        result = fuzzer.fuzz()\n",
    "        print(result)\n",
    "        result = ''\n",
    "import sys\n",
    "main(sys.argv)\"\"\", file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat testers/fuzzingbook_gfuzzer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python testers/fuzzingbook_gfuzzer.py 0 10 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FuzzingbookTester(Tester):\n",
    "    def exec_program(self, seed, max_depth, t):\n",
    "        # seed, maxnum, max_depth\n",
    "        fn = self.ofile(max_depth, seed)\n",
    "        return f\"python testers/fuzzingbook_gfuzzer.py {seed} {self.max_num} {max_depth} > {fn}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fuzzing book fuzzer does not improve with more depth.\n",
    "FuzzingbookTester(limit_depth=5).run_test().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a grammar fuzzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.platform == 'darwin'\n",
    "sys.setrecursionlimit(20900) # for OSX only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fuzzer:\n",
    "    def __init__(self, grammar):\n",
    "        self.grammar = grammar\n",
    "    \n",
    "    def fuzz(self, key='<start>', max_num=None, max_depth=None):\n",
    "        raise NotImplemented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NFuzzer(Fuzzer):\n",
    "    def gen_key(self, key):\n",
    "        return (self.gen_rule(random.choice(self.grammar[key]))\n",
    "                if key in self.grammar else key)\n",
    "\n",
    "    def gen_rule(self, rule):\n",
    "        return ''.join(self.gen_key(token) for token in rule)\n",
    "\n",
    "    def fuzz(self, key='<start>', max_depth=None):\n",
    "        return self.gen_key(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_fuzzer = NFuzzer(s_grammar)\n",
    "try:\n",
    "    for i in range(100):\n",
    "        print(repr(my_fuzzer.fuzz()))\n",
    "except:\n",
    "    exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "    print(exc_type, exc_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_opts(args, log=False):\n",
    "    seed = int(args[1])\n",
    "    max_num = int(args[2])\n",
    "    max_depth = int(args[3])\n",
    "    random.seed(seed)\n",
    "    sys.setrecursionlimit(20900)\n",
    "    if log:\n",
    "        print(\"seed=%d, num=%d, depth=%d\" % (seed, max_num, max_depth), file=sys.stderr)\n",
    "    return max_num, max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_class_definition(cls, log=False):\n",
    "    eldest = [c for c in cls.mro()\n",
    "                if c.__name__ == cls.__name__ and\n",
    "                   cls.__name__ not in {i.__name__ for i in c.__bases__}]\n",
    "    n_parents = sum([[j.__name__ for j in i.__bases__] for i in eldest], [])\n",
    "    s_parents = '(%s)' % ', '.join(set(n_parents)) if n_parents else ''\n",
    "    buf = [\"class %s%s:\" % (cls.__name__, s_parents)]\n",
    "    seen = set()\n",
    "    i = 0\n",
    "    for curcls in cls.mro():\n",
    "        i += 1\n",
    "        if log: print('Parent: %d' % i, curcls.__name__)\n",
    "        if curcls.__name__ != cls.__name__: continue\n",
    "        for fn_name in dir(curcls):\n",
    "            if log: print('\\t:', fn_name)\n",
    "            if fn_name in seen: continue\n",
    "            if fn_name == '__new__':\n",
    "                continue\n",
    "            fn = curcls.__dict__.get(fn_name)\n",
    "            if fn is None:\n",
    "                continue\n",
    "            if ('function' in str(type(fn))):\n",
    "                seen.add(fn_name)\n",
    "                buf.append(inspect.getsource(fn))\n",
    "    return '\\n'.join(buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(file_name, grammar, classes, fns=[get_opts], fuzzer=None):\n",
    "    with open(file_name, 'w+') as f:\n",
    "        print('''grammar = ''', pp_grammar(grammar), file=f)\n",
    "        for cls in classes:\n",
    "            print(extract_class_definition(cls), file=f)\n",
    "        for fn in fns:\n",
    "            print(inspect.getsource(fn), file=f)\n",
    "        print(\"\"\"\n",
    "import itertools\n",
    "import sys\n",
    "import random\n",
    "def main(args):\n",
    "    max_num, max_depth = get_opts(args)\n",
    "    my_fuzzer = %s(grammar)\n",
    "    for i in range(max_num):\n",
    "        print(my_fuzzer.fuzz(key='<start>', max_depth=max_depth))\n",
    "try:\n",
    "    main(sys.argv)\n",
    "    sys.exit(0)\n",
    "except RecursionError as e:\n",
    "    print(e, file=sys.stderr)\n",
    "    sys.exit(2)\n",
    "\"\"\" % fuzzer.__name__, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file('testers/grammar_producer_naive.py', s_grammar, [Fuzzer, NFuzzer], fuzzer=NFuzzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat testers/grammar_producer_naive.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the seed and max_num is chosen to avoid recusion error.\n",
    "!time python testers/grammar_producer_naive.py 1 96 0 > testers/fuzz.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting expansion limits\n",
    "\n",
    "We can compute the least cost paths to take and use only those paths after a given depth is exceeded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LimitFuzzer(Fuzzer):\n",
    "    def symbol_cost(self, grammar, symbol, seen):\n",
    "        if symbol in self.key_cost: return self.key_cost[symbol]\n",
    "        if symbol in seen:\n",
    "            self.key_cost[symbol] = float('inf')\n",
    "            return float('inf')\n",
    "        v = min((self.expansion_cost(grammar, rule, seen | {symbol})\n",
    "                    for rule in grammar.get(symbol, [])), default=0)\n",
    "        self.key_cost[symbol] = v\n",
    "        return v\n",
    "\n",
    "    def expansion_cost(self, grammar, tokens, seen):\n",
    "        return max((self.symbol_cost(grammar, token, seen)\n",
    "                    for token in tokens if token in grammar), default=0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LimitFuzzer(LimitFuzzer):\n",
    "    def gen_key(self, key, depth, max_depth):\n",
    "        if key not in self.grammar: return key\n",
    "        if depth > max_depth:\n",
    "            clst = sorted([(self.cost[key][str(rule)], rule) for rule in self.grammar[key]])\n",
    "            rules = [r for c,r in clst if c == clst[0][0]]\n",
    "        else:\n",
    "            rules = self.grammar[key]\n",
    "        return self.gen_rule(random.choice(rules), depth+1, max_depth)\n",
    "\n",
    "    def gen_rule(self, rule, depth, max_depth):\n",
    "        return ''.join(self.gen_key(token, depth, max_depth) for token in rule)\n",
    "\n",
    "    def fuzz(self, key='<start>', max_depth=10):\n",
    "        return self.gen_key(key=key, depth=0, max_depth=max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LimitFuzzer(LimitFuzzer):\n",
    "    def __init__(self, grammar):\n",
    "        super().__init__(grammar)\n",
    "        self.key_cost = {}\n",
    "        self.cost = self.compute_cost(grammar)\n",
    " \n",
    "    def compute_cost(self, grammar):\n",
    "        cost = {}\n",
    "        for k in grammar:\n",
    "            cost[k] = {}\n",
    "            for rule in grammar[k]:\n",
    "                cost[k][str(rule)] = self.expansion_cost(grammar, rule, set())  \n",
    "        return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_fuzzer = LimitFuzzer(s_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_fuzzer.fuzz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file('testers/grammar_producer_limit.py', s_grammar, [Fuzzer, LimitFuzzer], fuzzer=LimitFuzzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat testers/grammar_producer_limit.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyLimitTester(Tester):\n",
    "    def exec_program(self, seed, max_depth, t):\n",
    "        # seed, maxnum, max_depth\n",
    "        fn = self.ofile(max_depth, seed)\n",
    "        return f\"python ./testers/grammar_producer_limit.py {seed} {self.max_num} {max_depth} > {fn}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python testers/grammar_producer_limit.py 0 10 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PyLimitTester().run_test().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using precomputed string pools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**idea**: We can precompute the closing parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PooledFuzzer(LimitFuzzer):\n",
    "    def compute_cost(self, grammar, cost={}):\n",
    "        return {k:sorted([(self.expansion_cost(grammar, rule, set()), rule)\n",
    "                          for rule in grammar[k]])\n",
    "                for k in self.grammar}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PooledFuzzer(PooledFuzzer):\n",
    "    def cheap_grammar(self):\n",
    "        new_grammar = {}\n",
    "        for k in self.cost:\n",
    "            crules = self.cost[k]\n",
    "            min_cost = crules[0][0]\n",
    "            new_grammar[k] = [r for c,r in crules if c == min_cost]\n",
    "            assert len(new_grammar[k]) > 0\n",
    "        return new_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PooledFuzzer(s_grammar).cheap_grammar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PooledFuzzer(PooledFuzzer):\n",
    "    def get_strings_for_key(self, grammar, key='<start>'):\n",
    "        if key not in grammar: return [key]\n",
    "        v = sum([self.get_strings_for_rule(grammar, rule)\n",
    "                 for rule in grammar[key]], [])\n",
    "        return random.sample(v, min(self.MAX_SAMPLE, len(v)))\n",
    "\n",
    "    def get_strings_for_rule(self, grammar, rule):\n",
    "        my_strings_list = [self.get_strings_for_key(grammar, key) for key in rule]\n",
    "        v = [''.join(l) for l in itertools.product(*my_strings_list)]\n",
    "        return random.sample(v, min(self.MAX_SAMPLE, len(v)))\n",
    "\n",
    "    def completion_strings(self):\n",
    "        # we are being choosy\n",
    "        return {k:self.get_strings_for_key(self.c_grammar, k)\n",
    "                for k in self.c_grammar}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = PooledFuzzer(s_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.c_grammar = pf.cheap_grammar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.MAX_SAMPLE = 255\n",
    "strings = pf.completion_strings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in strings:\n",
    "    print(k, strings[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PooledFuzzer(PooledFuzzer):\n",
    "    def __init__(self, grammar):\n",
    "        super().__init__(grammar)\n",
    "        self.c_grammar = self.cheap_grammar()\n",
    "        self.MAX_SAMPLE = 255\n",
    "        self.pool_of_strings = self.completion_strings()\n",
    "        # reorder our grammar rules by cost.\n",
    "        for k in self.grammar:\n",
    "            self.grammar[k] = [r for (i,r) in self.cost[k]]\n",
    "        self.ordered_grammar = True\n",
    "        \n",
    "    def gen_key(self, key, depth, max_depth):\n",
    "        if key not in self.grammar: return key\n",
    "        if depth > max_depth:\n",
    "            return random.choice(self.pool_of_strings[key])\n",
    "        return self.gen_rule(random.choice(self.grammar[key]), depth+1, max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_fuzzer = PooledFuzzer(s_grammar)\n",
    "for i in range(10):\n",
    "    print(repr(my_fuzzer.fuzz()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file('testers/grammar_producer_pool.py', s_grammar, [Fuzzer, LimitFuzzer, PooledFuzzer], fuzzer=PooledFuzzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat testers/grammar_producer_pool.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyPooledTester(Tester):\n",
    "    def exec_program(self, seed, max_depth, t):\n",
    "        # seed, maxnum, max_depth\n",
    "        fn = self.ofile(max_depth, seed)\n",
    "        return f\"python testers/grammar_producer_pool.py {seed} {self.max_num} {max_depth} > {fn}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PyPooledTester().run_test().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTrans(Sanitize):\n",
    "    def split_tokens(self, t, grammar):\n",
    "        if t in grammar: return [t]\n",
    "        my_tokens = []\n",
    "        for i in t:\n",
    "            my_tokens.append(i)\n",
    "        return my_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyc_grammar = PyTrans(my_grammar).translate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyc_grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile to Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     23
    ]
   },
   "outputs": [],
   "source": [
    "# not clear what is the fastest: + or ''.join\n",
    "# https://stackoverflow.com/questions/1316887/what-is-the-most-efficient-string-concatenation-method-in-python\n",
    "class PyCompiledFuzzer(PooledFuzzer):\n",
    "    def add_indent(self, string, indent):\n",
    "        return '\\n'.join([indent + i for i in string.split('\\n')])\n",
    "\n",
    "    # used for escaping inside strings\n",
    "    def esc(self, t):\n",
    "        t = t.replace('\\\\', '\\\\\\\\')\n",
    "        t = t.replace('\\n', '\\\\n')\n",
    "        t = t.replace('\\r', '\\\\r')\n",
    "        t = t.replace('\\t', '\\\\t')\n",
    "        t = t.replace('\\b', '\\\\b')\n",
    "        t = t.replace('\\v', '\\\\v')\n",
    "        t = t.replace('\"', '\\\\\"')\n",
    "        return t\n",
    "    \n",
    "    def esc_char(self, t):\n",
    "        assert len(t) == 1\n",
    "        t = t.replace('\\\\', '\\\\\\\\')\n",
    "        t = t.replace('\\n', '\\\\n')\n",
    "        t = t.replace('\\r', '\\\\r')\n",
    "        t = t.replace('\\t', '\\\\t')\n",
    "        t = t.replace('\\b', '\\\\b')\n",
    "        t = t.replace('\\v', '\\\\v')\n",
    "        t = t.replace(\"'\", \"\\\\'\")\n",
    "        return t\n",
    "\n",
    "    def k_to_s(self, k): return k[1:-1].replace('-', '_')\n",
    "\n",
    "    def gen_rule_src(self, rule, key, i):\n",
    "        res = []\n",
    "        for token in rule:\n",
    "            if token in self.grammar:\n",
    "                res.append('''\\\n",
    "gen_%s(next_depth, max_depth)''' % self.k_to_s(token))\n",
    "            else:\n",
    "                res.append('''\\\n",
    "result.append(\"%s\")''' % self.esc(token))\n",
    "        return '\\n'.join(res)\n",
    "\n",
    "    def string_pool_defs(self):\n",
    "        result =[]\n",
    "        for k in self.pool_of_strings:\n",
    "            result.append('''\\\n",
    "pool_of_%(key)s = %(values)s''' % {\n",
    "                'key':self.k_to_s(k),\n",
    "                'values': self.pool_of_strings[k]})\n",
    "        result.append('''\n",
    "result = []''')\n",
    "        return '\\n'.join(result)\n",
    "\n",
    "    def gen_main_src(self):\n",
    "        result = []\n",
    "        result.append('''\n",
    "import random\n",
    "import sys\n",
    "def main(args):\n",
    "    global result\n",
    "    max_num, max_depth = get_opts(args)\n",
    "    for i in range(max_num):\n",
    "        gen_start(0, max_depth)\n",
    "        print(''.join(result))\n",
    "        result = []\n",
    " \n",
    "main(sys.argv)''')\n",
    "        return '\\n'.join(result)\n",
    "\n",
    "    def gen_alt_src(self, key):\n",
    "        rules = self.grammar[key]\n",
    "        result = []\n",
    "        result.append('''\n",
    "def gen_%(name)s(depth, max_depth):\n",
    "    next_depth = depth + 1\n",
    "    if depth > max_depth:\n",
    "        result.append(random.choice(pool_of_%(name)s))\n",
    "        return\n",
    "    val = random.randrange(%(nrules)s)''' % {\n",
    "            'name':self.k_to_s(key),\n",
    "            'nrules':len(rules)})\n",
    "        for i, rule in enumerate(rules):\n",
    "            result.append('''\\\n",
    "    if val == %d:\n",
    "%s\n",
    "        return''' % (i, self.add_indent(self.gen_rule_src(rule, key, i),'        ')))\n",
    "        return '\\n'.join(result)\n",
    "\n",
    "    def gen_fuzz_src(self):\n",
    "        result = []\n",
    "        result.append(self.string_pool_defs())\n",
    "        for key in self.grammar:\n",
    "            result.append(self.gen_alt_src(key))\n",
    "        return '\\n'.join(result)\n",
    "\n",
    "    def fuzz_src(self, key='<start>'):\n",
    "        result = [self.gen_fuzz_src(),\n",
    "                  self.gen_main_src()]\n",
    "        return ''.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('testers/grammar_producer_pycompiled.py', 'w+') as f:\n",
    "    for fn in [get_opts]:\n",
    "        print(inspect.getsource(fn), file=f)\n",
    "    result = PyCompiledFuzzer(pyc_grammar).fuzz_src()\n",
    "    print(result, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat testers/grammar_producer_pycompiled.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python testers/grammar_producer_pycompiled.py 0 10 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyCompiledTester(Tester):\n",
    "    def exec_program(self, seed, max_depth, t):\n",
    "        fn = self.ofile(max_depth, seed)\n",
    "        return f\"python testers/grammar_producer_pycompiled.py {seed} {self.max_num} {max_depth} > {fn}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PyCompiledTester().run_test().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class PyRecCompiledFuzzer(PyCompiledFuzzer):\n",
    "    def __init__(self, grammar):\n",
    "        super().__init__(grammar)\n",
    "        assert self.ordered_grammar\n",
    "        self.rec_cost = {}\n",
    "        self.compute_rule_recursion()\n",
    "\n",
    "    def kr_to_s(self, key, i): return 'gen_%s_%d' % (self.k_to_s(key), i)\n",
    "    # the grammar needs to be ordered by the cost.\n",
    "    # else the ordering will change at the end.\n",
    "    \n",
    "    def is_rule_recursive(self, rname, rule, seen):\n",
    "        if not rule: return False\n",
    "        if rname in seen:\n",
    "            return False # reached another recursive rule without seeing this one\n",
    "        for token in rule:\n",
    "            if token not in self.grammar: continue\n",
    "            for i,trule in enumerate(self.grammar[token]):\n",
    "                rn = self.kr_to_s(token, i)\n",
    "                if rn  == rname: return True\n",
    "                if rn in seen: return False\n",
    "                v = self.is_rule_recursive(rname, trule, seen | {rn})\n",
    "                if v: return True\n",
    "        return False\n",
    "    \n",
    "    def is_key_recursive(self, check, key, seen):\n",
    "        if not key in self.grammar: return False\n",
    "        if key in seen: return False\n",
    "        for rule in self.grammar[key]:\n",
    "            for token in rule:\n",
    "                if token not in self.grammar: continue\n",
    "                if token == check: return True\n",
    "                v = self.is_key_recursive(check, token, seen | {token})\n",
    "                if v: return True\n",
    "        return False\n",
    "    \n",
    "    def compute_rule_recursion(self):\n",
    "        if IS_HTML:   # TODO -- to much time -- only for HTML\n",
    "            self.rule_recursion = HTML_RULE_RECURSION\n",
    "            self.key_recursion = HTML_KEY_RECURSION\n",
    "            return\n",
    "        self.rule_recursion = {}\n",
    "        for k in self.grammar:\n",
    "            for i_rule,rule in enumerate(self.grammar[k]):\n",
    "                n = self.kr_to_s(k, i_rule)\n",
    "                self.rule_recursion[n] = self.is_rule_recursive(n, rule, set())\n",
    "        self.key_recursion = {}\n",
    "        for k in self.grammar:\n",
    "            self.key_recursion[k] = self.is_key_recursive(k, k, set())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PyRecCompiledFuzzer(pyc_grammar).rule_recursion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PyRecCompiledFuzzer(pyc_grammar).key_recursion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Evaluation in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PEFuzzer(PyRecCompiledFuzzer):\n",
    "    def pe_rule(self, rule, depth):\n",
    "        res = []\n",
    "        res.append('''\\\n",
    "# rule=%(rule)s len[%(len)d]''' % {'rule':str(rule), 'len':len(rule)})\n",
    "        if not rule:\n",
    "            res.append('''\\\n",
    "pass''')\n",
    "        for token in rule:\n",
    "            res.append('''\\\n",
    "# token=%(token)s''' % {'token':token})\n",
    "            if token in self.grammar:\n",
    "                if self.key_recursion[token]:\n",
    "                    res.append('''\\\n",
    "# sc\n",
    "gen_%(key)s(depth+%(depth)s, max_depth)''' % {'key':self.k_to_s(token),'depth':depth+2})\n",
    "                else:\n",
    "                    res.append(self.pe_key(token, depth=depth+1))\n",
    "            else:\n",
    "                res.append('''\\\n",
    "result.append(\"%s\")''' % self.esc(token))\n",
    "        return '\\n'.join(res)\n",
    " \n",
    "    def pe_key(self, key, depth):\n",
    "        if depth == self.MAX_PE_DEPTH:\n",
    "            return 'gen_%(key)s(depth+%(depth)d, max_depth)' % {'key':self.k_to_s(key), 'depth':depth+1}\n",
    "        rules = self.grammar[key]\n",
    "        result = ['''\\\n",
    "#* %(key)s begins\n",
    "if depth + %(depth)d > max_depth:\n",
    "    result.append(random.choice(pool_of_strings['%(key)s']))\n",
    "else:''' % {'name':self.k_to_s(key), 'key':key,  'depth':depth+1}]\n",
    "        if len(rules) == 0:\n",
    "            result.append('''\\\n",
    "    # indent dummy''')\n",
    "        elif len(rules) == 1:\n",
    "            result.append('''\\\n",
    "    # indent inline''')\n",
    "            result.append(self.add_indent(self.pe_rule(rules[0], depth),'    '))\n",
    "        else:\n",
    "            result.append('''\\\n",
    "    val = random.randrange(%(nrules)s)''' % {'nrules':len(rules)})\n",
    "            result.append('''\\\n",
    "    # indent here<\n",
    "    if False:\n",
    "        pass # dummy''')\n",
    "            assert len(rules) > 1\n",
    "            for i, rule in enumerate(rules):\n",
    "                result.append('''\\\n",
    "    elif val == %d:''' % i)               \n",
    "                result.append(self.add_indent(self.pe_rule(rule, depth),'        '))\n",
    "            result.append('''\\\n",
    "    # indent here>''')\n",
    "        result.append('''\\\n",
    "#* %(key)s ends''' % {'key': key})\n",
    "        return '\\n'.join(result)\n",
    "\n",
    "    def gen_rule_src(self, rule, key, i):\n",
    "        res = []\n",
    "        for token in rule:\n",
    "            if token in self.grammar:\n",
    "                if token == key:\n",
    "                    res.append('''\\\n",
    "# not unrolling\n",
    "gen_%s(depth+1, max_depth)''' % self.k_to_s(token))\n",
    "                else:\n",
    "                    res.append('''\\\n",
    "#indent -<''')\n",
    "                    res.append(self.pe_key(token, depth=0))\n",
    "                    res.append('''\\\n",
    "#indent >-''')\n",
    "                    \n",
    "            else:\n",
    "                res.append('''\\\n",
    "result.append(\"%s\")''' % self.esc(token))\n",
    "        return self.add_indent('\\n'.join(res), '            ')\n",
    "\n",
    "    def string_pool_defs(self):\n",
    "        result =[]\n",
    "        result.append('''\\\n",
    "pool_of_strings = %s''' % pp_grammar(self.pool_of_strings))\n",
    "        result.append('''\n",
    "result = [];''')\n",
    "        return '\\n'.join(result)\n",
    "\n",
    "    def gen_main_src(self):\n",
    "        result = []\n",
    "        result.append('''\n",
    "import random\n",
    "import sys\n",
    "def main(args):\n",
    "    global result\n",
    "    max_num, max_depth = get_opts(args)\n",
    "    for i in range(max_num):\n",
    "        gen_start(0, max_depth)\n",
    "        print(''.join(result))\n",
    "        result = []\n",
    "main(sys.argv)\n",
    "    ''')\n",
    "        return '\\n'.join(result)\n",
    "\n",
    "    def gen_alt_src(self, key):\n",
    "        rules = self.grammar[key]\n",
    "        result = []\n",
    "        result.append('''\n",
    "def gen_%(name)s(depth, max_depth):\n",
    "    # %(name)s begins\n",
    "    if depth > max_depth:\n",
    "        result.append(random.choice(pool_of_strings['%(key)s']))\n",
    "    else:\n",
    "        val = random.randrange(%(nrules)s)''' % {'name':self.k_to_s(key), 'key':key, 'nrules':len(rules)})\n",
    "        for i, rule in enumerate(rules):\n",
    "            result.append('''\\\n",
    "        if val == %d:\n",
    "%s\n",
    "            return''' % (i, self.gen_rule_src(rule, key, i)))\n",
    "        result.append('''\n",
    "    # %(name)s ends\n",
    "        ''')\n",
    "        return '\\n'.join(result)\n",
    "    \n",
    "    def fuzz_src(self, key='<start>'):\n",
    "        self.MAX_PE_DEPTH = 4\n",
    "        result = [self.gen_fuzz_src(),\n",
    "                  self.gen_main_src()]\n",
    "        return ''.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('testers/grammar_producer_pe.py', 'w+') as f:\n",
    "    for fn in [get_opts]:\n",
    "        print(inspect.getsource(fn), file=f)\n",
    "    result = PEFuzzer(pyc_grammar).fuzz_src()\n",
    "    print(result, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l testers/grammar_producer_pe.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python testers/grammar_producer_pe.py 0 10 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyPETester(Tester):\n",
    "    def exec_program(self, seed, max_depth, t):\n",
    "        fn = self.ofile(max_depth, seed)\n",
    "        return f\"python testers/grammar_producer_pe.py {seed} {self.max_num} {max_depth} > {fn}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PyPETester().run_test().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supercompile in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PySuperCompiledFuzzer(PyRecCompiledFuzzer):    \n",
    "    def supercompile_rule(self, key, rule, i_rule, depth):\n",
    "        gen_name = self.kr_to_s(key, i_rule)\n",
    "        if self.rule_recursion[gen_name]:\n",
    "            self.current_lst.append(gen_name)\n",
    "            return '''\\\n",
    "%(gen_name)s(depth_%(depth)d) # recursing''' % {'gen_name':gen_name, 'depth':depth}\n",
    "        res = []\n",
    "        if len(rule) == 0:\n",
    "            res.append('pass')\n",
    "        else:\n",
    "            for token in rule:\n",
    "                if token not in self.grammar:\n",
    "                    res.append('''\\\n",
    "result.append(\"%s\")''' % self.esc(token))\n",
    "                else:\n",
    "                    res.append(# no indent\n",
    "                        self.supercompile_key(token,\n",
    "                                              depth=(depth+1)))\n",
    "        return '\\n'.join(res)\n",
    "    def supercompile_key_internal(self, key, trule, i_trule, depth):\n",
    "        if depth > self.MAX_SUPERCOMPILE_DEPTH:\n",
    "            self.current_lst.append(self.kr_to_s(key, i_trule))\n",
    "            return '%(gen_name)s(depth_%(depth)d) #slimit*' % {\n",
    "                        'gen_name':self.kr_to_s(key, i_trule), 'depth':depth}\n",
    "        else:\n",
    "            return self.supercompile_rule(key, trule, i_trule, depth=depth)\n",
    " \n",
    "    def supercompile_key(self, key, depth):\n",
    "        # Should check for MAX_SUPERCOMPILE_DEPTH\n",
    "        # should first get the random number curresponding to\n",
    "        # len(grammar[key]) then it should unroll that elif cond.\n",
    "        if len(self.grammar[key]) == 0: return '' # no more jumping on the bed\n",
    "        res = ['''\\\n",
    "if depth_%(depth)d > max_depth:\n",
    "    result.append(random.choice(pool_of_%(key)s))\n",
    "else:\n",
    "    depth_%(d_1)d = depth + %(d_1)d'''%{\n",
    "            'key':self.k_to_s(key), 'depth': depth, 'd_1': depth+1}]\n",
    "        if len(self.grammar[key]) == 1:\n",
    "            # we do not have to get the random number, and check for\n",
    "            # equality first.\n",
    "            i_trule, trule  = 0, self.grammar[key][0]\n",
    "            res.append(self.add_indent(\n",
    "                self.supercompile_key_internal(key, trule, i_trule, depth),\n",
    "                '''\\\n",
    "    '''))\n",
    "        else:\n",
    "            # First get the random number, then compare and\n",
    "            # unroll\n",
    "            res.append('''\\\n",
    "    val = random.randrange(%(len_rules)d)\n",
    "    if False: # dummy for elsif\n",
    "        pass''' % {'len_rules': len(self.grammar[key])})\n",
    "            for i_trule, trule in enumerate(self.grammar[key]):\n",
    "                res.append('''\\\n",
    "    elif val == %(i_trule)d:''' % {'i_trule': i_trule})\n",
    "                res.append(self.add_indent(\n",
    "                    self.supercompile_key_internal(key, trule, i_trule, depth),\n",
    "                    '''\\\n",
    "        '''))\n",
    "\n",
    "        return '\\n'.join(res)\n",
    "   \n",
    "    def gen_rule_src(self, rule, key, i_rule):\n",
    "        res = ['''\\\n",
    "def %(gen_name)s(depth):\n",
    "    if depth > max_depth:\n",
    "        result.append(random.choice(pool_of_%(key)s))\n",
    "    else:\n",
    "        depth_%(d_1)d = depth + %(d_1)d''' % {\n",
    "            'gen_name':self.kr_to_s(key,i_rule),\n",
    "            'key':self.k_to_s(key),\n",
    "            'depth':0, 'd_1': 1}]\n",
    "        \n",
    "        # These should be a sequence of getting randon numbers\n",
    "        # and unrolling appropriately.\n",
    "        for token in rule:\n",
    "            if token not in self.grammar:\n",
    "                res.append('''\\\n",
    "        result.append(\"%s\")''' % self.esc(token))\n",
    "            else:\n",
    "                res.append(self.add_indent(\n",
    "                    self.supercompile_key(token, depth=1), '''\\\n",
    "        '''))\n",
    "        return '\\n'.join(res)\n",
    "\n",
    "    def gen_main_src(self):\n",
    "        result = []\n",
    "        result.append('''\n",
    "import random\n",
    "import sys\n",
    "max_depth = 0\n",
    "def main(args):\n",
    "    global result, max_depth\n",
    "    max_num, max_depth = get_opts(args)\n",
    "    for i in range(max_num):\n",
    "        gen_start_0(0)\n",
    "        print(''.join(result))\n",
    "        result = []\n",
    "main(sys.argv)\n",
    "    ''')\n",
    "        return '\\n'.join(result)\n",
    "    \n",
    "    def gen_fuzz_src(self):\n",
    "        keys_used = {}\n",
    "        result = [self.string_pool_defs()]\n",
    "        key_defs = {}\n",
    "        for key in self.grammar:\n",
    "            for i,rule in enumerate(self.grammar[key]):\n",
    "                self.current_lst = []\n",
    "                ks = self.kr_to_s(key, i)\n",
    "                keys_used[ks] = self.current_lst\n",
    "                key_defs[ks] = self.gen_rule_src(rule, key, i)\n",
    "        key_set = set(keys_used['gen_start_0']) | {'gen_start_0'}\n",
    "        old_len = 0\n",
    "        while old_len != len(key_set):\n",
    "            old_len = len(key_set)\n",
    "            key_set.update(k1 for k in list(key_set) for k1 in keys_used[k])\n",
    "            \n",
    "        for k in key_set:\n",
    "            result.append(key_defs[k])\n",
    "        return '\\n'.join(result)\n",
    "\n",
    "    def fuzz_src(self, key='<start>'):\n",
    "        self.MAX_SUPERCOMPILE_DEPTH = 100\n",
    "        result = [self.gen_fuzz_src(),\n",
    "                  self.gen_main_src()]\n",
    "        return ''.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('testers/grammar_producer_pysupercompiled.py', 'w+') as f:\n",
    "    for fn in [get_opts]:\n",
    "        print(inspect.getsource(fn), file=f)\n",
    "    result = PySuperCompiledFuzzer(pyc_grammar).fuzz_src()\n",
    "    print(result, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l testers/grammar_producer_pysupercompiled.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python testers/grammar_producer_pysupercompiled.py 0 10 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PySupercompiledTester(Tester):\n",
    "    def exec_program(self, seed, max_depth, t):\n",
    "        fn = self.ofile(max_depth, seed)\n",
    "        return f\"python testers/grammar_producer_pysupercompiled.py {seed} {self.max_num} {max_depth} > {fn}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PySupercompiledTester().run_test().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile to C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTrans(Sanitize):\n",
    "    def split_tokens(self, t, grammar):\n",
    "        if t in grammar: return [t]\n",
    "        my_tokens = []\n",
    "        esc = {\n",
    "           '\\r': '\\\\r',\n",
    "           '\\n': '\\\\n',\n",
    "           '\\t': '\\\\t',\n",
    "           '\\\\': '\\\\\\\\',\n",
    "        }\n",
    "        for i in t:\n",
    "            #if i in esc:\n",
    "            #    my_tokens.append(esc[i])\n",
    "            #else:\n",
    "                my_tokens.append(i)\n",
    "        return my_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_grammar = CTrans(my_grammar).translate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compile a grammar into a C program that produces from the grammar.  Just how fast can we be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFuzzer(PyRecCompiledFuzzer):    \n",
    "    def cheap_chars(self, string):\n",
    "        # to be embedded within single quotes\n",
    "        escaped = {'t':'\\t', 'n': '\\n', \"'\": \"\\\\'\", \"\\\\\":\"\\\\\\\\\", 'r': '\\r'}\n",
    "        slst = []\n",
    "        while string:\n",
    "            c, *string = string\n",
    "            if c in {'\\\\'}:\n",
    "                c1, *string = string\n",
    "                slst.append(escaped[c1])\n",
    "            elif c in {\"'\"}:\n",
    "                slst.append(\"\\'\")\n",
    "            else:\n",
    "                slst.append(c)\n",
    "        return slst\n",
    "    \n",
    "    def gen_rule_src(self, rule, key, i):\n",
    "        res = []\n",
    "        for token in rule:\n",
    "            if token in self.grammar:\n",
    "                res.append('gen_%s(depth +1);' % self.k_to_s(token))\n",
    "            else:\n",
    "                res.append(\"out('%s');\" % self.esc_char(token))\n",
    "        return '\\n        '.join(res)\n",
    "\n",
    "    def gen_alt_src(self, k):\n",
    "        rules = self.grammar[k]\n",
    "        cheap_strings = self.pool_of_strings[k]\n",
    "        result = ['''\n",
    "void gen_%(name)s(int depth) {\n",
    "    if (depth > max_depth) {\n",
    "        int val = map(%(num_cheap_strings)d);\n",
    "        const char* str = pool_%(name)s[val];\n",
    "        const int str_l = pool_l_%(name)s[val];\n",
    "        for (int i = 0; i < str_l; i++) {\n",
    "            out(str[i]);\n",
    "        }\n",
    "        return;\n",
    "    }\n",
    "\n",
    "    int val = map(%(nrules)d);\n",
    "    switch(val) {''' % {'name':self.k_to_s(k), 'nrules':len(rules),\n",
    "                        'num_cheap_strings': len(cheap_strings),\n",
    "                       }]\n",
    "        for i, rule in enumerate(rules):\n",
    "            result.append('''\n",
    "    case %d:\n",
    "        %s\n",
    "        break;''' % (i, self.gen_rule_src(rule, k, i)))\n",
    "        result.append('''\n",
    "    }\n",
    "}\n",
    "    ''')\n",
    "        return '\\n'.join(result)\n",
    "    \n",
    "    def string_pool_defs(self):\n",
    "        result = []\n",
    "        for k in self.grammar:\n",
    "            cheap_strings = self.pool_of_strings[k]\n",
    "            result.append('''\n",
    "const char* pool_%(k)s[] =  {%(cheap_strings)s};\n",
    "const int pool_l_%(k)s[] =  {%(cheap_strings_len)s};\n",
    "        ''' % {'k':self.k_to_s(k),\n",
    "               'cheap_strings': ', '.join(['\"%s\"' % self.esc(s) for s in cheap_strings]),\n",
    "               'cheap_strings_len': ', '.join([str(len(s)) for s in cheap_strings])})\n",
    "        return '\\n'.join(result)\n",
    "\n",
    "    \n",
    "    def fn_fuzz_decs(self):\n",
    "        result = []\n",
    "        for k in self.grammar:\n",
    "            result.append('''void gen_%s(int depth);''' % self.k_to_s(k))\n",
    "        return '\\n'.join(result)\n",
    "    \n",
    "    def fn_map_def(self):\n",
    "        return '''\n",
    "int map(int v) {\n",
    "    return random() % v;\n",
    "}\n",
    " '''    \n",
    "    def fn_out_def(self):\n",
    "        return '''\n",
    "void out(const char s) {\n",
    "    fputc(s, stdout);\n",
    "}       \n",
    " '''\n",
    "\n",
    "    def fuzz_hdefs(self):\n",
    "        return '''\n",
    "#include <stdlib.h>\n",
    "#include <stdio.h>\n",
    "#include <time.h>\n",
    "#include <string.h>\n",
    "'''\n",
    "    \n",
    "    def fuzz_out_var_defs(self):\n",
    "        return '''\n",
    "void out(const char s);'''\n",
    "    \n",
    "    def fuzz_rand_var_defs(self):\n",
    "        return '''\n",
    "int map(int v);'''\n",
    "    def fuzz_stack_var_defs(self):\n",
    "        return '''\n",
    "extern int max_depth;'''\n",
    "\n",
    "    def fuzz_var_defs(self):\n",
    "        return '\\n'.join([self.fuzz_out_var_defs(), self.fuzz_rand_var_defs(), self.fuzz_stack_var_defs()])\n",
    "\n",
    "    def fn_main_input_frag(self):\n",
    "        return '''\n",
    "    if (argc < 3) {\n",
    "        printf(\"%s <seed> <max_num> <max_depth>\\\\n\", argv[0]);\n",
    "        return 0;\n",
    "    }\n",
    "    seed = atoi(argv[1]);\n",
    "    max_num = atoi(argv[2]);\n",
    "    max_depth = atoi(argv[3]);'''\n",
    "    \n",
    "    def fn_main_loop_frag(self):\n",
    "        return '''\n",
    "    for(int i=0; i < max_num; i++) {\n",
    "        gen_init__();\n",
    "    }'''\n",
    "\n",
    "    def fn_main_def(self):\n",
    "        result = '''\n",
    "int main(int argc, char** argv) {\n",
    "    int seed, max_num;\n",
    "%(input_frag)s\n",
    "    //srandom(time(0));\n",
    "    srandom(seed);\n",
    "%(loop_frag)s\n",
    "    return 0;\n",
    "}''' % {'input_frag':self.fn_main_input_frag(),\n",
    "        'loop_frag': self.fn_main_loop_frag()}\n",
    "        return result\n",
    "    \n",
    "    def main_stack_var_defs(self):\n",
    "        return '''\n",
    "int max_depth = 0;'''\n",
    "    \n",
    "    def main_init_var_defs(self):\n",
    "        return '''\n",
    "void gen_init__();'''\n",
    "    \n",
    "    def main_var_defs(self):\n",
    "        return '\\n'.join([self.main_stack_var_defs(), self.main_init_var_defs()])\n",
    "    \n",
    "    def fuzz_fn_defs(self):\n",
    "        result = []\n",
    "        for key in self.grammar:\n",
    "            result.append(self.gen_alt_src(key))\n",
    "        return '\\n'.join(result)\n",
    "    \n",
    "    def fuzz_entry(self):\n",
    "        return '''\n",
    "void gen_init__() {\n",
    "    gen_start(0);\n",
    "    out('\\\\n');\n",
    "    return;\n",
    "}'''\n",
    "\n",
    "    def main_hdefs(self):\n",
    "        return '''\n",
    "#define _LARGEFILE64_SOURCE\n",
    "#define _FILE_OFFSET_BITS 64\n",
    "\n",
    "#include <stdlib.h>\n",
    "#include <stdio.h>\n",
    "#include <time.h>\n",
    "#include <string.h>\n",
    "'''\n",
    "\n",
    "    def gen_main_src(self):\n",
    "        return '\\n'.join([self.main_hdefs(),\n",
    "                          self.main_var_defs(),\n",
    "                          self.fn_map_def(),\n",
    "                          self.fn_out_def(),\n",
    "                          self.fn_main_def()])\n",
    "    \n",
    "    def gen_fuzz_src(self):\n",
    "        return '\\n'.join([self.fuzz_hdefs(),\n",
    "                          self.fuzz_var_defs(),\n",
    "                          self.fn_fuzz_decs(),\n",
    "                          self.string_pool_defs(),\n",
    "                          self.fuzz_fn_defs(),\n",
    "                          self.fuzz_entry()])\n",
    "\n",
    "    def fuzz_src(self, key='<start>'):\n",
    "        return self.gen_main_src(), self.gen_fuzz_src()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_src, fuzz_src = CFuzzer(c_grammar).fuzz_src()\n",
    "with open('testers/grammar_producer_c_fuzz.c', 'w+') as f:\n",
    "    print(fuzz_src, file=f)\n",
    "with open('testers/grammar_producer_c_main.c', 'w+') as f:\n",
    "    print(main_src, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nl -ba testers/grammar_producer_c_fuzz.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat testers/grammar_producer_c_main.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd testers\n",
    "!cc -Ofast grammar_producer_c_main.c grammar_producer_c_fuzz.c  -o grammar_producer_c\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./testers/grammar_producer_c 0 10 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# II\n",
    "class CTester(Tester):\n",
    "    def __init__(self, name=None, max_num=10000, start_depth=3, limit_depth=9, timeout=3600, iterations=100):\n",
    "        super().__init__(name, max_num, start_depth, limit_depth, timeout)\n",
    "\n",
    "    def exec_program(self, seed, max_depth, t):\n",
    "        # seed, maxnum, max_depth\n",
    "        fn = self.ofile(max_depth, seed)\n",
    "        return f\"./testers/grammar_producer_c {seed} {self.max_num} {max_depth} > {fn}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTester().run_test().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Evaluation in C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PECFuzzer(CFuzzer):\n",
    "    def pe_rule(self, rule, seen, depth):\n",
    "        res = []\n",
    "        res.append('''\\\n",
    "/* rule=%(rule)s len[%(len)d]*/''' % {'rule': str(rule), 'len': len(rule)})\n",
    "        if not rule:\n",
    "            res.append('''\n",
    "/*break;*/\n",
    "            ''')\n",
    "        for token in rule:\n",
    "            res.append('''\n",
    "/* token=%(token)s*/''' % {'token': token})\n",
    "            if token in self.grammar:\n",
    "                if token in seen:\n",
    "                    res.append('''\\\n",
    "/* sc */\n",
    "gen_%(key)s(depth+%(depth)d);''' % {'key': self.k_to_s(token), 'depth': depth+2})\n",
    "                else:\n",
    "                    res.append(self.pe_key(token, seen=(seen | {token}), depth=depth+1))\n",
    "            else:\n",
    "                res.append('''\\\n",
    "out('%s');''' % self.esc_char(token))\n",
    "        return '\\n'.join(res)\n",
    "        \n",
    "        \n",
    "    def pe_key(self, key, seen, depth):\n",
    "        if depth == self.MAX_PE_DEPTH:\n",
    "            return 'gen_%(key)s(depth+%(depth)d);' % {'key': self.k_to_s(key), 'depth': depth+1}\n",
    "        rules = self.grammar[key] # ordered by the cost\n",
    "        cheap_strings = self.pool_of_strings[key] \n",
    "        # we haven't restricted map to 256 yet.\n",
    "        result = ['''\\\n",
    "/* %(key)s begins*/\n",
    "if ((depth + %(depth)d) > max_depth) {\n",
    "    int val = map(%(num_cheap_strings)d);\n",
    "    const char* str = pool_%(name)s[val];\n",
    "    const int str_l = pool_l_%(name)s[val];\n",
    "    for (int i = 0; i < str_l; i++) {\n",
    "        out(str[i]);\n",
    "    }\n",
    "} else {''' %  {'name':self.k_to_s(key),\n",
    "                'key': key,\n",
    "                'depth': depth+1,\n",
    "                'num_cheap_strings': len(cheap_strings)}]\n",
    "        if len(rules) == 0:\n",
    "            result.append('''\\\n",
    "    /*indent dummy*/\n",
    "            ''')\n",
    "        elif len(rules) == 1:\n",
    "            result.append('''\\\n",
    "    /*indent inline*/\n",
    "            ''')\n",
    "            result.append(self.add_indent(self.pe_rule(rules[0], seen, depth),'    '))\n",
    "        else:\n",
    "            result.append('''\\\n",
    "    int val = map(%(nrules)d);\n",
    "            ''' % {'nrules': len(rules)})\n",
    "            result.append('''\\\n",
    "    switch(val) {\n",
    "            ''')\n",
    "            for i, rule in enumerate(rules):\n",
    "                result.append('''\\\n",
    "    case %d:\n",
    "        {''' % i)\n",
    "                result.append(self.add_indent(self.pe_rule(rule, seen, depth),'            '))\n",
    "                result.append('''\\\n",
    "            break;\n",
    "        }''')\n",
    "            result.append('''\\\n",
    "    }''')\n",
    "        result.append('''\\\n",
    "}\n",
    "/* %(key)s ends*/''' % {'key': key})\n",
    "        return '\\n'.join(result)\n",
    "        \n",
    "    def gen_rule_src(self, rule, key, i):\n",
    "        res = []\n",
    "        for token in rule:\n",
    "            if token in self.grammar:\n",
    "                if token == key:\n",
    "                    res.append('''\\\n",
    "/* not unrolling*/\n",
    "gen_%(key)s(depth +1);''' % {'key':self.k_to_s(token)})\n",
    "                else:\n",
    "                    res.append('''\\\n",
    "/*indent -<*/''')\n",
    "                    res.append(self.pe_key(token, seen={key, token}, depth=0))\n",
    "                    res.append('''\\\n",
    "/*indent >-*/''')\n",
    "            else:\n",
    "                res.append(\"out('%s');\" % self.esc_char(token))\n",
    "        return '\\n'.join(res)\n",
    "    \n",
    "    def fuzz_src(self, key='<start>'):\n",
    "        self.MAX_PE_DEPTH = 4\n",
    "        return self.gen_main_src(), self.gen_fuzz_src()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_src, fuzz_src = PECFuzzer(c_grammar).fuzz_src()\n",
    "with open('testers/grammar_producer_pec_fuzz.c', 'w+') as f:\n",
    "    print(fuzz_src, file=f)\n",
    "with open('testers/grammar_producer_pec_main.c', 'w+') as f:\n",
    "    print(main_src, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l testers/grammar_producer_pec_fuzz.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat testers/grammar_producer_pec_main.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd testers\n",
    "!cc -Ofast grammar_producer_pec_main.c grammar_producer_pec_fuzz.c  -o grammar_producer_pec\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./testers/grammar_producer_pec 0 10 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPETester(CTester):\n",
    "    def exec_program(self, seed, max_depth, t):\n",
    "        fn = self.ofile(max_depth, seed)\n",
    "        return f\"./testers/grammar_producer_pec {seed} {self.max_num} {max_depth} > {fn}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CPETester().run_test().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supercompile C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# II\n",
    "class CSuperCompiledFuzzer(CFuzzer):\n",
    "\n",
    "    def kr_to_s(self, key, i): return 'gen_%s_%d' % (self.k_to_s(key), i)\n",
    "    \n",
    "    def supercompile_rule(self, key, rule, i_rule, depth):\n",
    "        gen_name = self.kr_to_s(key, i_rule)\n",
    "        if self.rule_recursion[gen_name]:\n",
    "            self.current_lst.append(gen_name)\n",
    "            return '''\\\n",
    "%(gen_name)s(depth_%(depth)d); /* recurse*/''' % {'gen_name':gen_name, 'depth':depth}\n",
    "        res = []\n",
    "        if len(rule) == 0:\n",
    "            res.append('/*pass*/')\n",
    "        else:\n",
    "            for token in rule:\n",
    "                if token not in self.grammar:\n",
    "                    res.append('''\\\n",
    "out('%s');''' % self.esc_char(token))\n",
    "                else:\n",
    "                    res.append(# no indent\n",
    "                        self.supercompile_key(token,\n",
    "                                              depth=(depth+1)))\n",
    "        return '\\n'.join(res)\n",
    "    \n",
    "    def supercompile_key_internal(self, key, trule, i_trule, depth):\n",
    "        if depth > self.MAX_SUPERCOMPILE_DEPTH:\n",
    "            self.current_lst.append(self.kr_to_s(key, i_trule))\n",
    "            return '%(gen_name)s(depth_%(depth)d); /*slimit*/' % {\n",
    "                        'gen_name':self.kr_to_s(key, i_trule), 'depth':depth}\n",
    "        else:\n",
    "            return self.supercompile_rule(key, trule, i_trule, depth=depth)\n",
    "        \n",
    "    def choose_from_cheap_strings(self, key):\n",
    "        cheap_strings = self.pool_of_strings[key]\n",
    "        if len(cheap_strings) == 1:\n",
    "            return '\\n'.join([\"out('%s');\" % c for c in self.cheap_chars(cheap_strings[0])])\n",
    "        l = [len(s) for s in cheap_strings]\n",
    "        if len(set(l)) == 1:\n",
    "            name = ['''\\\n",
    "int val = map(%(num_cheap_strings)d);\n",
    "const char* str = pool_%(name)s[val];''' % {\n",
    "                'name':self.k_to_s(key), 'num_cheap_strings': len(cheap_strings)}]\n",
    "            out = [\"out(str[%d]);\" % i for i in range(l[0])]\n",
    "            return '\\n'.join(name + out)\n",
    "        else:\n",
    "            return '''\\\n",
    "int val = map(%(num_cheap_strings)d);\n",
    "const char* str = pool_%(name)s[val];\n",
    "const int str_l = pool_l_%(name)s[val];\n",
    "for (int i = 0; i < str_l; i++) {\n",
    "    out(str[i]);\n",
    "}\n",
    "    '''%{\n",
    "            'name':self.k_to_s(key),\n",
    "            'num_cheap_strings': len(cheap_strings)}\n",
    "\n",
    "    def supercompile_key(self, key, depth):\n",
    "        # Should check for MAX_SUPERCOMPILE_DEPTH\n",
    "        # should first get the random number curresponding to\n",
    "        # len(grammar[key]) then it should unroll that elif cond.\n",
    "        if len(self.grammar[key]) == 0: return '' # no more jumping on the bed\n",
    "\n",
    "        res = ['''\\\n",
    "if (depth_%(depth)d > max_depth) {\n",
    "%(select_from_pool)s\n",
    "} else {\n",
    "    int depth_%(d_1)d = depth + %(d_1)d;'''%{\n",
    "            'select_from_pool': self.add_indent(self.choose_from_cheap_strings(key), '    '),\n",
    "            'depth': depth,\n",
    "            'd_1': depth+1}]\n",
    "        if len(self.grammar[key]) == 1:\n",
    "            # we do not have to get the random number, and check for\n",
    "            # equality first.\n",
    "            i_trule, trule  = 0, self.grammar[key][0]\n",
    "            res.append(self.add_indent(\n",
    "                self.supercompile_key_internal(key, trule, i_trule, depth),\n",
    "                '''\\\n",
    "    '''))\n",
    "        else:\n",
    "            # First get the random number, then compare and\n",
    "            # unroll\n",
    "            res.append('''\\\n",
    "    int val = map(%(len_rules)d);\n",
    "    switch(val) {''' % {'len_rules': len(self.grammar[key])})\n",
    "            for i_trule, trule in enumerate(self.grammar[key]):\n",
    "                res.append('''\\\n",
    "    case %(i_trule)d:\n",
    "        {''' % {'i_trule': i_trule})\n",
    "                res.append(self.add_indent(\n",
    "                    self.supercompile_key_internal(key, trule, i_trule, depth),\n",
    "                    '''\\\n",
    "            '''))\n",
    "                res.append('''\\\n",
    "            break;\n",
    "        } /*case %d*/''' % i_trule)\n",
    "            res.append('''\\\n",
    "    }/*switch*/''')\n",
    "        res.append('''\\\n",
    "}/*ifelse %d*/''' % depth)\n",
    "        return '\\n'.join(res)\n",
    "   \n",
    "    def gen_rule_src(self, rule, key, i_rule):\n",
    "        res = ['''\\\n",
    "void %(gen_name)s(int depth) {\n",
    "    if (depth > max_depth) {\n",
    "%(select_from_pool)s\n",
    "    } else {\n",
    "        int depth_%(d_1)d = depth + %(d_1)d; ''' % {\n",
    "            'gen_name':self.kr_to_s(key,i_rule),\n",
    "            'select_from_pool': self.add_indent(self.choose_from_cheap_strings(key),'        '),\n",
    "            'depth':0,\n",
    "            'd_1': 1}]\n",
    "        # These should be a sequence of getting randon numbers\n",
    "        # and unrolling appropriately.\n",
    "        for token in rule:\n",
    "            if token not in self.grammar:\n",
    "                res.append('''\\\n",
    "        out('%s');''' % self.esc_char(token))\n",
    "            else:\n",
    "                res.append(self.add_indent(\n",
    "                    self.supercompile_key(token, depth=1), '''\\\n",
    "        '''))\n",
    "        res.append('''\\\n",
    "    } /*else*/\n",
    "} /* %s */''' % self.kr_to_s(key, i_rule))\n",
    "        return '\\n'.join(res)\n",
    "    \n",
    "    # ----  \n",
    " \n",
    "    def fn_fuzz_decs(self):\n",
    "        result = []\n",
    "        for k in self.grammar:\n",
    "            for i,r in enumerate(self.grammar[k]):\n",
    "                result.append('void %(name)s(int depth);' % {'name':self.kr_to_s(k, i)})\n",
    "        return '\\n'.join(result)\n",
    "    \n",
    "    def fuzz_fn_defs(self):\n",
    "        keys_used = {}\n",
    "        result = []\n",
    "        key_defs = {}\n",
    "        for key in self.grammar:\n",
    "            for i_rule,rule in enumerate(self.grammar[key]):\n",
    "                self.current_lst = []\n",
    "                ks = self.kr_to_s(key, i_rule)\n",
    "                keys_used[ks] = self.current_lst\n",
    "                key_defs[ks] = self.gen_rule_src(rule, key, i_rule)\n",
    "        key_set = set(keys_used['gen_start_0']) | {'gen_start_0'}\n",
    "        old_len = 0\n",
    "        while old_len != len(key_set):\n",
    "            old_len = len(key_set)\n",
    "            key_set.update(k1 for k in list(key_set) for k1 in keys_used[k])\n",
    "            \n",
    "        for k in key_set:\n",
    "            result.append(key_defs[k])\n",
    "        return '\\n'.join(result)\n",
    " \n",
    "    def fuzz_entry(self):\n",
    "        return '''\n",
    "void gen_init__() {\n",
    "    gen_start_0(0);\n",
    "    out('\\\\n');\n",
    "    return;\n",
    "}'''\n",
    " \n",
    "    def fuzz_src(self, key='<start>'):\n",
    "        self.MAX_SUPERCOMPILE_DEPTH = 0\n",
    "        return self.gen_main_src(), self.gen_fuzz_src()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going full supercompilation (below) actually reduces the speed by a small %. It is not clear why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSuperCompiledFuzzer(CSuperCompiledFuzzer):\n",
    "    def string_pool_defs(self): return ''\n",
    "    \n",
    "    def choose_from_cheap_strings(self, key):\n",
    "        short = False\n",
    "        cheap_strings = self.pool_of_strings[key]\n",
    "        if len(cheap_strings) == 1:\n",
    "            return '\\n'.join([\"out('%s');\" % self.esc_char(c) for c in cheap_strings[0]])\n",
    "        elif len(cheap_strings) == 0:\n",
    "            return ''\n",
    "        else:\n",
    "            lst = ['''\n",
    "int val = map(%(num_cheap_strings)d);\n",
    "switch(val){'''% {\n",
    "            'name':self.k_to_s(key),\n",
    "            'num_cheap_strings': len(cheap_strings)}]\n",
    "            for i in range(len(cheap_strings)):\n",
    "                lst.append('''\n",
    "case %d:\n",
    "    {''' % i)\n",
    "                lst.extend([\"    out('%s');\" % self.esc_char(c) for c in  cheap_strings[i]])\n",
    "                lst.append('''\n",
    "    break;\n",
    "    }''')\n",
    "            lst.append('''\n",
    "}''')\n",
    "            return '\\n'.join(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_src, fuzz_src = CSuperCompiledFuzzer(c_grammar).fuzz_src()\n",
    "with open('testers/grammar_producer_superc_fuzz.c', 'w+') as f:\n",
    "    print(fuzz_src, file=f)\n",
    "with open('testers/grammar_producer_superc_main.c', 'w+') as f:\n",
    "    print(main_src, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l testers/grammar_producer_superc_fuzz.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat testers/grammar_producer_superc_main.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd testers\n",
    "!cc -Ofast grammar_producer_superc_main.c grammar_producer_superc_fuzz.c  -o grammar_producer_superc\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./testers/grammar_producer_superc 0 10 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSupercompiledTester(CTester):\n",
    "    def exec_program(self, seed, max_depth, t):\n",
    "        fn = self.ofile(max_depth, seed)\n",
    "        return f\"./testers/grammar_producer_superc {seed} {self.max_num} {max_depth} > {fn}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSupercompiledTester().run_test().show() # 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSuperCompiledFuzzer(CFuzzer):\n",
    "    def fuzz_src(self, key='<start>'):\n",
    "        self.MAX_SUPERCOMPILE_DEPTH = 1\n",
    "        return self.gen_main_src(), self.gen_fuzz_src()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd testers\n",
    "!cc -Ofast grammar_producer_superc_main.c grammar_producer_superc_fuzz.c  -o grammar_producer_superc\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSupercompiledTester().run_test().show() # 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSuperCompiledFuzzer(CFuzzer):\n",
    "    def fuzz_src(self, key='<start>'):\n",
    "        self.MAX_SUPERCOMPILE_DEPTH = 2\n",
    "        return self.gen_main_src(), self.gen_fuzz_src()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd testers\n",
    "!cc -Ofast grammar_producer_superc_main.c grammar_producer_superc_fuzz.c  -o grammar_producer_superc\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSupercompiledTester().run_test().show() # 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSuperCompiledFuzzer(CFuzzer):\n",
    "    def fuzz_src(self, key='<start>'):\n",
    "        self.MAX_SUPERCOMPILE_DEPTH = 3\n",
    "        return self.gen_main_src(), self.gen_fuzz_src()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd testers\n",
    "!cc -Ofast grammar_producer_superc_main.c grammar_producer_superc_fuzz.c  -o grammar_producer_superc\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSupercompiledTester().run_test().show() # 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSuperCompiledFuzzer(CFuzzer):\n",
    "    def fuzz_src(self, key='<start>'):\n",
    "        self.MAX_SUPERCOMPILE_DEPTH = 5\n",
    "        return self.gen_main_src(), self.gen_fuzz_src()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd testers\n",
    "!cc -Ofast grammar_producer_superc_main.c grammar_producer_superc_fuzz.c  -o grammar_producer_superc\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSupercompiledTester().run_test().show() # 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSuperCompiledFuzzer(CFuzzer):\n",
    "    def fuzz_src(self, key='<start>'):\n",
    "        self.MAX_SUPERCOMPILE_DEPTH = 10\n",
    "        return self.gen_main_src(), self.gen_fuzz_src()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd testers\n",
    "!cc -Ofast grammar_producer_superc_main.c grammar_producer_superc_fuzz.c  -o grammar_producer_superc\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSupercompiledTester().run_test().show() # 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the random -> choices map faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Replace the division by mulitplication](//https://lemire.me/blog/2016/06/27/a-fast-alternative-to-the-modulo-reduction/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, here is our map function.\n",
    "```c\n",
    "int map(int v) {\n",
    "    return random() % v;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c\n",
    "uint32_t\n",
    "__attribute__((always_inline))\n",
    "map(uint32_t to) {\n",
    "    uint32_t from = random();\n",
    "    return ((uint64_t) from * (uint64_t) to) >> 32;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace random() by a faster pseudo random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Xoshiro**](http://xoshiro.di.unimi.it/xoshiro128starstar.c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://lemire.me/blog/2016/06/27/a-fast-alternative-to-the-modulo-reduction/ \n",
    "class CFuzzerPRNG(CFuzzer):\n",
    "    def fn_map_def(self):\n",
    "        return '''\n",
    "uint64_t next(void);\n",
    "uint32_t map(uint32_t to) {\n",
    "    uint32_t from = next();\n",
    "    return ((uint64_t) from * (uint64_t) to) >> 32;\n",
    "}\n",
    "\n",
    "static inline uint64_t rotl(const uint64_t x, int k) {\n",
    "    return (x << k) | (x >> (64 - k));\n",
    "}\n",
    "static uint64_t r__s[4] = {13343, 9838742, 223185, 802124}; /*TODO: initialize with seed.*/\n",
    "uint64_t next(void) {\n",
    "    const uint64_t result_starstar = rotl(r__s[1] * 5, 7) * 9;\n",
    "\n",
    "    const uint64_t t = r__s[1] << 17;\n",
    "\n",
    "    r__s[2] ^= r__s[0];\n",
    "    r__s[3] ^= r__s[1];\n",
    "    r__s[1] ^= r__s[2];\n",
    "    r__s[0] ^= r__s[3];\n",
    "\n",
    "    r__s[2] ^= t;\n",
    "\n",
    "    r__s[3] = rotl(r__s[3], 45);\n",
    "\n",
    "    return result_starstar;\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_src, fuzz_src = CFuzzerPRNG(c_grammar).fuzz_src()\n",
    "with open('testers/grammar_producer_cprng_main.c', 'w+') as f:\n",
    "    print(main_src, file=f)\n",
    "with open('testers/grammar_producer_cprng_fuzz.c', 'w+') as f:\n",
    "    print(fuzz_src, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat testers/grammar_producer_cprng_main.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd testers\n",
    "!cc -Ofast grammar_producer_cprng_main.c grammar_producer_cprng_fuzz.c -o grammar_producer_cprng\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTesterPRNG(CTester):\n",
    "    def exec_program(self, seed, max_depth, t):\n",
    "        fn = self.ofile(max_depth, seed)\n",
    "        return f\"./testers/grammar_producer_cprng {seed} {self.max_num} {max_depth} > {fn}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./testers/grammar_producer_cprng 0 10 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTesterPRNG().run_test().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we make the random go faster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**idea**: Do the random allocation in one place, and use that later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How fast is /dev/random (and variants)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the best block size, and fastest #counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with timeit() as t:\n",
    "    !dd if=/dev/random of=random.x bs=1024 count=1000 2>/dev/null\n",
    "print(\"throughput=\",os.stat('random.x').st_size/1024/t.runtime, 'kb per second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with timeit() as t:\n",
    "    !dd if=/dev/urandom of=random.x bs=1024 count=1000 2>/dev/null\n",
    "print(\"throughput=\",os.stat('random.x').st_size/1024/t.runtime, 'kb per second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with timeit() as t:\n",
    "    !dd if=/dev/zero of=io.x bs=1024 count=1000 2>/dev/null\n",
    "print(\"throughput=\",os.stat('io.x').st_size/1024/t.runtime, 'kb per second')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Idea**:\n",
    "* Pre-allocate random bits, and use only as much as necessary.\n",
    "* Optimize for < 256 bits by using only a single byte at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c\n",
    "uint8_t\n",
    "map(uint8_t to) {\n",
    "    uint8_t from = rand_region[rand_cursor++];\n",
    "    if (rand_cursor >= rand_region_size)\n",
    "        rand_cursor = 0;\n",
    "    return ((uint16_t) from * (uint16_t) to) >> 8;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**idea**: Wraparound at 4 GB to avoid comparisons (this did not work as expected.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_max_int = 4096 * 1024 * 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The better idea is to use a pointer to the last element, and increment it rather than use an array address, which is required for this trick.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we make random faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_str = '''\n",
    "#include <stdlib.h>\n",
    "#include <stdio.h>\n",
    "#include <unistd.h>\n",
    "#include <limits.h>\n",
    "#include <fcntl.h>\n",
    "#include <sys/types.h>\n",
    "#include <sys/mman.h>\n",
    "#include <sys/stat.h>\n",
    "#include <math.h>\n",
    "#include <errno.h>\n",
    "#include <string.h>\n",
    "\n",
    "uint8_t* rand_region;\n",
    "void* stack[INT_MAX];\n",
    "\n",
    "static inline uint64_t rotl(const uint64_t x, int k) {\n",
    "    return (x << k) | (x >> (64 - k));\n",
    "}\n",
    "static uint64_t r__s[4] = {13343, 9838742, 223185, 802124}; /*TODO: initialize with seed.*/\n",
    "uint64_t\n",
    "next(void) {\n",
    "    const uint64_t result_starstar = rotl(r__s[1] * 5, 7) * 9;\n",
    "\n",
    "    const uint64_t t = r__s[1] << 17;\n",
    "\n",
    "    r__s[2] ^= r__s[0];\n",
    "    r__s[3] ^= r__s[1];\n",
    "    r__s[1] ^= r__s[2];\n",
    "    r__s[0] ^= r__s[3];\n",
    "\n",
    "    r__s[2] ^= t;\n",
    "\n",
    "    r__s[3] = rotl(r__s[3], 45);\n",
    "\n",
    "    return result_starstar;\n",
    "}\n",
    "\n",
    "uint8_t* rand_region_size = 0;\n",
    "\n",
    "void\n",
    "__attribute__((flatten))\n",
    "initialize_random(uint64_t max_chars) {\n",
    "    uint64_t* arr = (uint64_t*) rand_region;\n",
    "    uint64_t i;\n",
    "    for (i=0; i < max_chars/8; i++) { /*max_space/8 because we have 8 bytes*/\n",
    "        arr[i] = next();\n",
    "    }\n",
    "    rand_region_size = (uint8_t*) (arr+i);\n",
    "}\n",
    "\n",
    "int main(int argc, char** argv) {\n",
    "    struct stat st;\n",
    "    int rand_fd;\n",
    "    uint8_t* rand_region_init;\n",
    "\n",
    "    char* rand_file = argv[1];\n",
    "    rand_fd = open(rand_file, O_RDWR | O_CREAT, 0600);\n",
    "    size_t u_max = (uint64_t)powl(2,32);\n",
    "    int res = ftruncate(rand_fd, u_max);\n",
    "    if (res == -1) {\n",
    "        fprintf(stdout, \"Error: %s\\\\n\", strerror(errno));\n",
    "        return 4;\n",
    "    }\n",
    "    rand_region = mmap(0, u_max, PROT_READ| PROT_WRITE, MAP_SHARED, rand_fd, 0);\n",
    "    rand_region_init = rand_region;\n",
    "    if (rand_region == (uint8_t*)-1) {\n",
    "        exit(3);\n",
    "    }\n",
    "    initialize_random(u_max);\n",
    "    msync(rand_region, st.st_size, MS_SYNC);\n",
    "    munmap(rand_region, st.st_size);\n",
    "    long rand_size = rand_region_size - rand_region_init;\n",
    "    ftruncate(rand_fd, rand_size);\n",
    "    /*fprintf(stdout, \"%ld\\\\n\", rand_size);*/\n",
    "    close(rand_fd);\n",
    "    return 0;\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('testers/rand.c', 'w+') as f:\n",
    "    print(my_str, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd testers\n",
    "!cc -g -o rand -Ofast rand.c\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with timeit() as t:\n",
    "    !./testers/rand random1.x\n",
    "print(\"throughput=\",os.stat('random1.x').st_size/1024/t.runtime)\n",
    "!rm random1.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allocate a file `u_max_int` size, and mmap it to memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c\n",
    "uint32_t rand_cursor = 0;\n",
    "uint8_t\n",
    "__attribute__((always_inline))\n",
    "map(uint8_t to) {\n",
    "    uint8_t from = rand_region[rand_cursor++];\n",
    "    return ((uint16_t) from * (uint16_t) to) >> 8;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "# II\n",
    "# https://lemire.me/blog/2016/06/27/a-fast-alternative-to-the-modulo-reduction/\n",
    "class CFuzzerExtRand(CFuzzer):\n",
    "    def main_hdefs(self):\n",
    "        s = super().main_hdefs()\n",
    "        return s + '''\n",
    "#include <unistd.h>\n",
    "#include <limits.h>\n",
    "#include <fcntl.h>\n",
    "#include <sys/types.h>\n",
    "#include <sys/mman.h>\n",
    "#include <sys/stat.h>\n",
    "#include <math.h>\n",
    "'''\n",
    "    \n",
    "    def fn_map_def(self):\n",
    "        return '''\n",
    "uint8_t\n",
    "__attribute__((always_inline))\n",
    "map(uint8_t to) {\n",
    "    uint8_t from = rand_regionp[rand_cursor++];\n",
    "    if (rand_cursor >= rand_region_size)\n",
    "        rand_cursor = 0;\n",
    "    return ((uint16_t) from * (uint16_t) to) >> 8;\n",
    "}\n",
    "\n",
    "static inline uint64_t rotl(const uint64_t x, int k) {\n",
    "    return (x << k) | (x >> (64 - k));\n",
    "}\n",
    "static uint64_t r__s[4] = {13343, 9838742, 223185, 802124}; /*TODO: initialize with seed.*/\n",
    "uint64_t\n",
    "next(void) {\n",
    "    const uint64_t result_starstar = rotl(r__s[1] * 5, 7) * 9;\n",
    "\n",
    "    const uint64_t t = r__s[1] << 17;\n",
    "\n",
    "    r__s[2] ^= r__s[0];\n",
    "    r__s[3] ^= r__s[1];\n",
    "    r__s[1] ^= r__s[2];\n",
    "    r__s[0] ^= r__s[3];\n",
    "\n",
    "    r__s[2] ^= t;\n",
    "\n",
    "    r__s[3] = rotl(r__s[3], 45);\n",
    "\n",
    "    return result_starstar;\n",
    "}\n",
    "\n",
    "uint8_t* rand_region_sizep = 0;\n",
    "\n",
    "void\n",
    "__attribute__((flatten))\n",
    "initialize_random(uint64_t max_chars) {\n",
    "    uint64_t* arr = (uint64_t*) rand_regionp;\n",
    "    uint64_t i;\n",
    "    for (i=0; i < max_chars/8; i++) { /*max_space/8 because we have 8 bytes*/\n",
    "        arr[i] = next();\n",
    "    }\n",
    "    rand_region_sizep = (uint8_t*) (arr+i);\n",
    "}\n",
    "'''\n",
    "    def main_rand_var_defs(self):\n",
    "        return '''\n",
    "const uint64_t rand_region_size = 1ULL << 16;\n",
    "uint8_t rand_regionp[rand_region_size];\n",
    "uint64_t rand_cursor = 0;\n",
    "'''\n",
    "    \n",
    "    def main_var_defs(self):\n",
    "        s = super().main_var_defs()\n",
    "        return s + self.main_rand_var_defs()\n",
    "\n",
    "    def fuzz_hdefs(self):\n",
    "        s = super().fuzz_hdefs()\n",
    "        return s + '''\n",
    "#include <unistd.h>\n",
    "#include <stdint.h>'''\n",
    "    \n",
    "    def fuzz_rand_var_defs(self):\n",
    "        return '''\n",
    "extern uint8_t* rand_regionp;\n",
    "extern uint64_t rand_cursor;\n",
    "extern uint64_t rand_region_size;\n",
    "uint8_t map(uint8_t to);'''\n",
    " \n",
    "    def fn_main_rand_frag(self):\n",
    "        return '''\\\n",
    "    initialize_random(rand_region_size);\n",
    "    rand_cursor = seed;\n",
    "    '''\n",
    " \n",
    "    def fn_main_def(self):\n",
    "        return '''\n",
    "int main(int argc, char** argv) {\n",
    "    struct stat st;\n",
    "    int max_num, seed, rand_fd, out_fd;\n",
    "    long out_size;\n",
    "%(input_frag)s\n",
    "%(rand_frag)s\n",
    "%(loop_frag)s\n",
    "    return 0;\n",
    "}''' % { 'input_frag': self.fn_main_input_frag(),\n",
    "         'loop_frag': self.fn_main_loop_frag(),\n",
    "         'rand_frag': self.fn_main_rand_frag()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "main_src, fuzz_src = CFuzzerExtRand(c_grammar).fuzz_src()\n",
    "with open('testers/grammar_producer_cprngextr_main.c', 'w+') as f:\n",
    "    print(main_src, file=f)\n",
    "with open('testers/grammar_producer_cprngextr_fuzz.c', 'w+') as f:\n",
    "    print(fuzz_src, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat testers/grammar_producer_cprngextr_fuzz.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nl -ba testers/grammar_producer_cprngextr_main.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd testers\n",
    "!cc -g -Ofast -o grammar_producer_cprngextr grammar_producer_cprngextr_main.c grammar_producer_cprngextr_fuzz.c\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# II\n",
    "!./testers/grammar_producer_cprngextr 0 10 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# II\n",
    "class CTesterPRNGExt(CTester):\n",
    "    def exec_program(self, seed, max_depth, t):\n",
    "        fn = self.ofile(max_depth, seed)\n",
    "        return f\"./testers/grammar_producer_cprngextr {seed} {self.max_num} {max_depth} ./random.x > {fn}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTesterPRNGExt().run_test().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# II\n",
    "class CFuzzerExtRandP(CFuzzerExtRand):\n",
    "    def fn_map_def(self):\n",
    "        return '''\n",
    "uint8_t\n",
    "__attribute__((always_inline))\n",
    "map(uint8_t to) {\n",
    "    uint8_t from = *rand_regionp++;\n",
    "    if (rand_regionp >= rand_region_sizep)\n",
    "        rand_regionp = rand_region_initp;\n",
    "    return ((uint16_t) from * (uint16_t) to) >> 8;\n",
    "}\n",
    "\n",
    "\n",
    "static inline uint64_t rotl(const uint64_t x, int k) {\n",
    "    return (x << k) | (x >> (64 - k));\n",
    "}\n",
    "static uint64_t r__s[4] = {13343, 9838742, 223185, 802124}; /*TODO: initialize with seed.*/\n",
    "uint64_t\n",
    "next(void) {\n",
    "    const uint64_t result_starstar = rotl(r__s[1] * 5, 7) * 9;\n",
    "\n",
    "    const uint64_t t = r__s[1] << 17;\n",
    "\n",
    "    r__s[2] ^= r__s[0];\n",
    "    r__s[3] ^= r__s[1];\n",
    "    r__s[1] ^= r__s[2];\n",
    "    r__s[0] ^= r__s[3];\n",
    "\n",
    "    r__s[2] ^= t;\n",
    "\n",
    "    r__s[3] = rotl(r__s[3], 45);\n",
    "\n",
    "    return result_starstar;\n",
    "}\n",
    "\n",
    "void\n",
    "__attribute__((flatten))\n",
    "initialize_random(uint64_t max_chars) {\n",
    "    uint64_t* arr = (uint64_t*) rand_regionp;\n",
    "    uint64_t i;\n",
    "    for (i=0; i < max_chars/8; i++) { /*max_space/8 because we have 8 bytes*/\n",
    "        arr[i] = next();\n",
    "    }\n",
    "    rand_region_sizep = (uint8_t*) (arr+i);\n",
    "}\n",
    "'''\n",
    "    def main_rand_var_defs(self):\n",
    "        return '''\n",
    "uint8_t* rand_region_sizep = 0;\n",
    "const uint64_t rand_region_size = 1ULL << 16;\n",
    "uint8_t rand_region_initp[rand_region_size];\n",
    "\n",
    "uint8_t* rand_regionp = rand_region_initp;\n",
    "'''\n",
    "    def fuzz_rand_var_defs(self):\n",
    "        return '''\n",
    "uint8_t map(uint8_t to);\n",
    "'''\n",
    "    \n",
    "    def fn_main_loop_frag(self):\n",
    "        return '''\n",
    "    for (int i = 0; i < max_num; i++) {\n",
    "        gen_init__();\n",
    "    }\n",
    "'''\n",
    "    def fn_main_rand_frag(self):\n",
    "        return '''\\\n",
    "    initialize_random(rand_region_size);\n",
    "    rand_regionp += seed;\n",
    "    '''\n",
    "    def fn_main_def(self):\n",
    "        return '''\n",
    "int main(int argc, char** argv) {\n",
    "    struct stat st;\n",
    "    long out_size;\n",
    "    char* out_region_sizep = 0;\n",
    "    char* out_region_initp;\n",
    "    int out_fd;\n",
    "    int seed, max_num;\n",
    "%(input_frag)s\n",
    "%(rand_frag)s\n",
    "%(loop_frag)s\n",
    "    return 0;\n",
    "}''' % {'input_frag': self.fn_main_input_frag(),\n",
    "        'rand_frag': self.fn_main_rand_frag(),\n",
    "        'loop_frag': self.fn_main_loop_frag()\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_src, fuzz_src = CFuzzerExtRandP(c_grammar).fuzz_src()\n",
    "with open('testers/grammar_producer_cprngextrP_main.c', 'w+') as f:\n",
    "    print(main_src, file=f)\n",
    "with open('testers/grammar_producer_cprngextrP_fuzz.c', 'w+') as f:\n",
    "    print(fuzz_src, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat testers/grammar_producer_cprngextrP_main.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat testers/grammar_producer_cprngextrP_fuzz.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd testers\n",
    "!cc -g -Ofast -o grammar_producer_cprngextrP grammar_producer_cprngextrP_main.c grammar_producer_cprngextrP_fuzz.c\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./testers/grammar_producer_cprngextrP 0 10 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# II\n",
    "class CTesterPRNGExtP(CTester):\n",
    "    def exec_program(self, seed, max_depth, t):\n",
    "        fn = self.ofile(max_depth, seed)\n",
    "        return f\"./testers/grammar_producer_cprngextrP {seed} {self.max_num} {max_depth} > {fn}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTesterPRNGExtP().run_test().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using faster IO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Idea**:\n",
    "* `mmap` to a file, write the bits, and `ftruncate()` to the new size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# II\n",
    "class CMMapIOFuzzer(CFuzzerExtRandP):\n",
    "    def main_out_var_defs(self):\n",
    "        return '''\n",
    "char* out_regionp;\n",
    "uint64_t out_cursor = 0;\n",
    "'''\n",
    "    def main_var_defs(self):\n",
    "        s = super().main_var_defs()\n",
    "        return s + self.main_out_var_defs()\n",
    "     \n",
    "    def fn_out_def(self):\n",
    "        return '''\n",
    "void\n",
    "__attribute__((always_inline))\n",
    "out(char c) {\n",
    "    out_regionp[out_cursor++] = c;\n",
    "}\n",
    "'''\n",
    "    \n",
    "    def fuzz_out_var_defs(self):\n",
    "        return '''\n",
    "void out(char c);\n",
    "extern char* out_regionp;\n",
    "extern uint64_t out_cursor;\n",
    "'''\n",
    "\n",
    "    def fn_main_input_frag(self):\n",
    "        return '''\n",
    "    if (argc < 3) {\n",
    "        printf(\"%s <seed> <max_num> <max_depth>\\\\n\", argv[0]);\n",
    "        return 0;\n",
    "    }\n",
    "    seed = atoi(argv[1]);\n",
    "    max_num = atoi(argv[2]);\n",
    "    max_depth = atoi(argv[3]);'''\n",
    "\n",
    "    def fn_main_out_frag(self):\n",
    "        return '''\n",
    "    char* iomax = getenv(\"IO_LIMIT\");\n",
    "    uint64_t u_iomax = UINT_MAX * 10ULL; // 40G\n",
    "    if (iomax) {\n",
    "        u_iomax = 1ULL << atoi(iomax);\n",
    "    }\n",
    "    if (argc > 4) {\n",
    "        out_fd = open(argv[4], O_RDWR | O_CREAT, 0600);\n",
    "    } else {\n",
    "        out_fd = open(\"io.x\", O_RDWR | O_CREAT, 0600);\n",
    "    }\n",
    "    if (iomax) {\n",
    "        int res = ftruncate(out_fd, u_iomax);\n",
    "        if (res != 0) {\n",
    "            perror(\"truncate failed\");\n",
    "            exit(2);\n",
    "        }\n",
    "    } else {\n",
    "        int res = try_truncate(out_fd);\n",
    "        if (res < 32) {\n",
    "            perror(\"truncate failed\");\n",
    "            fprintf(stderr,\"%d\\\\n\", res);\n",
    "            exit(5);\n",
    "        }\n",
    "    }\n",
    "    fstat(out_fd, &st);\n",
    "    out_regionp = mmap(0, st.st_size, PROT_READ|\n",
    "                      PROT_WRITE, MAP_SHARED, out_fd, 0);\n",
    "    if (out_regionp == (caddr_t)-1) {\n",
    "        exit(3);\n",
    "    }\n",
    "    '''\n",
    "    \n",
    "    def fn_main_sync_frag(self):\n",
    "        return '''\n",
    "    msync(out_regionp, st.st_size, MS_SYNC);\n",
    "    munmap(out_regionp, st.st_size);\n",
    "    ftruncate(out_fd, out_cursor);\n",
    "    close(out_fd);\n",
    "'''\n",
    "\n",
    "    def fn_truncateio(self):\n",
    "        return '''\n",
    "#include <errno.h>\n",
    "int try_truncate(int fd) {\n",
    "    for (off_t len = 63; len > 0; len--) {\n",
    "      uint64_t m = 1ULL << len;\n",
    "      errno = 0;\n",
    "      int ret = ftruncate(fd, m);\n",
    "      if (ret == 0) {\n",
    "        return len;\n",
    "      }\n",
    "    }\n",
    "    return -1;\n",
    "}\n",
    "'''\n",
    "    \n",
    "    def fn_main_def(self):\n",
    "        return self.fn_truncateio() + '''\n",
    "int main(int argc, char** argv) {\n",
    "    struct stat st;\n",
    "    int rand_fd, out_fd;\n",
    "    int seed, max_num;\n",
    "%(input_frag)s\n",
    "%(rand_frag)s\n",
    "%(out_frag)s\n",
    "%(loop_frag)s\n",
    "%(sync_frag)s\n",
    "    return 0;\n",
    "}''' % {'input_frag': self.fn_main_input_frag(),\n",
    "        'out_frag': self.fn_main_out_frag(),\n",
    "        'sync_frag': self.fn_main_sync_frag(),\n",
    "        'rand_frag': self.fn_main_rand_frag(),\n",
    "        'loop_frag': self.fn_main_loop_frag(),\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_src, fuzz_src = CMMapIOFuzzer(c_grammar).fuzz_src()\n",
    "with open('./testers/grammar_producer_mmapio_main.c', 'w+') as f:\n",
    "    print(main_src, file=f)\n",
    "with open('./testers/grammar_producer_mmapio_fuzz.c', 'w+') as f:\n",
    "    print(fuzz_src, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat testers/grammar_producer_mmapio_main.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd testers\n",
    "!cc -g -Ofast -o grammar_producer_mmapio grammar_producer_mmapio_main.c grammar_producer_mmapio_fuzz.c\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# II\n",
    "!./testers/grammar_producer_mmapio 0 10 10 io.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat io.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# II\n",
    "class CTesterMMap(CTester):\n",
    "    def exec_program(self, seed, max_depth, t):\n",
    "        fn = self.ofile(max_depth, seed)\n",
    "        return f\"./testers/grammar_producer_mmapio {seed} {self.max_num} {max_depth} {fn}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTesterMMap().run_test().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FWrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "class CFWriteFuzzer(CFuzzerExtRandP):\n",
    "    def main_out_var_defs(self):\n",
    "        return '''\n",
    "const uint64_t size = UINT_MAX; /*max size of a single input -- 4G*/\n",
    "char out_region_initp[size];\n",
    "char *out_regionp = out_region_initp;\n",
    "uint64_t out_cursor = 0;\n",
    "FILE* fs;\n",
    "'''\n",
    "    def main_var_defs(self):\n",
    "        s = super().main_var_defs()\n",
    "        return s + self.main_out_var_defs()\n",
    "     \n",
    "    def fn_out_def(self):\n",
    "        return '''\n",
    "void\n",
    "__attribute__((always_inline))\n",
    "out(char c) {\n",
    "    out_regionp[out_cursor++] = c;\n",
    "}'''\n",
    "    \n",
    "    def fuzz_out_var_defs(self):\n",
    "        return '''\n",
    "void out(char c);\n",
    "extern char* out_regionp;\n",
    "extern uint64_t out_cursor;\n",
    "'''\n",
    "\n",
    "    def fn_main_input_frag(self):\n",
    "        return '''\n",
    "    if (argc < 3) {\n",
    "        printf(\"%s <seed> <max_num> <max_depth>\\\\n\", argv[0]);\n",
    "        return 0;\n",
    "    }\n",
    "    seed = atoi(argv[1]);\n",
    "    max_num = atoi(argv[2]);\n",
    "    max_depth = atoi(argv[3]);'''\n",
    "\n",
    "    def fn_main_out_frag(self):\n",
    "        return '''\n",
    "    if (argc > 4) {\n",
    "        out_fd = open(argv[4], O_RDWR | O_CREAT, 0600);\n",
    "    } else {\n",
    "        out_fd = open(\"io.x\", O_RDWR | O_CREAT, 0600);\n",
    "    }\n",
    "    fs = fdopen(out_fd, \"w\");\n",
    "'''\n",
    "\n",
    "    def fn_main_sync_frag(self):\n",
    "        return '''\n",
    "    fclose(fs);\n",
    "    close(out_fd);\n",
    "'''\n",
    "\n",
    "    def fn_truncateio(self):\n",
    "        return '''\n",
    "'''\n",
    "    def fn_main_loop_frag(self):\n",
    "        return '''\n",
    "    for(int i=0; i < max_num; i++) {\n",
    "        gen_init__();\n",
    "        fwrite(out_regionp, sizeof(char), out_cursor, fs);\n",
    "        out_cursor = 0;\n",
    "    }\n",
    "'''\n",
    "\n",
    "    def fn_main_def(self):\n",
    "        return self.fn_truncateio() + '''\n",
    "int main(int argc, char** argv) {\n",
    "    struct stat st;\n",
    "    int rand_fd, out_fd;\n",
    "    int seed, max_num;\n",
    "%(input_frag)s\n",
    "%(rand_frag)s\n",
    "%(out_frag)s\n",
    "%(loop_frag)s\n",
    "%(sync_frag)s\n",
    "    return 0;\n",
    "}''' % {'input_frag': self.fn_main_input_frag(),\n",
    "        'out_frag': self.fn_main_out_frag(),\n",
    "        'sync_frag': self.fn_main_sync_frag(),\n",
    "        'rand_frag': self.fn_main_rand_frag(),\n",
    "        'loop_frag': self.fn_main_loop_frag(),\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "main_src, fuzz_src = CFWriteFuzzer(c_grammar).fuzz_src()\n",
    "with open('./testers/grammar_producer_fwrite_main.c', 'w+') as f:\n",
    "    print(main_src, file=f)\n",
    "with open('./testers/grammar_producer_fwrite_fuzz.c', 'w+') as f:\n",
    "    print(fuzz_src, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat testers/grammar_producer_fwrite_fuzz.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat testers/grammar_producer_fwrite_main.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd testers\n",
    "!cc -g -Ofast -o grammar_producer_fwrite grammar_producer_fwrite_main.c grammar_producer_fwrite_fuzz.c\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# II\n",
    "!./testers/grammar_producer_fwrite 0 10 10 io.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat io.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# II\n",
    "class CTesterFWrite(CTester):\n",
    "    def exec_program(self, seed, max_depth, t):\n",
    "        fn = self.ofile(max_depth, seed)\n",
    "        return f\"./testers/grammar_producer_fwrite {seed} {self.max_num} {max_depth} {fn}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTesterFWrite().run_test().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNoWriteFuzzer(CFuzzerExtRandP):\n",
    "    def main_out_var_defs(self):\n",
    "        return '''\n",
    "const uint64_t size = UINT_MAX; // size of a single output item -- 4G\n",
    "char out_region_initp[size];\n",
    "char *out_regionp = out_region_initp;\n",
    "uint64_t out_cursor = 0;'''\n",
    "    def main_var_defs(self):\n",
    "        s = super().main_var_defs()\n",
    "        return s + self.main_out_var_defs()\n",
    "     \n",
    "    def fn_out_def(self):\n",
    "        return '''\n",
    "void\n",
    "__attribute__((always_inline))\n",
    "out(char c) {\n",
    "    out_regionp[out_cursor++] = c;\n",
    "}'''\n",
    "    \n",
    "    def fuzz_out_var_defs(self):\n",
    "        return '''\\\n",
    "void out(char c);\n",
    "extern char* out_regionp;\n",
    "extern uint64_t out_cursor;\n",
    "'''\n",
    "\n",
    "    def fn_main_input_frag(self):\n",
    "        return '''\n",
    "    if (argc < 3) {\n",
    "        printf(\"%s <seed> <max_num> <max_depth>\\\\n\", argv[0]);\n",
    "        return 0;\n",
    "    }\n",
    "    seed = atoi(argv[1]);\n",
    "    max_num = atoi(argv[2]);\n",
    "    max_depth = atoi(argv[3]);'''\n",
    "\n",
    "    def fn_main_out_frag(self):\n",
    "        return '''\n",
    "    '''\n",
    "    \n",
    "    def fn_main_sync_frag(self):\n",
    "        return '''\n",
    "    '''\n",
    "\n",
    "    def fn_truncateio(self):\n",
    "        return '''\n",
    "        '''\n",
    "    def fn_main_loop_frag(self):\n",
    "        return '''\n",
    "    uint64_t out_size = 0;\n",
    "    for(int i=0; i < max_num; i++) {\n",
    "        gen_init__();\n",
    "        // throw away\n",
    "        out_size += out_cursor;\n",
    "        out_cursor = 0;\n",
    "    }\n",
    "    printf(\"%lld\\\\n\", out_size);\n",
    "    '''\n",
    "\n",
    "    def fn_main_def(self):\n",
    "        return self.fn_truncateio() + '''\n",
    "int main(int argc, char** argv) {\n",
    "    struct stat st;\n",
    "    int rand_fd;\n",
    "    int seed, max_num;\n",
    "%(input_frag)s\n",
    "%(rand_frag)s\n",
    "%(out_frag)s\n",
    "%(loop_frag)s\n",
    "%(sync_frag)s\n",
    "    return 0;\n",
    "}''' % {'input_frag': self.fn_main_input_frag(),\n",
    "        'out_frag': self.fn_main_out_frag(),\n",
    "        'sync_frag': self.fn_main_sync_frag(),\n",
    "        'rand_frag': self.fn_main_rand_frag(),\n",
    "        'loop_frag': self.fn_main_loop_frag(),\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_src, fuzz_src = CNoWriteFuzzer(c_grammar).fuzz_src()\n",
    "with open('./testers/grammar_producer_nowrite_main.c', 'w+') as f:\n",
    "    print(main_src, file=f)\n",
    "with open('./testers/grammar_producer_nowrite_fuzz.c', 'w+') as f:\n",
    "    print(fuzz_src, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd testers\n",
    "!cc -g -Ofast -o grammar_producer_nowrite grammar_producer_nowrite_main.c grammar_producer_nowrite_fuzz.c\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# II\n",
    "!./testers/grammar_producer_nowrite 0 10 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# II\n",
    "class CTesterNoWrite(CTester):\n",
    "    def exec_program(self, seed, max_depth, t):\n",
    "        fn = self.ofile(max_depth, seed)\n",
    "        return f\"./testers/grammar_producer_nowrite {seed} {self.max_num} {max_depth} > {fn}\"\n",
    "  \n",
    "    def post_time(self):\n",
    "        super().post_time()\n",
    "        with open(self.file) as f:\n",
    "            self.size = int(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTesterNoWrite().run_test().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuzzer as a VM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Direct threaded VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# II\n",
    "class DTMMapFuzzer(CMMapIOFuzzer):\n",
    "    def fn_out_def(self): return ''\n",
    "    def gen_rule_src(self, rule, k, j):\n",
    "        res = []\n",
    "        leaf = True\n",
    "        for i, token in enumerate(rule):\n",
    "            if token in self.grammar:\n",
    "                leaf = False\n",
    "                trules = self.grammar[token] # ordered by cost\n",
    "                len_min_choices = len(self.c_grammar[token])\n",
    "                assert len(trules) < 256\n",
    "                cheap_strings = self.pool_of_strings[token]\n",
    "                if len(cheap_strings) < 256: # we only have 255 random choices\n",
    "                    check_pool = '''\n",
    "        val = map(%(len_cheap_strings)s);\n",
    "        const char* str = pool_%(k)s[val];\n",
    "        const int str_l = pool_l_%(k)s[val];\n",
    "        for (int i = 0; i < str_l; i++) {\n",
    "            *out_regionp++ = str[i];\n",
    "        }\n",
    "        --returnp;\n",
    "        goto **returnp; \n",
    "            ''' % { 'len_cheap_strings': len(cheap_strings), 'k': self.k_to_s(token)}\n",
    "                else:\n",
    "                    check_pool = '''\n",
    "        val = map(%(len_min_choices)s);\n",
    "                ''' % {'len_min_choices':len_min_choices}\n",
    "                res.append('''\\\n",
    "    *returnp = &&return__%(i)d__%(j)d__%(k)s;\n",
    "    if (returnp > max_depthp) {\n",
    "        %(check_pool)s;\n",
    "    } else {\n",
    "        val = map(%(len_rules)s);\n",
    "    }\n",
    "    goto *gen_%(t)s[val];\n",
    "return__%(i)d__%(j)d__%(k)s:;\n",
    "            ''' % {'i':i, 'j':j, 'k':self.k_to_s(k),\n",
    "                   't':self.k_to_s(token), 'rnum':0, 'len_rules':len(trules), 'len_min_choices':len_min_choices, 'check_pool':check_pool})\n",
    "            else:\n",
    "                res.append('''\\\n",
    "    *out_regionp++ = '%s';''' % self.esc_char(token))\n",
    "        return res, leaf\n",
    "    \n",
    "    def gen_alt_src_1rule(self, k):\n",
    "        rule = self.grammar[k][0]\n",
    "        ri = 0\n",
    "        src, leaf = self.gen_rule_src(rule, k, ri)\n",
    "        body = '\\n'.join(src)\n",
    "        result = []\n",
    "        if leaf:\n",
    "            return '''\n",
    "gen_%(name)s_0: {\n",
    "%(body)s\n",
    "    goto **returnp;\n",
    "}''' % {'name':self.k_to_s(k), 'body':body}\n",
    "        else:\n",
    "             return '''\n",
    "gen_%(name)s_0: {\n",
    "    ++returnp;\n",
    "    // single -- no switch\n",
    "%(body)s\n",
    "    --returnp;\n",
    "    goto **returnp;\n",
    "}''' % {'name':self.k_to_s(k), 'body':body}\n",
    "\n",
    "    def gen_alt_src(self, k):\n",
    "        rules = self.grammar[k]\n",
    "        ret = self.k_to_s(k)\n",
    "        result = []\n",
    "        if len(rules) == 1: return self.gen_alt_src_1rule(k)\n",
    "        for ri, rule in enumerate(rules):\n",
    "            src, leaf = self.gen_rule_src(rule, k, ri)\n",
    "            body = '\\n'.join(src)\n",
    "            if leaf:\n",
    "                result.append('''\n",
    "gen_%(name)s_%(rnum)d: {\n",
    "%(body)s\n",
    "    goto **returnp;\n",
    "}\n",
    "    ''' % {'name': self.k_to_s(k), 'rnum': ri, 'body':body})\n",
    "            else:\n",
    "                 result.append('''\n",
    "gen_%(name)s_%(rnum)d: {\n",
    "    ++returnp;\n",
    "%(body)s\n",
    "    --returnp;\n",
    "    goto **returnp;\n",
    "}\n",
    "    ''' % {'name': self.k_to_s(k), 'rnum': ri, 'body':body})\n",
    "        return '\\n'.join(result)\n",
    "\n",
    "    def fuzz_out_var_defs(self):\n",
    "        return '''\\\n",
    "extern char* out_regionp;'''\n",
    "    \n",
    "    def fuzz_rand_var_defs(self):\n",
    "        return '''\n",
    "uint8_t map(uint8_t to);'''\n",
    "    \n",
    "    def fuzz_stack_var_defs(self):\n",
    "        return '''\n",
    "extern void* stackp[];\n",
    "'''\n",
    "\n",
    "    def fuzz_entry(self):\n",
    "        result = ['''\n",
    "void gen_init__(void** max_depthp) {\n",
    "    uint8_t val;\n",
    "    void** returnp = stackp;\n",
    "    *returnp =  &&return__init;\n",
    "''']\n",
    "        for k in self.grammar:\n",
    "            l = []\n",
    "            for ri,rule in enumerate(self.grammar[k]):\n",
    "                l.append('&&gen_%(k)s_%(ri)d' % {'k':self.k_to_s(k), 'ri':ri})\n",
    "            s = '''\n",
    "    void** gen_%(k)s[] = {\n",
    "%(body)s\n",
    "    };''' % {'k': self.k_to_s(k), 'body': ',\\n'.join(l)}\n",
    "            result.append(s)\n",
    "        result.append('''\n",
    "    goto gen_start_0;''')\n",
    "        result.append(self.fuzz_fn_defs())\n",
    "        result.append(\"\"\"\n",
    "return__init:\n",
    "    return;\n",
    "return_abort:\n",
    "    exit(10); \n",
    "}\"\"\")\n",
    "        return '\\n'.join(result)\n",
    "    \n",
    "    def main_out_var_defs(self):\n",
    "        return'''\n",
    "char* out_regionp;\n",
    "int out_cursor;\n",
    "'''\n",
    "    \n",
    "    def main_stack_var_defs(self):\n",
    "        return'''\n",
    "int max_depth;\n",
    "void** max_depthp;\n",
    "void* stackp[INT_MAX];\n",
    "'''\n",
    "    def main_init_var_defs(self):\n",
    "        return'''\n",
    "void gen_init__(void** max_depthp);\n",
    "'''\n",
    "\n",
    "    def fn_main_loop_frag(self):\n",
    "        return '''\n",
    "    for(int i=0; i < max_num; i++) {\n",
    "        gen_init__(max_depthp);\n",
    "        *out_regionp++ = '\\\\n';\n",
    "    }\n",
    "    *out_regionp = 0;'''\n",
    "    \n",
    "    def fn_main_def(self):\n",
    "        return self.fn_truncateio() + '''\n",
    "int main(int argc, char** argv) {\n",
    "    struct stat st;\n",
    "    long out_size;\n",
    "    char* out_region_sizep = 0;\n",
    "    char* out_region_initp;\n",
    "    int rand_fd, out_fd;\n",
    "    int seed, max_num;\n",
    "%(input_frag)s\n",
    "    max_depthp = stackp + max_depth;\n",
    "%(rand_frag)s\n",
    "%(out_frag)s\n",
    "    out_region_initp = out_regionp;\n",
    "    out_region_sizep = out_regionp + st.st_size;\n",
    "%(loop_frag)s\n",
    "    out_size = out_regionp - out_region_initp;\n",
    "    out_cursor = out_size;\n",
    "%(sync_frag)s\n",
    "    return 0;\n",
    "}''' % {'input_frag': self.fn_main_input_frag(),\n",
    "        'rand_frag': self.fn_main_rand_frag(),\n",
    "        'out_frag': self.fn_main_out_frag(),\n",
    "        'loop_frag': self.fn_main_loop_frag(),\n",
    "        'sync_frag': self.fn_main_sync_frag()\n",
    "       }\n",
    "\n",
    "\n",
    "    def gen_fuzz_src(self):\n",
    "        return '\\n'.join([self.fuzz_hdefs(),\n",
    "                          self.fuzz_var_defs(),\n",
    "                          self.fn_fuzz_decs(),\n",
    "                          self.string_pool_defs(),\n",
    "                          # self.fuzz_fn_defs(),\n",
    "                          self.fuzz_entry()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_src, fuzz_src = DTMMapFuzzer(c_grammar).fuzz_src()\n",
    "with open('testers/grammar_producer_dtmmap_main.c', 'w+') as f:\n",
    "    print(main_src, file=f)\n",
    "with open('testers/grammar_producer_dtmmap_fuzz.c', 'w+') as f:\n",
    "    print(fuzz_src, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l testers/grammar_producer_dtmmap_fuzz.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd testers\n",
    "!cc -g -Ofast -o grammar_producer_dtmmap grammar_producer_dtmmap_main.c grammar_producer_dtmmap_fuzz.c\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# II\n",
    "!./testers/grammar_producer_dtmmap 0 10 10 io.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat io.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# II\n",
    "class CTesterMMapDT(CTester):\n",
    "    def exec_program(self, seed, max_depth, t):\n",
    "        fn = self.ofile(max_depth, seed)\n",
    "        return f\"./testers/grammar_producer_dtmmap {seed} {self.max_num} {max_depth} {fn}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTesterMMapDT().run_test().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context threaded VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# II\n",
    "class CTMMapFuzzer(DTMMapFuzzer):\n",
    "    def fn_choice(self, val):\n",
    "        return '''\n",
    "    # [ random \n",
    "    # extract one byte from the random stream %%r14,\n",
    "    movq (%%r14), %%rdi\n",
    "    # advance the random cursor\n",
    "    inc %%r14                                     # rand_region++\n",
    "    movzbl %%dil, %%edi                           # X  --- (rdi:(edi:(di:(dil))))\n",
    "    # then multiply with the choices we have\n",
    "\n",
    "    xor %%rsi, %%rsi                              # avoid data dependencies\n",
    "    movb $%(val)s, %%sil                          # = %(val)s       \n",
    "    movzbl %%sil, %%edx\n",
    "    imull %%edi, %%edx                            # m = (short) x * (short) N)\n",
    "    sarl $8, %%edx                                # return (char)(m >> 8) ;\n",
    "    # random ]\n",
    "    # %%edx now contains the selected random value from %(val)d options''' % {'val':val}\n",
    "\n",
    "    def cheap_strings(self, k):\n",
    "        cheap_strings = self.pool_of_strings[k]\n",
    "        results = ['''\n",
    "    # --- cheap -- [''']\n",
    "        results.append('''\n",
    "%(choices)s\n",
    "''' % {'choices':self.fn_choice(len(cheap_strings)), 'len_choices': len(cheap_strings)})\n",
    "        # get the choices from vm, then call it, and return.\n",
    "        \n",
    "        results.append('''\n",
    "    # now we have the right print quad in %%edx. Load the right address and call it.\n",
    "    leaq _%(key)s_prints(%%rip), %%rcx\n",
    "    leaq (%%rcx, %%rdx, 8), %%rax\n",
    "    callq *(%%rax)\n",
    "    ret\n",
    "    ''' % {'key': self.k_to_s(k)})\n",
    "        results.append('''\n",
    "    # --- cheap -- ]''')\n",
    "        return '\\n'.join(results)\n",
    "    \n",
    "    def output_char(self, c):\n",
    "        if len(c) != 1:\n",
    "            assert c[0] == '\\\\'\n",
    "            c = c[-1]\n",
    "        return '''\n",
    "   movb $%(ichar)d, (%%r13)                     # '%(char)s'\n",
    "   inc %%r13                                    # out_region++   : increment a byte (r13++)\n",
    "   ''' % {'char':repr(c), 'ichar':ord(c)}\n",
    "\n",
    "    def gen_rule_src(self, rule, k, j):\n",
    "        # in each rule, there are a number of tokens.\n",
    "        # iter each token in turn, choose the right rule and call.\n",
    "        result = []\n",
    "        for token in rule:\n",
    "            if token not in self.grammar:\n",
    "                result.append(self.output_char(token))\n",
    "                continue\n",
    "            else:\n",
    "                # how many choices do we have?\n",
    "                rules = self.grammar[token]\n",
    "                result.append('''\n",
    "    # start the choice machine.\n",
    "    # length of rules = %(len_rules)d\n",
    "%(choices)s\n",
    "    # --- switch ---\n",
    "    ''' % {'choices': self.fn_choice(len(rules)), 'len_rules':len(rules)})\n",
    "                result.append('''\n",
    "    # now we have the right choice in %%edx. Load the right address and call it.\n",
    "    leaq _%(key)s_choices(%%rip), %%rcx\n",
    "    leaq (%%rcx, %%rdx, 8), %%rax\n",
    "    callq *(%%rax)\n",
    "    ''' % {'key': self.k_to_s(token)})\n",
    "        return '\\n'.join(result)\n",
    "\n",
    "    def gen_alt_src(self, k):\n",
    "        result = []\n",
    "        for ruleid, rule in enumerate(self.grammar[k]):\n",
    "            # produce a skeletal subroutine structure.\n",
    "            result.append('''\n",
    "gen_%(key)s_%(ruleid)s:\n",
    "    # check if the max depth is breached.\n",
    "    cmpq %%rsp, %%r8                             # returnp(rbp) <> max_depth(r8) ?\n",
    "    jle _%(key)s_%(ruleid)s_fi                       # returnp <= max_depth\n",
    "    \n",
    "%(return_cheap_string)s\n",
    "_%(key)s_%(ruleid)s_fi:\n",
    "''' % {'return_cheap_string': self.cheap_strings(k),\n",
    "       'key':self.k_to_s(k),\n",
    "       'ruleid':ruleid,\n",
    "       'last_label':self.last_label})\n",
    "            self.last_label += 1\n",
    "            result.append(self.gen_rule_src(rule, k, ruleid))\n",
    "            # we were called. So simply return.\n",
    "            result.append('''\n",
    "    ret\n",
    "            ''')\n",
    "        return '\\n'.join(result)\n",
    " \n",
    "    def fn_fuzz_decs(self):\n",
    "        result = ['''\n",
    "  .section  __DATA,__data\n",
    "\n",
    "# Virtual Machine OPS.\n",
    "        ''']\n",
    "        for k in self.grammar:\n",
    "            result.append('''\n",
    "    .globl  _%(key)s_choices\n",
    "    .p2align 4\n",
    "_%(key)s_choices:''' % {'key':self.k_to_s(k)})\n",
    "            for i, rule in enumerate(self.grammar[k]):\n",
    "                result.append('''\\\n",
    "    .quad gen_%s_%d''' % (self.k_to_s(k), i))\n",
    "                \n",
    "        for k in self.pool_of_strings:\n",
    "            result.append('''\n",
    "    .globl  _%(key)s_prints\n",
    "    .p2align 4\n",
    "_%(key)s_prints:''' % {'key':self.k_to_s(k)})\n",
    "            for string in self.pool_of_strings[k]:\n",
    "                result.append('''\\\n",
    "    .quad %s''' % (self.all_prints[string]))\n",
    "                \n",
    "                \n",
    "        result.append('''\n",
    "# End Virtual Machine OPS.''')\n",
    "        return '\\n'.join(result)\n",
    "\n",
    "    def gen_cheap(self, grammar):\n",
    "        all_strings = set()\n",
    "        for k in grammar:\n",
    "            all_strings |= set(self.pool_of_strings[k])\n",
    "        all_strings = list(all_strings)\n",
    "        all_strings.sort(key=lambda item: (-len(item), item))\n",
    "        all_prints_hash = {}\n",
    "        result = ['''\n",
    ".text\n",
    "        ''']\n",
    "        for i, s_ in enumerate(all_strings):\n",
    "            s = s_\n",
    "            result.append('''\\\n",
    "print_%(name)d: # \"%(value)s\"''' % {'name': i, 'value': repr(s)})\n",
    "            for j in s:\n",
    "                result.append('''\\\n",
    "    movb $%(ichar)s, (%%r13)            # '%(char)s'\n",
    "    inc %%r13''' % {'ichar':ord(j), 'char':repr(j)})\n",
    "            result.append('''\\\n",
    "    ret''')\n",
    "            all_prints_hash[s_] = 'print_%d' % i\n",
    "        return ('\\n'.join(result), all_prints_hash)\n",
    " \n",
    "    def fuzz_entry(self):\n",
    "        result = [\"\"\"\n",
    "#include \"ctmmap_vm_ops.s\"\n",
    ".macro pushaq\n",
    "    push %%rsp\n",
    "    push %%rbp\n",
    "    push %%r8\n",
    "    push %%r9\n",
    "    push %%r10\n",
    "    push %%r11\n",
    "    push %%r12\n",
    "    push %%r13\n",
    "    push %%r14\n",
    "    push %%r15\n",
    ".endm\n",
    "\n",
    "\n",
    ".macro popaq\n",
    "    pop %%r15\n",
    "    pop %%r14\n",
    "    pop %%r13\n",
    "    pop %%r12\n",
    "    pop %%r11\n",
    "    pop %%r10\n",
    "    pop %%r9\n",
    "    pop %%r8\n",
    "    pop %%rbp\n",
    "    pop %%rsp\n",
    ".endm\n",
    "\n",
    ".global %(os)sgen_init__\n",
    ".global return__init\n",
    ".text\n",
    "%(os)sgen_init__:\n",
    "    # 1 rdi = max_depth\n",
    "    # 2 rsi = returnp\n",
    "    # 3 rdx = &out_region\n",
    "    # 4 rcx = &rand_region\n",
    "    pushaq\n",
    "\n",
    "    leal 0(,%%rdi,8), %%eax\n",
    "    movq %%rsp, %%r8\n",
    "    subq %%rax, %%r8\n",
    "\n",
    "    movq %%rdx, %%r11                              # &out_region\n",
    "    movq %%rcx, %%r12                              # &rand_region\n",
    "    movq (%%r11),%%r13                             # out_region\n",
    "    movq (%%r12),%%r14                             # rand_region\n",
    "\n",
    "    # general regs\n",
    "    # rax, rcx, rdx, rbx, rsi,rdi\n",
    "    # rbp, r8-r15\n",
    "    \n",
    "    call gen_start_0\n",
    "    movq %%r13, (%%r11)                            # *(&out_region) <-\n",
    "    movq %%r14, (%%r12)                            # *(&rand_region) <-\n",
    "    popaq\n",
    "    movq  $0, %%rax\n",
    "    ret   \n",
    "\"\"\" % {'os': '_' if sys.platform == 'darwin' else ''}]\n",
    "        result.append(self.fuzz_fn_defs())\n",
    "        return ''.join(result)\n",
    "\n",
    "    def main_init_var_defs(self):\n",
    "        return'''\\\n",
    "void gen_init__(uint32_t max_depth, void** returnp, char** out_region, uint8_t** rand_region);\n",
    "'''\n",
    "\n",
    "    def fn_main_loop_frag(self):\n",
    "        return '''\\\n",
    "    for(int i=0; i < max_num; i++) {\n",
    "        gen_init__(max_depth32, stackp, &out_regionp, &rand_regionp);\n",
    "        *out_regionp++ = '\\\\n';\n",
    "    }\n",
    "    *out_regionp = 0;'''\n",
    "    \n",
    "    def fn_main_def(self):\n",
    "        return self.fn_truncateio() + '''\n",
    "int main(int argc, char** argv) {\n",
    "    struct stat st;\n",
    "    long out_size;\n",
    "    char* out_region_initp;\n",
    "    int out_fd;\n",
    "    uint32_t max_depth32;\n",
    "    int seed, max_num;\n",
    "%(input_frag)s\n",
    "    max_depth32 = max_depth;\n",
    "%(rand_frag)s\n",
    "%(out_frag)s\n",
    "    out_region_initp = out_regionp;\n",
    "%(loop_frag)s\n",
    "    out_size = out_regionp - out_region_initp;\n",
    "    out_cursor = out_size;\n",
    "%(sync_frag)s\n",
    "    return 0;\n",
    "}''' % {'input_frag': self.fn_main_input_frag(),\n",
    "        'rand_frag': self.fn_main_rand_frag(),\n",
    "        'out_frag': self.fn_main_out_frag(),\n",
    "        'loop_frag': self.fn_main_loop_frag(),\n",
    "        'sync_frag': self.fn_main_sync_frag()\n",
    "       }\n",
    "    \n",
    "    def fuzz_src(self, key='<start>'):\n",
    "        self.last_label = 0\n",
    "        self.cheap, self.all_prints = self.gen_cheap(self.grammar)\n",
    "        ext_strings = '\\n'.join([self.fn_fuzz_decs(), self.cheap])\n",
    "        return ext_strings, self.gen_main_src(), self.gen_fuzz_src()\n",
    "    \n",
    "    def gen_fuzz_src(self):\n",
    "        return '\\n'.join([self.fuzz_entry()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_ops, main_src, fuzz_src = CTMMapFuzzer(c_grammar).fuzz_src()\n",
    "with open('testers/grammar_producer_ctmmap_main.c', 'w+') as f:\n",
    "    print(main_src, file=f)\n",
    "with open('testers/grammar_producer_ctmmap_fuzz.s', 'w+') as f:\n",
    "    print(fuzz_src, file=f)\n",
    "with open('testers/ctmmap_vm_ops.s', 'w+') as f:\n",
    "    print(vm_ops, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nl -ba testers/grammar_producer_ctmmap_fuzz.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nl -ba testers/grammar_producer_ctmmap_main.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd testers\n",
    "!cc -g -Ofast -o grammar_producer_ctmmap grammar_producer_ctmmap_main.c grammar_producer_ctmmap_fuzz.s\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# II\n",
    "!./testers/grammar_producer_ctmmap 0 10 10 io.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat io.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# II\n",
    "class CTesterMMapCT(CTester):\n",
    "    def exec_program(self, seed, max_depth, t):\n",
    "        fn = self.ofile(max_depth, seed)\n",
    "        return f\"./testers/grammar_producer_ctmmap {seed} {self.max_num} {max_depth} {fn}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTesterMMapCT().run_test().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FWrite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Direct threaded VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# II\n",
    "class CFWriteDTFuzzer(CFWriteFuzzer):\n",
    "    def fn_out_def(self): return ''\n",
    "    def gen_rule_src(self, rule, k, j):\n",
    "        res = []\n",
    "        leaf = True\n",
    "        for i, token in enumerate(rule):\n",
    "            if token in self.grammar:\n",
    "                leaf = False\n",
    "                trules = self.grammar[token] # ordered by cost\n",
    "                len_min_choices = len(self.c_grammar[token])\n",
    "                assert len(trules) < 256\n",
    "                cheap_strings = self.pool_of_strings[token]\n",
    "                if len(cheap_strings) < 256: # we only have 255 random choices\n",
    "                    check_pool = '''\n",
    "        val = map(%(len_cheap_strings)s);\n",
    "        const char* str = pool_%(k)s[val];\n",
    "        const int str_l = pool_l_%(k)s[val];\n",
    "        for (int i = 0; i < str_l; i++) {\n",
    "            *out_regionp++ = str[i];\n",
    "        }\n",
    "        --returnp;\n",
    "        goto **returnp; \n",
    "            ''' % { 'len_cheap_strings': len(cheap_strings), 'k': self.k_to_s(token)}\n",
    "                else:\n",
    "                    check_pool = '''\n",
    "        val = map(%(len_min_choices)s);\n",
    "                ''' % {'len_min_choices':len_min_choices}\n",
    "                res.append('''\\\n",
    "    *returnp = &&return__%(i)d__%(j)d__%(k)s;\n",
    "    if (returnp > max_depthp) {\n",
    "        %(check_pool)s;\n",
    "    } else {\n",
    "        val = map(%(len_rules)s);\n",
    "    }\n",
    "    goto *gen_%(t)s[val];\n",
    "return__%(i)d__%(j)d__%(k)s:;\n",
    "            ''' % {'i':i, 'j':j, 'k':self.k_to_s(k),\n",
    "                   't':self.k_to_s(token), 'rnum':0, 'len_rules':len(trules), 'len_min_choices':len_min_choices, 'check_pool':check_pool})\n",
    "            else:\n",
    "                t = self.esc_char(token)\n",
    "                res.append('''\\\n",
    "    *out_regionp++ = '%s';''' % t)\n",
    "        return res, leaf\n",
    "    \n",
    "    def gen_alt_src_1rule(self, k):\n",
    "        rule = self.grammar[k][0]\n",
    "        ri = 0\n",
    "        src, leaf = self.gen_rule_src(rule, k, ri)\n",
    "        body = '\\n'.join(src)\n",
    "        result = []\n",
    "        if leaf:\n",
    "            return '''\n",
    "gen_%(name)s_0: {\n",
    "%(body)s\n",
    "    goto **returnp;\n",
    "}''' % {'name':self.k_to_s(k), 'body':body}\n",
    "        else:\n",
    "             return '''\n",
    "gen_%(name)s_0: {\n",
    "    ++returnp;\n",
    "    // single -- no switch\n",
    "%(body)s\n",
    "    --returnp;\n",
    "    goto **returnp;\n",
    "}''' % {'name':self.k_to_s(k), 'body':body}\n",
    "\n",
    "    def gen_alt_src(self, k):\n",
    "        rules = self.grammar[k]\n",
    "        ret = self.k_to_s(k)\n",
    "        result = []\n",
    "        if len(rules) == 1: return self.gen_alt_src_1rule(k)\n",
    "        for ri, rule in enumerate(rules):\n",
    "            src, leaf = self.gen_rule_src(rule, k, ri)\n",
    "            body = '\\n'.join(src)\n",
    "            if leaf:\n",
    "                result.append('''\n",
    "gen_%(name)s_%(rnum)d: {\n",
    "%(body)s\n",
    "    goto **returnp;\n",
    "}\n",
    "    ''' % {'name': self.k_to_s(k), 'rnum': ri, 'body':body})\n",
    "            else:\n",
    "                 result.append('''\n",
    "gen_%(name)s_%(rnum)d: {\n",
    "    ++returnp;\n",
    "%(body)s\n",
    "    --returnp;\n",
    "    goto **returnp;\n",
    "}\n",
    "    ''' % {'name': self.k_to_s(k), 'rnum': ri, 'body':body})\n",
    "        return '\\n'.join(result)\n",
    "\n",
    "    def fuzz_out_var_defs(self):\n",
    "        return '''\\\n",
    "extern char* out_regionp;'''\n",
    "    \n",
    "    def fuzz_rand_var_defs(self):\n",
    "        return '''\n",
    "uint8_t map(uint8_t to);'''\n",
    "    \n",
    "    def fuzz_stack_var_defs(self):\n",
    "        return '''\n",
    "extern void* stackp[];\n",
    "'''\n",
    "\n",
    "    def fuzz_entry(self):\n",
    "        result = ['''\n",
    "void gen_init__(void** max_depthp) {\n",
    "    uint8_t val;\n",
    "    void** returnp = stackp;\n",
    "    *returnp =  &&return__init;\n",
    "''']\n",
    "        for k in self.grammar:\n",
    "            l = []\n",
    "            for ri,rule in enumerate(self.grammar[k]):\n",
    "                l.append('&&gen_%(k)s_%(ri)d' % {'k':self.k_to_s(k), 'ri':ri})\n",
    "            s = '''\n",
    "    void** gen_%(k)s[] = {\n",
    "%(body)s\n",
    "    };''' % {'k': self.k_to_s(k), 'body': ',\\n'.join(l)}\n",
    "            result.append(s)\n",
    "        result.append('''\n",
    "    goto gen_start_0;''')\n",
    "        result.append(self.fuzz_fn_defs())\n",
    "        result.append(\"\"\"\n",
    "return__init:\n",
    "    *out_regionp++ = '\\\\n';\n",
    "    return;\n",
    "return_abort:\n",
    "    exit(10); \n",
    "}\"\"\")\n",
    "        return '\\n'.join(result)\n",
    "\n",
    "    \n",
    "    def main_stack_var_defs(self):\n",
    "        return'''\n",
    "int max_depth;\n",
    "void** max_depthp;\n",
    "void* stackp[INT_MAX];\n",
    "'''\n",
    "    def main_init_var_defs(self):\n",
    "        return'''\n",
    "void gen_init__(void** max_depthp);\n",
    "'''\n",
    "\n",
    "    def fn_main_loop_frag(self):\n",
    "        return '''\n",
    "    fs = fdopen(out_fd, \"w\");\n",
    "    for(int i=0; i < max_num; i++) {\n",
    "        out_regionp = out_region_initp;\n",
    "        gen_init__(max_depthp);\n",
    "        out_cursor = out_regionp - out_region_initp;\n",
    "        fwrite(out_region_initp, sizeof(char), out_cursor, fs);\n",
    "    }\n",
    "    '''\n",
    "    \n",
    "    def fn_main_def(self):\n",
    "        return self.fn_truncateio() + '''\n",
    "int main(int argc, char** argv) {\n",
    "    struct stat st;\n",
    "    long out_size;\n",
    "    char* out_region_sizep = 0;\n",
    "    int out_fd;\n",
    "    int seed, max_num;\n",
    "%(input_frag)s\n",
    "    max_depthp = stackp + max_depth;\n",
    "%(rand_frag)s\n",
    "%(out_frag)s\n",
    "%(loop_frag)s\n",
    "%(sync_frag)s\n",
    "    return 0;\n",
    "}''' % {'input_frag': self.fn_main_input_frag(),\n",
    "        'rand_frag': self.fn_main_rand_frag(),\n",
    "        'out_frag': self.fn_main_out_frag(),\n",
    "        'loop_frag': self.fn_main_loop_frag(),\n",
    "        'sync_frag': self.fn_main_sync_frag()\n",
    "       }\n",
    "\n",
    "\n",
    "    def gen_fuzz_src(self):\n",
    "        return '\\n'.join([self.fuzz_hdefs(),\n",
    "                          self.fuzz_var_defs(),\n",
    "                          self.fn_fuzz_decs(),\n",
    "                          self.string_pool_defs(),\n",
    "                          # self.fuzz_fn_defs(),\n",
    "                          self.fuzz_entry()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_src, fuzz_src = CFWriteDTFuzzer(c_grammar).fuzz_src()\n",
    "with open('testers/grammar_producer_fwritedt_main.c', 'w+') as f:\n",
    "    print(main_src, file=f)\n",
    "with open('testers/grammar_producer_fwritedt_fuzz.c', 'w+') as f:\n",
    "    print(fuzz_src, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l testers/grammar_producer_fwritedt_fuzz.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat testers/grammar_producer_fwritedt_main.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd testers\n",
    "!cc -g -Ofast -o grammar_producer_fwritedt grammar_producer_fwritedt_main.c grammar_producer_fwritedt_fuzz.c\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# II\n",
    "class CTesterFWriteDT(CTester):\n",
    "    def exec_program(self, seed, max_depth, t):\n",
    "        fn = self.ofile(max_depth, seed)\n",
    "        return f\"./testers/grammar_producer_fwritedt {seed} {self.max_num} {max_depth} {fn}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# II\n",
    "!./testers/grammar_producer_fwritedt 0 10 10 io.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat io.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTesterFWriteDT().run_test().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context threaded VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# II\n",
    "class CFWriteCTFuzzer(CFWriteDTFuzzer):\n",
    "    \n",
    "    def fn_choice(self, val):\n",
    "        return '''\n",
    "    # [ random \n",
    "    # extract one byte from the random stream %%r14,\n",
    "    movq (%%r14), %%rdi\n",
    "    # advance the random cursor\n",
    "    inc %%r14                                     # rand_region++\n",
    "    movzbl %%dil, %%edi                           # X  --- (rdi:(edi:(di:(dil))))\n",
    "    # then multiply with the choices we have\n",
    "\n",
    "    xor %%rsi, %%rsi                              # avoid data dependencies\n",
    "    movb $%(val)s, %%sil                          # = %(val)s       \n",
    "    movzbl %%sil, %%edx\n",
    "    imull %%edi, %%edx                            # m = (short) x * (short) N)\n",
    "    sarl $8, %%edx                                # return (char)(m >> 8) ;\n",
    "    # random ]\n",
    "    # %%edx now contains the selected random value from %(val)d options''' % {'val':val}\n",
    "\n",
    "    def cheap_strings(self, k):\n",
    "        cheap_strings = self.pool_of_strings[k]\n",
    "        results = ['''\n",
    "    # --- cheap -- [''']\n",
    "        results.append('''\n",
    "%(choices)s\n",
    "''' % {'choices':self.fn_choice(len(cheap_strings)), 'len_choices': len(cheap_strings)})\n",
    "        # get the choices from vm, then call it, and return.\n",
    "        \n",
    "        results.append('''\n",
    "    # now we have the right print quad in %%edx. Load the right address and call it.\n",
    "    leaq _%(key)s_prints(%%rip), %%rcx\n",
    "    leaq (%%rcx, %%rdx, 8), %%rax\n",
    "    callq *(%%rax)\n",
    "    ret\n",
    "    ''' % {'key': self.k_to_s(k)})\n",
    "        results.append('''\n",
    "    # --- cheap -- ]''')\n",
    "        return '\\n'.join(results)\n",
    "    \n",
    "    def output_char(self, c):\n",
    "        if len(c) != 1:\n",
    "            assert c[0] == '\\\\'\n",
    "            c = c[-1]\n",
    "        return '''\n",
    "   movb $%(ichar)d, (%%r13)                     # '%(char)s'\n",
    "   inc %%r13                                    # out_region++   : increment a byte (r13++)\n",
    "   ''' % {'char':self.esc(c), 'ichar':ord(c)}\n",
    "\n",
    "    def gen_rule_src(self, rule, k, j):\n",
    "        # in each rule, there are a number of tokens.\n",
    "        # iter each token in turn, choose the right rule and call.\n",
    "        result = []\n",
    "        for token in rule:\n",
    "            if token not in self.grammar:\n",
    "                result.append(self.output_char(token))\n",
    "                continue\n",
    "            else:\n",
    "                # how many choices do we have?\n",
    "                rules = self.grammar[token]\n",
    "                result.append('''\n",
    "    # start the choice machine.\n",
    "    # length of rules = %(len_rules)d\n",
    "%(choices)s\n",
    "    # --- switch ---\n",
    "    ''' % {'choices': self.fn_choice(len(rules)), 'len_rules':len(rules)})\n",
    "                result.append('''\n",
    "    # now we have the right choice in %%edx. Load the right address and call it.\n",
    "    leaq _%(key)s_choices(%%rip), %%rcx\n",
    "    leaq (%%rcx, %%rdx, 8), %%rax\n",
    "    callq *(%%rax)\n",
    "    ''' % {'key': self.k_to_s(token)})\n",
    "        return '\\n'.join(result)\n",
    "\n",
    "    def gen_alt_src(self, k):\n",
    "        result = []\n",
    "        for ruleid, rule in enumerate(self.grammar[k]):\n",
    "            # produce a skeletal subroutine structure.\n",
    "            result.append('''\n",
    "gen_%(key)s_%(ruleid)s:\n",
    "    # check if the max depth is breached.\n",
    "    cmpq %%rsp, %%r8                             # returnp(rbp) <> max_depth(r8) ?\n",
    "    jle _%(key)s_%(ruleid)s_fi                       # returnp <= max_depth\n",
    "    \n",
    "%(return_cheap_string)s\n",
    "_%(key)s_%(ruleid)s_fi:\n",
    "''' % {'return_cheap_string': self.cheap_strings(k),\n",
    "       'key':self.k_to_s(k),\n",
    "       'ruleid':ruleid,\n",
    "       'last_label':self.last_label})\n",
    "            self.last_label += 1\n",
    "            result.append(self.gen_rule_src(rule, k, ruleid))\n",
    "            # we were called. So simply return.\n",
    "            result.append('''\n",
    "    ret\n",
    "            ''')\n",
    "        return '\\n'.join(result)\n",
    " \n",
    "    def fn_fuzz_decs(self):\n",
    "        result = ['''\n",
    "  .section  __DATA,__data\n",
    "\n",
    "# Virtual Machine OPS.\n",
    "        ''']\n",
    "        for k in self.grammar:\n",
    "            result.append('''\n",
    "    .globl  _%(key)s_choices\n",
    "    .p2align 4\n",
    "_%(key)s_choices:''' % {'key':self.k_to_s(k)})\n",
    "            for i, rule in enumerate(self.grammar[k]):\n",
    "                result.append('''\\\n",
    "    .quad gen_%s_%d''' % (self.k_to_s(k), i))\n",
    "                \n",
    "        for k in self.pool_of_strings:\n",
    "            result.append('''\n",
    "    .globl  _%(key)s_prints\n",
    "    .p2align 4\n",
    "_%(key)s_prints:''' % {'key':self.k_to_s(k)})\n",
    "            for string in self.pool_of_strings[k]:\n",
    "                result.append('''\\\n",
    "    .quad %s''' % (self.all_prints[string]))\n",
    "                \n",
    "                \n",
    "        result.append('''\n",
    "# End Virtual Machine OPS.''')\n",
    "        return '\\n'.join(result)\n",
    "\n",
    "    def gen_cheap(self, grammar):\n",
    "        all_strings = set()\n",
    "        for k in grammar:\n",
    "            all_strings |= set(self.pool_of_strings[k])\n",
    "        all_strings = list(all_strings)\n",
    "        all_strings.sort(key=lambda item: (-len(item), item))\n",
    "        all_prints_hash = {}\n",
    "        result = ['''\n",
    ".text\n",
    "        ''']\n",
    "        for i, s_ in enumerate(all_strings):\n",
    "            s = s_\n",
    "            result.append('''\\\n",
    "print_%(name)d: # \"%(value)s\"''' % {'name': i, 'value': self.esc(s)})\n",
    "            for j in s:\n",
    "                result.append('''\\\n",
    "    movb $%(ichar)s, (%%r13)            # '%(char)s'\n",
    "    inc %%r13''' % {'ichar':ord(j), 'char':self.esc(j)})\n",
    "            result.append('''\\\n",
    "    ret''')\n",
    "            all_prints_hash[s_] = 'print_%d' % i\n",
    "        return ('\\n'.join(result), all_prints_hash)\n",
    " \n",
    "    def fuzz_entry(self):\n",
    "        result = [\"\"\"\n",
    "#include \"ctfwrite_vm_ops.s\"\n",
    ".macro pushaq\n",
    "    push %%rsp\n",
    "    push %%rbp\n",
    "    push %%r8\n",
    "    push %%r9\n",
    "    push %%r10\n",
    "    push %%r11\n",
    "    push %%r12\n",
    "    push %%r13\n",
    "    push %%r14\n",
    "    push %%r15\n",
    ".endm\n",
    "\n",
    "\n",
    ".macro popaq\n",
    "    pop %%r15\n",
    "    pop %%r14\n",
    "    pop %%r13\n",
    "    pop %%r12\n",
    "    pop %%r11\n",
    "    pop %%r10\n",
    "    pop %%r9\n",
    "    pop %%r8\n",
    "    pop %%rbp\n",
    "    pop %%rsp\n",
    ".endm\n",
    "\n",
    ".global %(os)sgen_init__\n",
    ".global return__init\n",
    ".text\n",
    "%(os)sgen_init__:\n",
    "    # 1 rdi = max_depth\n",
    "    # 2 rsi = returnp\n",
    "    # 3 rdx = &out_region\n",
    "    # 4 rcx = &rand_region\n",
    "    pushaq\n",
    "\n",
    "    leal 0(,%%rdi,8), %%eax\n",
    "    movq %%rsp, %%r8\n",
    "    subq %%rax, %%r8\n",
    "\n",
    "    movq %%rdx, %%r11                              # &out_region\n",
    "    movq %%rcx, %%r12                              # &rand_region\n",
    "    movq (%%r11),%%r13                             # out_region\n",
    "    movq (%%r12),%%r14                             # rand_region\n",
    "\n",
    "    # general regs\n",
    "    # rax, rcx, rdx, rbx, rsi,rdi\n",
    "    # rbp, r8-r15\n",
    "    \n",
    "    call gen_start_0\n",
    "    movq %%r13, (%%r11)                            # *(&out_region) <-\n",
    "    movq %%r14, (%%r12)                            # *(&rand_region) <-\n",
    "    popaq\n",
    "    movq  $0, %%rax\n",
    "    ret   \n",
    "\"\"\" % {'os': '_' if sys.platform == 'darwin' else ''}]\n",
    "        result.append(self.fuzz_fn_defs())\n",
    "        return ''.join(result)\n",
    "\n",
    "    def main_init_var_defs(self):\n",
    "        return'''\n",
    "void gen_init__(uint32_t max_depth, void** returnp, char** out_region, uint8_t** rand_region);\n",
    "'''\n",
    "\n",
    "    def fn_main_loop_frag(self):\n",
    "        return '''\n",
    "    fs = fdopen(out_fd, \"w\");\n",
    "    for(int i=0; i < max_num; i++) {\n",
    "        out_regionp = out_region_initp;\n",
    "        gen_init__(max_depth32, stackp, &out_regionp, &rand_regionp);\n",
    "        *out_regionp++ = '\\\\n';\n",
    "        out_cursor = out_regionp - out_region_initp;\n",
    "        fwrite(out_region_initp, sizeof(char), out_cursor, fs);\n",
    "    }\n",
    "    '''\n",
    "    \n",
    "    def fn_main_def(self):\n",
    "        return self.fn_truncateio() + '''\n",
    "int main(int argc, char** argv) {\n",
    "    struct stat st;\n",
    "    long out_size;\n",
    "    int out_fd;\n",
    "    uint32_t max_depth32;\n",
    "    int seed, max_num;\n",
    "%(input_frag)s\n",
    "    max_depth32 = max_depth;\n",
    "%(rand_frag)s\n",
    "%(out_frag)s\n",
    "%(loop_frag)s\n",
    "%(sync_frag)s\n",
    "    return 0;\n",
    "}''' % {'input_frag': self.fn_main_input_frag(),\n",
    "        'rand_frag': self.fn_main_rand_frag(),\n",
    "        'out_frag': self.fn_main_out_frag(),\n",
    "        'loop_frag': self.fn_main_loop_frag(),\n",
    "        'sync_frag': self.fn_main_sync_frag()\n",
    "       }\n",
    "    \n",
    "    def fuzz_src(self, key='<start>'):\n",
    "        self.last_label = 0\n",
    "        self.cheap, self.all_prints = self.gen_cheap(self.grammar)\n",
    "        ext_strings = '\\n'.join([self.fn_fuzz_decs(), self.cheap])\n",
    "        return ext_strings, self.gen_main_src(), self.gen_fuzz_src()\n",
    "    \n",
    "    def gen_fuzz_src(self):\n",
    "        return '\\n'.join([self.fuzz_entry()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_ops, main_src, fuzz_src = CFWriteCTFuzzer(c_grammar).fuzz_src()\n",
    "with open('testers/grammar_producer_ctfwrite_main.c', 'w+') as f:\n",
    "    print(main_src, file=f)\n",
    "with open('testers/grammar_producer_ctfwrite_fuzz.s', 'w+') as f:\n",
    "    print(fuzz_src, file=f)\n",
    "with open('testers/ctfwrite_vm_ops.s', 'w+') as f:\n",
    "    print(vm_ops, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nl -ba testers/ctfwrite_vm_ops.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l testers/grammar_producer_ctfwrite_fuzz.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat testers/grammar_producer_ctfwrite_main.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd testers\n",
    "!cc -g -Ofast -o grammar_producer_ctfwrite grammar_producer_ctfwrite_main.c grammar_producer_ctfwrite_fuzz.s\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# II\n",
    "!./testers/grammar_producer_ctfwrite 0 10 10 io.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat io.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTesterFWriteCT(CTester):\n",
    "    def exec_program(self, seed, max_depth, t):\n",
    "        fn = self.ofile(max_depth, seed)\n",
    "        return f\"./testers/grammar_producer_ctfwrite {seed} {self.max_num} {max_depth} {fn}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTesterFWriteCT().run_test().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Direct threaded VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTNoWriteFuzzer(CNoWriteFuzzer):\n",
    "    def fn_out_def(self): return ''\n",
    "    def fn_main_out_frag(self): return ''\n",
    "    def gen_rule_src(self, rule, k, j):\n",
    "        res = []\n",
    "        leaf = True\n",
    "        for i, token in enumerate(rule):\n",
    "            if token in self.grammar:\n",
    "                leaf = False\n",
    "                trules = self.grammar[token] # ordered by cost\n",
    "                len_min_choices = len(self.c_grammar[token])\n",
    "                assert len(trules) < 256\n",
    "                cheap_strings = self.pool_of_strings[token]\n",
    "                if len(cheap_strings) < 256: # we only have 255 random choices\n",
    "                    check_pool = '''\n",
    "        val = map(%(len_cheap_strings)s);\n",
    "        const char* str = pool_%(k)s[val];\n",
    "        const int str_l = pool_l_%(k)s[val];\n",
    "        for (int i = 0; i < str_l; i++) {\n",
    "            *out_regionp++ = str[i];\n",
    "        }\n",
    "        --returnp;\n",
    "        goto **returnp; \n",
    "            ''' % { 'len_cheap_strings': len(cheap_strings), 'k': self.k_to_s(token)}\n",
    "                else:\n",
    "                    check_pool = '''\n",
    "        val = map(%(len_min_choices)s);\n",
    "                ''' % {'len_min_choices':len_min_choices}\n",
    "                res.append('''\\\n",
    "    *returnp = &&return__%(i)d__%(j)d__%(k)s;\n",
    "    if (returnp > max_depthp) {\n",
    "        %(check_pool)s;\n",
    "    } else {\n",
    "        val = map(%(len_rules)s);\n",
    "    }\n",
    "    goto *gen_%(t)s[val];\n",
    "return__%(i)d__%(j)d__%(k)s:;\n",
    "            ''' % {'i':i, 'j':j, 'k':self.k_to_s(k),\n",
    "                   't':self.k_to_s(token), 'rnum':0, 'len_rules':len(trules), 'len_min_choices':len_min_choices, 'check_pool':check_pool})\n",
    "            else:\n",
    "                t = self.esc_char(token)\n",
    "                res.append('''\\\n",
    "    *out_regionp++ = '%s';''' % t)\n",
    "        return res, leaf\n",
    "    \n",
    "    def gen_alt_src_1rule(self, k):\n",
    "        rule = self.grammar[k][0]\n",
    "        ri = 0\n",
    "        src, leaf = self.gen_rule_src(rule, k, ri)\n",
    "        body = '\\n'.join(src)\n",
    "        result = []\n",
    "        if leaf:\n",
    "            return '''\n",
    "gen_%(name)s_0: {\n",
    "%(body)s\n",
    "    goto **returnp;\n",
    "}''' % {'name':self.k_to_s(k), 'body':body}\n",
    "        else:\n",
    "             return '''\n",
    "gen_%(name)s_0: {\n",
    "    ++returnp;\n",
    "    // single -- no switch\n",
    "%(body)s\n",
    "    --returnp;\n",
    "    goto **returnp;\n",
    "}''' % {'name':self.k_to_s(k), 'body':body}\n",
    "\n",
    "    def gen_alt_src(self, k):\n",
    "        rules = self.grammar[k]\n",
    "        ret = self.k_to_s(k)\n",
    "        result = []\n",
    "        if len(rules) == 1: return self.gen_alt_src_1rule(k)\n",
    "        for ri, rule in enumerate(rules):\n",
    "            src, leaf = self.gen_rule_src(rule, k, ri)\n",
    "            body = '\\n'.join(src)\n",
    "            if leaf:\n",
    "                result.append('''\n",
    "gen_%(name)s_%(rnum)d: {\n",
    "%(body)s\n",
    "    goto **returnp;\n",
    "}\n",
    "    ''' % {'name': self.k_to_s(k), 'rnum': ri, 'body':body})\n",
    "            else:\n",
    "                 result.append('''\n",
    "gen_%(name)s_%(rnum)d: {\n",
    "    ++returnp;\n",
    "%(body)s\n",
    "    --returnp;\n",
    "    goto **returnp;\n",
    "}\n",
    "    ''' % {'name': self.k_to_s(k), 'rnum': ri, 'body':body})\n",
    "        return '\\n'.join(result)\n",
    "\n",
    "    def fuzz_out_var_defs(self):\n",
    "        return '''\\\n",
    "extern char* out_regionp;'''\n",
    "    \n",
    "    def fuzz_rand_var_defs(self):\n",
    "        return '''\n",
    "uint8_t map(uint8_t to);'''\n",
    "    \n",
    "    def fuzz_stack_var_defs(self):\n",
    "        return '''\n",
    "extern void* stackp[];\n",
    "'''\n",
    "\n",
    "    def fuzz_entry(self):\n",
    "        result = ['''\n",
    "void gen_init__(void** max_depthp) {\n",
    "    uint8_t val;\n",
    "    void** returnp = stackp;\n",
    "    *returnp =  &&return__init;\n",
    "''']\n",
    "        for k in self.grammar:\n",
    "            l = []\n",
    "            for ri,rule in enumerate(self.grammar[k]):\n",
    "                l.append('&&gen_%(k)s_%(ri)d' % {'k':self.k_to_s(k), 'ri':ri})\n",
    "            s = '''\n",
    "    void** gen_%(k)s[] = {\n",
    "%(body)s\n",
    "    };''' % {'k': self.k_to_s(k), 'body': ',\\n'.join(l)}\n",
    "            result.append(s)\n",
    "        result.append('''\n",
    "    goto gen_start_0;''')\n",
    "        result.append(self.fuzz_fn_defs())\n",
    "        result.append(\"\"\"\n",
    "return__init:\n",
    "    *out_regionp++ = '\\\\n';\n",
    "    return;\n",
    "return_abort:\n",
    "    exit(10); \n",
    "}\"\"\")\n",
    "        return '\\n'.join(result)\n",
    "    \n",
    "    def main_stack_var_defs(self):\n",
    "        return'''\n",
    "int max_depth;\n",
    "void** max_depthp;\n",
    "void* stackp[INT_MAX];\n",
    "'''\n",
    "    def main_init_var_defs(self):\n",
    "        return'''\n",
    "void gen_init__(void** max_depthp);\n",
    "'''\n",
    "\n",
    "    def fn_main_loop_frag(self):\n",
    "        return '''\n",
    "    uint64_t out_size = 0;\n",
    "    for(int i=0; i < max_num; i++) {\n",
    "        out_regionp = out_region_initp;\n",
    "        gen_init__(max_depthp);\n",
    "        out_cursor = out_regionp - out_region_initp;\n",
    "        out_size += out_cursor;\n",
    "    }\n",
    "    printf(\"%lld\\\\n\", out_size);\n",
    "    '''\n",
    "    \n",
    "    def fn_main_def(self):\n",
    "        return self.fn_truncateio() + '''\n",
    "int main(int argc, char** argv) {\n",
    "    struct stat st;\n",
    "    char* out_region_sizep = 0;\n",
    "    int out_fd;\n",
    "    int seed, max_num;\n",
    "%(input_frag)s\n",
    "    max_depthp = stackp + max_depth;\n",
    "%(rand_frag)s\n",
    "%(out_frag)s\n",
    "%(loop_frag)s\n",
    "%(sync_frag)s\n",
    "    return 0;\n",
    "}''' % {'input_frag': self.fn_main_input_frag(),\n",
    "        'rand_frag': self.fn_main_rand_frag(),\n",
    "        'out_frag': self.fn_main_out_frag(),\n",
    "        'loop_frag': self.fn_main_loop_frag(),\n",
    "        'sync_frag': self.fn_main_sync_frag()\n",
    "       }\n",
    "\n",
    "\n",
    "    def gen_fuzz_src(self):\n",
    "        return '\\n'.join([self.fuzz_hdefs(),\n",
    "                          self.fuzz_var_defs(),\n",
    "                          self.fn_fuzz_decs(),\n",
    "                          self.string_pool_defs(),\n",
    "                          # self.fuzz_fn_defs(),\n",
    "                          self.fuzz_entry()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_src, fuzz_src = DTNoWriteFuzzer(c_grammar).fuzz_src()\n",
    "with open('testers/grammar_producer_dtnowrite_main.c', 'w+') as f:\n",
    "    print(main_src, file=f)\n",
    "with open('testers/grammar_producer_dtnowrite_fuzz.c', 'w+') as f:\n",
    "    print(fuzz_src, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat testers/grammar_producer_dtnowrite_main.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd testers\n",
    "!cc -g -Ofast -o grammar_producer_dtnowrite grammar_producer_dtnowrite_main.c grammar_producer_dtnowrite_fuzz.c\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./testers/grammar_producer_dtnowrite  0 10 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTesterNoWriteDT(CTester):\n",
    "    def exec_program(self, seed, max_depth, t):\n",
    "        fn = self.ofile(max_depth, seed)\n",
    "        return f\"./testers/grammar_producer_dtnowrite {seed} {self.max_num} {max_depth} > {fn}\"\n",
    "  \n",
    "    def post_time(self):\n",
    "        super().post_time()\n",
    "        with open(self.file) as f:\n",
    "            self.size = int(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTesterNoWriteDT().run_test().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context threaded VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTNoWriteFuzzer(DTNoWriteFuzzer):\n",
    "    def fn_main_out_frag(self): return ''\n",
    "    def fn_main_sync_frag(self): return ''\n",
    "    \n",
    "    def fn_choice(self, val):\n",
    "        return '''\n",
    "    # [ random \n",
    "    # extract one byte from the random stream %%r14,\n",
    "    movq (%%r14), %%rdi\n",
    "    # advance the random cursor\n",
    "    inc %%r14                                     # rand_region++\n",
    "    movzbl %%dil, %%edi                           # X  --- (rdi:(edi:(di:(dil))))\n",
    "    # then multiply with the choices we have\n",
    "\n",
    "    xor %%rsi, %%rsi                              # avoid data dependencies\n",
    "    movb $%(val)s, %%sil                          # = %(val)s       \n",
    "    movzbl %%sil, %%edx\n",
    "    imull %%edi, %%edx                            # m = (short) x * (short) N)\n",
    "    sarl $8, %%edx                                # return (char)(m >> 8) ;\n",
    "    # random ]\n",
    "    # %%edx now contains the selected random value from %(val)d options''' % {'val':val}\n",
    "\n",
    "    def cheap_strings(self, k):\n",
    "        cheap_strings = self.pool_of_strings[k]\n",
    "        results = ['''\n",
    "    # --- cheap -- [''']\n",
    "        results.append('''\n",
    "%(choices)s\n",
    "''' % {'choices':self.fn_choice(len(cheap_strings)), 'len_choices': len(cheap_strings)})\n",
    "        # get the choices from vm, then call it, and return.\n",
    "        \n",
    "        results.append('''\n",
    "    # now we have the right print quad in %%edx. Load the right address and call it.\n",
    "    leaq _%(key)s_prints(%%rip), %%rcx\n",
    "    leaq (%%rcx, %%rdx, 8), %%rax\n",
    "    callq *(%%rax)\n",
    "    ret\n",
    "    ''' % {'key': self.k_to_s(k)})\n",
    "        results.append('''\n",
    "    # --- cheap -- ]''')\n",
    "        return '\\n'.join(results)\n",
    "    \n",
    "    def output_char(self, c):\n",
    "        if len(c) != 1:\n",
    "            assert c[0] == '\\\\'\n",
    "            c = c[-1]\n",
    "        return '''\n",
    "   movb $%(ichar)d, (%%r13)                     # '%(char)s'\n",
    "   inc %%r13                                    # out_region++   : increment a byte (r13++)\n",
    "   ''' % {'char':self.esc(c), 'ichar':ord(c)}\n",
    "\n",
    "    def gen_rule_src(self, rule, k, j):\n",
    "        # in each rule, there are a number of tokens.\n",
    "        # iter each token in turn, choose the right rule and call.\n",
    "        result = []\n",
    "        for token in rule:\n",
    "            if token not in self.grammar:\n",
    "                result.append(self.output_char(token))\n",
    "                continue\n",
    "            else:\n",
    "                # how many choices do we have?\n",
    "                rules = self.grammar[token]\n",
    "                result.append('''\n",
    "    # start the choice machine.\n",
    "    # length of rules = %(len_rules)d\n",
    "%(choices)s\n",
    "    # --- switch ---\n",
    "    ''' % {'choices': self.fn_choice(len(rules)), 'len_rules':len(rules)})\n",
    "                result.append('''\n",
    "    # now we have the right choice in %%edx. Load the right address and call it.\n",
    "    leaq _%(key)s_choices(%%rip), %%rcx\n",
    "    leaq (%%rcx, %%rdx, 8), %%rax\n",
    "    callq *(%%rax)\n",
    "    ''' % {'key': self.k_to_s(token)})\n",
    "        return '\\n'.join(result)\n",
    "\n",
    "    def gen_alt_src(self, k):\n",
    "        result = []\n",
    "        for ruleid, rule in enumerate(self.grammar[k]):\n",
    "            # produce a skeletal subroutine structure.\n",
    "            result.append('''\n",
    "gen_%(key)s_%(ruleid)s:\n",
    "    # check if the max depth is breached.\n",
    "    cmpq %%rsp, %%r8                             # returnp(rbp) <> max_depth(r8) ?\n",
    "    jle _%(key)s_%(ruleid)s_fi                       # returnp <= max_depth\n",
    "    \n",
    "%(return_cheap_string)s\n",
    "_%(key)s_%(ruleid)s_fi:\n",
    "''' % {'return_cheap_string': self.cheap_strings(k),\n",
    "       'key':self.k_to_s(k),\n",
    "       'ruleid':ruleid,\n",
    "       'last_label':self.last_label})\n",
    "            self.last_label += 1\n",
    "            result.append(self.gen_rule_src(rule, k, ruleid))\n",
    "            # we were called. So simply return.\n",
    "            result.append('''\n",
    "    ret\n",
    "            ''')\n",
    "        return '\\n'.join(result)\n",
    " \n",
    "    def fn_fuzz_decs(self):\n",
    "        result = ['''\n",
    "  .section  __DATA,__data\n",
    "\n",
    "# Virtual Machine OPS.\n",
    "        ''']\n",
    "        for k in self.grammar:\n",
    "            result.append('''\n",
    "    .globl  _%(key)s_choices\n",
    "    .p2align 4\n",
    "_%(key)s_choices:''' % {'key':self.k_to_s(k)})\n",
    "            for i, rule in enumerate(self.grammar[k]):\n",
    "                result.append('''\\\n",
    "    .quad gen_%s_%d''' % (self.k_to_s(k), i))\n",
    "                \n",
    "        for k in self.pool_of_strings:\n",
    "            result.append('''\n",
    "    .globl  _%(key)s_prints\n",
    "    .p2align 4\n",
    "_%(key)s_prints:''' % {'key':self.k_to_s(k)})\n",
    "            for string in self.pool_of_strings[k]:\n",
    "                result.append('''\\\n",
    "    .quad %s''' % (self.all_prints[string]))\n",
    "                \n",
    "                \n",
    "        result.append('''\n",
    "# End Virtual Machine OPS.''')\n",
    "        return '\\n'.join(result)\n",
    "\n",
    "    def gen_cheap(self, grammar):\n",
    "        all_strings = set()\n",
    "        for k in grammar:\n",
    "            all_strings |= set(self.pool_of_strings[k])\n",
    "        all_strings = list(all_strings)\n",
    "        all_strings.sort(key=lambda item: (-len(item), item))\n",
    "        all_prints_hash = {}\n",
    "        result = ['''\n",
    ".text\n",
    "        ''']\n",
    "        for i, s_ in enumerate(all_strings):\n",
    "            s = s_\n",
    "            result.append('''\\\n",
    "print_%(name)d: # \"%(value)s\"''' % {'name': i, 'value': self.esc(s)})\n",
    "            for j in s:\n",
    "                result.append('''\\\n",
    "    movb $%(ichar)s, (%%r13)            # '%(char)s'\n",
    "    inc %%r13''' % {'ichar':ord(j), 'char':self.esc(j)})\n",
    "            result.append('''\\\n",
    "    ret''')\n",
    "            all_prints_hash[s_] = 'print_%d' % i\n",
    "        return ('\\n'.join(result), all_prints_hash)\n",
    " \n",
    "    def fuzz_entry(self):\n",
    "        result = [\"\"\"\n",
    "#include \"ctnowrite_vm_ops.s\"\n",
    ".macro pushaq\n",
    "    push %%rsp\n",
    "    push %%rbp\n",
    "    push %%r8\n",
    "    push %%r9\n",
    "    push %%r10\n",
    "    push %%r11\n",
    "    push %%r12\n",
    "    push %%r13\n",
    "    push %%r14\n",
    "    push %%r15\n",
    ".endm\n",
    "\n",
    "\n",
    ".macro popaq\n",
    "    pop %%r15\n",
    "    pop %%r14\n",
    "    pop %%r13\n",
    "    pop %%r12\n",
    "    pop %%r11\n",
    "    pop %%r10\n",
    "    pop %%r9\n",
    "    pop %%r8\n",
    "    pop %%rbp\n",
    "    pop %%rsp\n",
    ".endm\n",
    "\n",
    ".global %(os)sgen_init__\n",
    ".global return__init\n",
    ".text\n",
    "%(os)sgen_init__:\n",
    "    # 1 rdi = max_depth\n",
    "    # 2 rsi = returnp\n",
    "    # 3 rdx = &out_region\n",
    "    # 4 rcx = &rand_region\n",
    "    pushaq\n",
    "\n",
    "    leal 0(,%%rdi,8), %%eax\n",
    "    movq %%rsp, %%r8\n",
    "    subq %%rax, %%r8\n",
    "\n",
    "    movq %%rdx, %%r11                              # &out_region\n",
    "    movq %%rcx, %%r12                              # &rand_region\n",
    "    movq (%%r11),%%r13                             # out_region\n",
    "    movq (%%r12),%%r14                             # rand_region\n",
    "\n",
    "    # general regs\n",
    "    # rax, rcx, rdx, rbx, rsi,rdi\n",
    "    # rbp, r8-r15\n",
    "    \n",
    "    call gen_start_0\n",
    "    movq %%r13, (%%r11)                            # *(&out_region) <-\n",
    "    movq %%r14, (%%r12)                            # *(&rand_region) <-\n",
    "    popaq\n",
    "    movq  $0, %%rax\n",
    "    ret   \n",
    "\"\"\" % {'os': '_' if sys.platform == 'darwin' else ''}]\n",
    "        result.append(self.fuzz_fn_defs())\n",
    "        return ''.join(result)\n",
    "\n",
    "    def main_init_var_defs(self):\n",
    "        return'''\n",
    "void gen_init__(uint32_t max_depth, void** returnp, char** out_region, uint8_t** rand_region);\n",
    "'''\n",
    "\n",
    "    def fn_main_loop_frag(self):\n",
    "        return '''\n",
    "    uint64_t out_size = 0;\n",
    "    for(int i=0; i < max_num; i++) {\n",
    "        out_regionp = out_region_initp;\n",
    "        gen_init__(max_depth32, stackp, &out_regionp, &rand_regionp);\n",
    "        *out_regionp++ = '\\\\n';\n",
    "        out_cursor = out_regionp - out_region_initp;\n",
    "        out_size += out_cursor;\n",
    "    }\n",
    "    printf(\"%lld\\\\n\", out_size);\n",
    "    '''\n",
    "    \n",
    "    def fn_main_def(self):\n",
    "        return self.fn_truncateio() + '''\n",
    "int main(int argc, char** argv) {\n",
    "    struct stat st;\n",
    "    int rand_fd;\n",
    "    uint32_t max_depth32;\n",
    "    int seed, max_num;\n",
    "%(input_frag)s\n",
    "    max_depth32 = max_depth;\n",
    "%(rand_frag)s\n",
    "%(loop_frag)s\n",
    "    return 0;\n",
    "}''' % {'input_frag': self.fn_main_input_frag(),\n",
    "        'rand_frag': self.fn_main_rand_frag(),\n",
    "        'loop_frag': self.fn_main_loop_frag()\n",
    "       }\n",
    "    \n",
    "    def fuzz_src(self, key='<start>'):\n",
    "        self.last_label = 0\n",
    "        self.cheap, self.all_prints = self.gen_cheap(self.grammar)\n",
    "        ext_strings = '\\n'.join([self.fn_fuzz_decs(), self.cheap])\n",
    "        return ext_strings, self.gen_main_src(), self.gen_fuzz_src()\n",
    "    \n",
    "    def gen_fuzz_src(self):\n",
    "        return '\\n'.join([self.fuzz_entry()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_ops, main_src, fuzz_src = CTNoWriteFuzzer(c_grammar).fuzz_src()\n",
    "with open('testers/grammar_producer_ctnowrite_main.c', 'w+') as f:\n",
    "    print(main_src, file=f)\n",
    "with open('testers/grammar_producer_ctnowrite_fuzz.s', 'w+') as f:\n",
    "    print(fuzz_src, file=f)\n",
    "with open('testers/ctnowrite_vm_ops.s', 'w+') as f:\n",
    "    print(vm_ops, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd testers\n",
    "!cc -g -Ofast -o grammar_producer_ctnowrite grammar_producer_ctnowrite_main.c grammar_producer_ctnowrite_fuzz.s\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./testers/grammar_producer_ctnowrite 0 10 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTesterNoWriteCT(CTester):\n",
    "    def exec_program(self, seed, max_depth, t):\n",
    "        fn = self.ofile(max_depth, seed)\n",
    "        return f\"./testers/grammar_producer_ctnowrite {seed} {self.max_num} {max_depth} > {fn}\"\n",
    " \n",
    "    def post_time(self):\n",
    "        super().post_time()\n",
    "        with open(self.file) as f:\n",
    "            self.size = int(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTesterNoWriteCT().run_test().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in TX:\n",
    "    print(k)\n",
    "    for depth in TX[k]:\n",
    "        print(depth)\n",
    "        if 'avgruntime' not in TX[k][depth]: continue\n",
    "        print('\\truntime =',TX[k][depth]['avgruntime'])\n",
    "        print('\\tsize = ',TX[k][depth]['avgsize'])\n",
    "        print('\\tthroughput =',TX[k][depth]['avgthroughput'])\n",
    "    print()\n",
    "    \n",
    "END_TIME = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(END_TIME - START_TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "curtime = datetime.now().isoformat()\n",
    "name = 'results/tx-%s.json' % curtime\n",
    "with open(name, 'w+') as f:\n",
    "    print(json.dumps(TX), file=f)\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grammar Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed vs Code size Tradeoffs\n",
    "\n",
    "A sliding scale of\n",
    "\n",
    "* Completely String pools\n",
    "* Compile to a state machine (CFG to Regular expression of fixed depth)\n",
    "* Encode depth in function name (Remove depth comparisons)\n",
    "\n",
    "### Expanding the use of string pools from just closing to before max_depth is exhausted\n",
    "\n",
    " We are not stuck with a pool of strings only after exhaustion of max_depth. But we will have to account for differing probabilties of different strings if we want to achieve a distribution of strings as the original. Whether it is required to have the same distribution as the original is a different question (because the original is clearly non-optimial -- shallow paths have more chance of being explroed again)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inlining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For inlining, we simply iterate through each key in the grammar, and each rule corresponding to a single key. For each rule, we inline one level, which will give us a list of corresponding rules. This set of rules will replace the original rule for the key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Fuzzing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the problems with our dumb grammar fuzzer is that each alternative rule for a key expansion is given the same probability. Hence, given a JSON element that can be a boolean, number, object or an array, the boolean (true and false) values will occur very 5 elements. This is clearly non-optimal. Hence, we need to extend our fuzzer to include probabilities in the grammar definition.\n",
    "\n",
    "Once we adopt probabilistic fuzzing, we can generate a probabilistic profile of the grammar rules by expanding the grammar to a given depth, and simply counting the number of complete items produced by each expansion. This can ensure that there is a high probability of exploring at least to that depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import operator\n",
    "\n",
    "def items_in_rule(grammar, rule, depth, max_depth):\n",
    "    if depth > max_depth: return 1\n",
    "    return reduce(operator.mul,\n",
    "                  [items_in_key(grammar, key, depth+1, max_depth)\n",
    "                   for key in  rule], 1)\n",
    "\n",
    "\n",
    "def items_in_key(grammar, key, depth=0, max_depth=10):\n",
    "    if key not in grammar: return 1\n",
    "    return sum(items_in_rule(grammar, rule, depth, max_depth)\n",
    "               for rule in grammar[key])\n",
    "\n",
    "def explore_grammar(grammar, max_depth):\n",
    "    new_g = {}\n",
    "    for k in grammar:\n",
    "        new_rules = []\n",
    "        for rule in grammar[k]:\n",
    "            items = items_in_rule(grammar, rule, depth=0, max_depth=max_depth)\n",
    "            new_rule = (rule, items)\n",
    "            new_rules.append(new_rule)\n",
    "        new_g[k] = new_rules\n",
    "    return new_g\n",
    "\n",
    "def to_ranges(pgrammar):\n",
    "    new_g = {}\n",
    "    for k in pgrammar:\n",
    "        last = 0\n",
    "        elts = []\n",
    "        for elt in pgrammar[k]:\n",
    "            rule, count = elt\n",
    "            frm = last\n",
    "            last += count\n",
    "            to = last\n",
    "            new_elt = rule, (frm, to)\n",
    "            elts.append(new_elt)\n",
    "        new_g[k] = elts\n",
    "    return new_g\n",
    "p_grammar = explore_grammar(my_grammar, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_included_rule(idx, rules_):\n",
    "    for r,rng in rules_:\n",
    "        if rng[0] <= idx and idx < rng[1]: return r\n",
    "    assert False\n",
    "\n",
    "def gen_key(grammar, key, depth=0, max_depth=10):\n",
    "    if key not in grammar: return [key]\n",
    "    if depth > max_depth: return [random.choice(pool_of_strings[key])]\n",
    "    \n",
    "    rules_ = grammar[key]\n",
    "    max_val = max([j for rule, (i,j) in rules_])\n",
    "    rule = get_included_rule(random.randrange(max_val), rules_)\n",
    "    return gen_rule(grammar, rule, depth+1, max_depth)\n",
    "\n",
    "def gen_rule(grammar, rule, depth, max_depth):\n",
    "    return sum([gen_key(grammar, token, depth, max_depth) for token in rule], [])\n",
    "\n",
    "def grammar_producer_p(grammar, key='<start>'):\n",
    "    cp_grammar = to_ranges(grammar)\n",
    "    return ''.join(gen_key(cp_grammar, key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = PooledFuzzer(my_grammar)\n",
    "pool_of_strings = pf.pool_of_strings\n",
    "for i in range(10):\n",
    "    print(grammar_producer_p(p_grammar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Large Inputs Fast"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "540.85px",
    "left": "1381px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
